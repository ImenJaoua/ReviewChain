{"old_hunk": "@@ -53,6 +53,75 @@ def use_gpu():\n         yield\n \n \n+def create_virtual_devices(num_devices,\n+                           force_device=None,\n+                           memory_limit_per_device=1024):\n+    \"\"\"Virtualize a the physical device into logical devices.\n+\n+    Args:\n+        num_devices: The number of virtual devices needed.\n+        force_device: 'CPU'/'GPU'. Defaults to None, where the\n+            devices is selected based on the system.\n+        memory_limit_per_device: Specify memory for each\n+            virtual GPU. Only for GPUs.\n+\n+    Returns:\n+        virtual_devices: A list of virtual devices which can be passed to\n+            tf.distribute.MirroredStrategy()\n+    \"\"\"\n+    if force_device is None:\n+        device_type = 'GPU' if len(\n+            tf.config.list_physical_devices('GPU')) > 0 else 'CPU'\n+    else:\n+        assert (force_device in ['CPU', 'GPU'])\n+        device_type = force_device\n+\n+    physical_devices = tf.config.list_physical_devices(device_type)\n+\n+    if device_type == 'CPU':\n+        memory_limit_per_device = None\n+\n+    tf.config.experimental.set_virtual_device_configuration(\n+        physical_devices[0], [\n+            tf.config.experimental.VirtualDeviceConfiguration(\n+                memory_limit=memory_limit_per_device)\n+            for _ in range(num_devices)\n+        ])\n+\n+    return tf.config.experimental.list_logical_devices(device_type)\n+\n+\n+def run_all_distributed(num_devices):\n+    base_decorator = run_distributed(num_devices)\n+\n+    def decorator(cls):\n+        for name, method in cls.__dict__.copy().items():\n+            if (callable(method)\n+                    and name.startswith(unittest.TestLoader.testMethodPrefix)\n+                    and name != \"test_session\"):\n+                setattr(cls, name, base_decorator(method))\n+        return cls\n+\n+    return decorator\n+\n+\n+def run_distributed(num_devices):\n+    def decorator(f):\n+        if inspect.isclass(f):\n+            raise TypeError(\"`run_distributed` only supports test methods. \"\n+                            \"Did you mean to use `run_all_distributed`?\")\n+\n+        def decorated(self, *args, **kwargs):\n+            logical_devices = create_virtual_devices(num_devices)\n+            strategy = tf.distribute.MirroredStrategy(logical_devices)", "oldf": "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Utilities for tf.test.TestCase.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport inspect\nimport unittest\n\nimport tensorflow as tf\n# yapf: disable\n# pylint: disable=unused-import\n# TODO: find public API alternative to these\nfrom tensorflow.python.framework.test_util import run_all_in_graph_and_eager_modes\nfrom tensorflow.python.framework.test_util import run_deprecated_v1\nfrom tensorflow.python.framework.test_util import run_in_graph_and_eager_modes\nfrom tensorflow.python.keras.testing_utils import layer_test\nfrom tensorflow.python.keras import keras_parameterized\n\n# pylint: enable=unused-import\n# yapf: enable\n\n\n@contextlib.contextmanager\ndef device(use_gpu):\n    \"\"\"Uses gpu when requested and available.\"\"\"\n    if use_gpu and tf.test.is_gpu_available():\n        dev = \"/device:GPU:0\"\n    else:\n        dev = \"/device:CPU:0\"\n    with tf.device(dev):\n        yield\n\n\n@contextlib.contextmanager\ndef use_gpu():\n    \"\"\"Uses gpu when requested and available.\"\"\"\n    with device(use_gpu=True):\n        yield\n\n\ndef create_virtual_devices(num_devices,\n                           force_device=None,\n                           memory_limit_per_device=1024):\n    \"\"\"Virtualize a the physical device into logical devices.\n\n    Args:\n        num_devices: The number of virtual devices needed.\n        force_device: 'CPU'/'GPU'. Defaults to None, where the\n            devices is selected based on the system.\n        memory_limit_per_device: Specify memory for each\n            virtual GPU. Only for GPUs.\n\n    Returns:\n        virtual_devices: A list of virtual devices which can be passed to\n            tf.distribute.MirroredStrategy()\n    \"\"\"\n    if force_device is None:\n        device_type = 'GPU' if len(\n            tf.config.list_physical_devices('GPU')) > 0 else 'CPU'\n    else:\n        assert (force_device in ['CPU', 'GPU'])\n        device_type = force_device\n\n    physical_devices = tf.config.list_physical_devices(device_type)\n\n    if device_type == 'CPU':\n        memory_limit_per_device = None\n\n    tf.config.experimental.set_virtual_device_configuration(\n        physical_devices[0], [\n            tf.config.experimental.VirtualDeviceConfiguration(\n                memory_limit=memory_limit_per_device)\n            for _ in range(num_devices)\n        ])\n\n    return tf.config.experimental.list_logical_devices(device_type)\n\n\ndef run_all_distributed(num_devices):\n    base_decorator = run_distributed(num_devices)\n\n    def decorator(cls):\n        for name, method in cls.__dict__.copy().items():\n            if (callable(method)\n                    and name.startswith(unittest.TestLoader.testMethodPrefix)\n                    and name != \"test_session\"):\n                setattr(cls, name, base_decorator(method))\n        return cls\n\n    return decorator\n\n\ndef run_distributed(num_devices):\n    def decorator(f):\n        if inspect.isclass(f):\n            raise TypeError(\"`run_distributed` only supports test methods. \"\n                            \"Did you mean to use `run_all_distributed`?\")\n\n        def decorated(self, *args, **kwargs):\n            logical_devices = create_virtual_devices(num_devices)\n            strategy = tf.distribute.MirroredStrategy(logical_devices)\n            with strategy.scope():\n                f(self, *args, **kwargs)\n\n        return decorated\n\n    return decorator\n\n\ndef run_all_with_types(dtypes):\n    \"\"\"Execute all test methods in the given class with and without eager.\"\"\"\n    base_decorator = run_with_types(dtypes)\n\n    def decorator(cls):\n        for name, method in cls.__dict__.copy().items():\n            if (callable(method)\n                    and name.startswith(unittest.TestLoader.testMethodPrefix)\n                    and name != \"test_session\"):\n                setattr(cls, name, base_decorator(method))\n        return cls\n\n    return decorator\n\n\ndef run_with_types(dtypes):\n    def decorator(f):\n        if inspect.isclass(f):\n            raise TypeError(\"`run_with_types` only supports test methods. \"\n                            \"Did you mean to use `run_all_with_types`?\")\n\n        def decorated(self, *args, **kwargs):\n            for t in dtypes:\n                f(self, *args, dtype=t, **kwargs)\n\n        return decorated\n\n    return decorator\n", "hunk": "@@ -105,6 +105,7 @@ def run_all_distributed(num_devices):\n     return decorator\n \n \n+# TODO: Add support for other distribution strategies\n def run_distributed(num_devices):\n     def decorator(f):\n         if inspect.isclass(f):\n", "comment": "Is there a way we can make the distribution strategy configurable? This might be useful for certain types of bugs", "ids": ["15645", "8ffc471b78a839e82bf6bba73a846be8ef4e22a3", "aa8c60f04e55d8da007eb1aedc0c72007a4892d2"], "repo": "tensorflow/addons", "ghid": 842, "old": "     return decorator\n def run_distributed(num_devices):\n     def decorator(f):\n         if inspect.isclass(f):", "new": "     return decorator\n+# TODO: Add support for other distribution strategies\n def run_distributed(num_devices):\n     def decorator(f):\n         if inspect.isclass(f):", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -0,0 +1,36 @@\n+#!/usr/bin/env python3\n+\n+\n+import sys\n+import os\n+import pika\n+from influxdb import InfluxDBClient\n+from influxdb.exceptions import InfluxDBClientError, InfluxDBServerError\n+import listenbrainz.config as config\n+from listenbrainz.listenstore import InfluxListenStore\n+from listenbrainz.utils import escape, get_measurement_name, get_escaped_measurement_name, \\\n+                               get_influx_query_timestamp, convert_to_unix_timestamp, \\\n+                               convert_timestamp_to_influx_row_format\n+\n+COUNT_RETENTION_POLICY = \"one_week\"", "oldf": "\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n<html>\n  <head>\n    <title>503 Backend is unhealthy</title>\n  </head>\n  <body>\n    <h1>Error 503 Backend is unhealthy</h1>\n    <p>Backend is unhealthy</p>\n    <h3>Guru Mediation:</h3>\n    <p>Details: cache-sea4458-SEA 1645542657 1832937762</p>\n    <hr>\n    <p>Varnish cache server</p>\n  </body>\n</html>\n", "hunk": "@@ -10,7 +10,7 @@\n     <h1>Error 503 Backend is unhealthy</h1>\n     <p>Backend is unhealthy</p>\n     <h3>Guru Mediation:</h3>\n-    <p>Details: cache-sea4458-SEA 1645542657 1832937762</p>\n+    <p>Details: cache-sea4445-SEA 1645542657 290303048</p>\n     <hr>\n     <p>Varnish cache server</p>\n   </body>\n", "comment": "would it be worth importing these defines from `influx_listenstore`?", "ids": ["14410", "f01f1713e47fc3837556ecdb64944c56bdec6786", "d411868b16ad20b6040341f9ccfac82a4881d09e"], "repo": "metabrainz/listenbrainz-server", "ghid": 209, "old": "     <h1>Error 503 Backend is unhealthy</h1>\n     <p>Backend is unhealthy</p>\n     <h3>Guru Mediation:</h3>\n-    <p>Details: cache-sea4458-SEA 1645542657 1832937762</p>\n     <hr>\n     <p>Varnish cache server</p>\n   </body>", "new": "     <h1>Error 503 Backend is unhealthy</h1>\n     <p>Backend is unhealthy</p>\n     <h3>Guru Mediation:</h3>\n+    <p>Details: cache-sea4445-SEA 1645542657 290303048</p>\n     <hr>\n     <p>Varnish cache server</p>\n   </body>", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -276,7 +274,10 @@ def edge_subgraph(graph, edges, preserve_nodes=False, store_ids=True):\n     --------\n     node_subgraph\n     \"\"\"\n-    if graph.is_block and not preserve_nodes:\n+    if len(deprecated_kwargs) != 0:\n+        raise DGLError(\"Key word argument preserve_nodes is deprecated. \"\n+                       \"Use relabel_nodes instead.\")", "oldf": "\"\"\"Functions for extracting subgraphs.\n\nThe module only contains functions for extracting subgraphs deterministically.\nFor stochastic subgraph extraction, please see functions under :mod:`dgl.sampling`.\n\"\"\"\nfrom collections.abc import Mapping\n\nfrom ._ffi.function import _init_api\nfrom .base import DGLError\nfrom . import backend as F\nfrom . import graph_index\nfrom . import heterograph_index\nfrom . import ndarray as nd\nfrom .heterograph import DGLHeteroGraph\nfrom . import utils\n\n__all__ = ['node_subgraph', 'edge_subgraph', 'node_type_subgraph', 'edge_type_subgraph',\n           'in_subgraph', 'out_subgraph']\n\ndef node_subgraph(graph, nodes, *, relabel_nodes=True, store_ids=True):\n    \"\"\"Return a subgraph induced on the given nodes.\n\n    A node-induced subgraph is a graph with edges whose endpoints are both in the\n    specified node set. In addition to extracting the subgraph, DGL also copies\n    the features of the extracted nodes and edges to the resulting graph. The copy\n    is *lazy* and incurs data movement only when needed.\n\n    If the graph is heterogeneous, DGL extracts a subgraph per relation and composes\n    them as the resulting graph. Thus, the resulting graph has the same set of relations\n    as the input one.\n\n    Parameters\n    ----------\n    graph : DGLGraph\n        The graph to extract subgraphs from.\n    nodes : nodes or dict[str, nodes]\n        The nodes to form the subgraph. The allowed nodes formats are:\n\n        * Int Tensor: Each element is a node ID. The tensor must have the same device type\n          and ID data type as the graph's.\n        * iterable[int]: Each element is a node ID.\n        * Bool Tensor: Each :math:`i^{th}` element is a bool flag indicating whether\n          node :math:`i` is in the subgraph.\n\n        If the graph is homogeneous, one can directly pass the above formats.\n        Otherwise, the argument must be a dictionary with keys being node types\n        and values being the node IDs in the above formats.\n    relabel_nodes : bool, optional\n        If True, the extracted subgraph will only have the nodes in the specified node set\n        and it will relabel the nodes in order.\n    store_ids : bool, optional\n        If True, it will store the raw IDs of the extracted edges in the ``edata`` of the\n        resulting graph under name ``dgl.EID``; if ``relabel_nodes`` is ``True``, it will\n        also store the raw IDs of the specified nodes in the ``ndata`` of the resulting\n        graph under name ``dgl.NID``.\n\n    Returns\n    -------\n    G : DGLGraph\n        The subgraph.\n\n    Notes\n    -----\n\n    This function discards the batch information. Please use\n    :func:`dgl.DGLGraph.set_batch_num_nodes`\n    and :func:`dgl.DGLGraph.set_batch_num_edges` on the transformed graph\n    to maintain the information.\n\n    Examples\n    --------\n    The following example uses PyTorch backend.\n\n    >>> import dgl\n    >>> import torch\n\n    Extract a subgraph from a homogeneous graph.\n\n    >>> g = dgl.graph(([0, 1, 2, 3, 4], [1, 2, 3, 4, 0]))  # 5-node cycle\n    >>> sg = dgl.node_subgraph(g, [0, 1, 4])\n    >>> sg\n    Graph(num_nodes=3, num_edges=2,\n          ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n          edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)})\n    >>> sg.edges()\n    (tensor([0, 2]), tensor([1, 0]))\n    >>> sg.ndata[dgl.NID]  # original node IDs\n    tensor([0, 1, 4])\n    >>> sg.edata[dgl.EID]  # original edge IDs\n    tensor([0, 4])\n\n    Specify nodes using a boolean mask.\n\n    >>> nodes = torch.tensor([True, True, False, False, True])  # choose nodes [0, 1, 4]\n    >>> dgl.node_subgraph(g, nodes)\n    Graph(num_nodes=3, num_edges=2,\n          ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n          edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)})\n\n    The resulting subgraph also copies features from the parent graph.\n\n    >>> g.ndata['x'] = torch.arange(10).view(5, 2)\n    >>> sg = dgl.node_subgraph(g, [0, 1, 4])\n    >>> sg\n    Graph(num_nodes=3, num_edges=2,\n          ndata_schemes={'x': Scheme(shape=(2,), dtype=torch.int64),\n                         '_ID': Scheme(shape=(), dtype=torch.int64)}\n          edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)})\n    >>> sg.ndata['x']\n    tensor([[0, 1],\n            [2, 3],\n            [8, 9]])\n\n    Extract a subgraph from a hetergeneous graph.\n\n    >>> g = dgl.heterograph({\n    >>>     ('user', 'plays', 'game'): ([0, 1, 1, 2], [0, 0, 2, 1]),\n    >>>     ('user', 'follows', 'user'): ([0, 1, 1], [1, 2, 2])\n    >>> })\n    >>> sub_g = dgl.node_subgraph(g, {'user': [1, 2]})\n    >>> sub_g\n    Graph(num_nodes={'user': 2, 'game': 0},\n          num_edges={('user', 'plays', 'game'): 0, ('user', 'follows', 'user'): 2},\n          metagraph=[('user', 'game'), ('user', 'user')])\n\n    See Also\n    --------\n    edge_subgraph\n    \"\"\"\n    if graph.is_block:\n        raise DGLError('Extracting subgraph from a block graph is not allowed.')\n    if not isinstance(nodes, Mapping):\n        assert len(graph.ntypes) == 1, \\\n            'need a dict of node type and IDs for graph with multiple node types'\n        nodes = {graph.ntypes[0]: nodes}\n\n    def _process_nodes(ntype, v):\n        if F.is_tensor(v) and F.dtype(v) == F.bool:\n            return F.astype(F.nonzero_1d(F.copy_to(v, graph.device)), graph.idtype)\n        else:\n            return utils.prepare_tensor(graph, v, 'nodes[\"{}\"]'.format(ntype))\n\n    induced_nodes = []\n    for ntype in graph.ntypes:\n        nids = nodes.get(ntype, F.copy_to(F.tensor([], graph.idtype), graph.device))\n        induced_nodes.append(_process_nodes(ntype, nids))\n    sgi = graph._graph.node_subgraph(induced_nodes, relabel_nodes)\n    induced_edges = sgi.induced_edges\n    induced_nodes = sgi.induced_nodes if relabel_nodes else None\n    return _create_hetero_subgraph(graph, sgi, induced_nodes, induced_edges, store_ids=store_ids)\n\nDGLHeteroGraph.subgraph = utils.alias_func(node_subgraph)\n\ndef edge_subgraph(graph, edges, *, relabel_nodes=True, store_ids=True, **deprecated_kwargs):\n    \"\"\"Return a subgraph induced on the given edges.\n\n    An edge-induced subgraph is equivalent to creating a new graph using the given\n    edges. In addition to extracting the subgraph, DGL also copies the features\n    of the extracted nodes and edges to the resulting graph. The copy is *lazy*\n    and incurs data movement only when needed.\n\n    If the graph is heterogeneous, DGL extracts a subgraph per relation and composes\n    them as the resulting graph. Thus, the resulting graph has the same set of relations\n    as the input one.\n\n    Parameters\n    ----------\n    graph : DGLGraph\n        The graph to extract the subgraph from.\n    edges : edges or dict[(str, str, str), edges]\n        The edges to form the subgraph. The allowed edges formats are:\n\n        * Int Tensor: Each element is an edge ID. The tensor must have the same device type\n          and ID data type as the graph's.\n        * iterable[int]: Each element is an edge ID.\n        * Bool Tensor: Each :math:`i^{th}` element is a bool flag indicating whether\n          edge :math:`i` is in the subgraph.\n\n        If the graph is homogeneous, one can directly pass the above formats.\n        Otherwise, the argument must be a dictionary with keys being edge types\n        and values being the edge IDs in the above formats.\n    relabel_nodes : bool, optional\n        If True, it will remove the isolated nodes and relabel the incident nodes in the\n        extracted subgraph.\n    store_ids : bool, optional\n        If True, it will store the raw IDs of the extracted edges in the ``edata`` of the\n        resulting graph under name ``dgl.EID``; if ``relabel_nodes`` is ``True``, it will\n        also store the raw IDs of the incident nodes in the ``ndata`` of the resulting\n        graph under name ``dgl.NID``.\n\n    Returns\n    -------\n    G : DGLGraph\n        The subgraph.\n\n    Notes\n    -----\n\n    This function discards the batch information. Please use\n    :func:`dgl.DGLGraph.set_batch_num_nodes`\n    and :func:`dgl.DGLGraph.set_batch_num_edges` on the transformed graph\n    to maintain the information.\n\n    Examples\n    --------\n    The following example uses PyTorch backend.\n\n    >>> import dgl\n    >>> import torch\n\n    Extract a subgraph from a homogeneous graph.\n\n    >>> g = dgl.graph(([0, 1, 2, 3, 4], [1, 2, 3, 4, 0]))  # 5-node cycle\n    >>> sg = dgl.edge_subgraph(g, [0, 4])\n    >>> sg\n    Graph(num_nodes=3, num_edges=2,\n          ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n          edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)})\n    >>> sg.edges()\n    (tensor([0, 1]), tensor([2, 0]))\n    >>> sg.ndata[dgl.NID]  # original node IDs\n    tensor([0, 4, 1])\n    >>> sg.edata[dgl.EID]  # original edge IDs\n    tensor([0, 4])\n\n    Extract a subgraph without node relabeling.\n\n    >>> sg = dgl.edge_subgraph(g, [0, 4], relabel_nodes=False)\n    >>> sg\n    Graph(num_nodes=5, num_edges=2,\n          ndata_schemes={}\n          edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)})\n    >>> sg.edges()\n    (tensor([0, 4]), tensor([1, 0]))\n\n    Specify edges using a boolean mask.\n\n    >>> nodes = torch.tensor([True, False, False, False, True])  # choose edges [0, 4]\n    >>> dgl.edge_subgraph(g, nodes)\n    Graph(num_nodes=3, num_edges=2,\n          ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n          edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)})\n\n    The resulting subgraph also copies features from the parent graph.\n\n    >>> g.ndata['x'] = torch.arange(10).view(5, 2)\n    >>> sg = dgl.edge_subgraph(g, [0, 4])\n    >>> sg\n    Graph(num_nodes=3, num_edges=2,\n          ndata_schemes={'x': Scheme(shape=(2,), dtype=torch.int64),\n                         '_ID': Scheme(shape=(), dtype=torch.int64)}\n          edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)})\n    >>> sg.ndata[dgl.NID]\n    tensor([0, 4, 1])\n    >>> sg.ndata['x']\n    tensor([[0, 1],\n            [8, 9],\n            [2, 3]])\n\n    Extract a subgraph from a hetergeneous graph.\n\n    >>> g = dgl.heterograph({\n    >>>     ('user', 'plays', 'game'): ([0, 1, 1, 2], [0, 0, 2, 1]),\n    >>>     ('user', 'follows', 'user'): ([0, 1, 1], [1, 2, 2])\n    >>> })\n    >>> sub_g = dgl.edge_subgraph(g, {('user', 'follows', 'user'): [1, 2],\n    ...                               ('user', 'plays', 'game'): [2]})\n    >>> print(sub_g)\n    Graph(num_nodes={'user': 2, 'game': 1},\n          num_edges={('user', 'plays', 'game'): 1, ('user', 'follows', 'user'): 2},\n          metagraph=[('user', 'game'), ('user', 'user')])\n\n    See Also\n    --------\n    node_subgraph\n    \"\"\"\n    if len(deprecated_kwargs) != 0:\n        raise DGLError(\"Key word argument preserve_nodes is deprecated. \"\n                       \"Use relabel_nodes instead.\")\n    if graph.is_block and relabel_nodes:\n        raise DGLError('Extracting subgraph from a block graph is not allowed.')\n    if not isinstance(edges, Mapping):\n        assert len(graph.canonical_etypes) == 1, \\\n            'need a dict of edge type and IDs for graph with multiple edge types'\n        edges = {graph.canonical_etypes[0]: edges}\n\n    def _process_edges(etype, e):\n        if F.is_tensor(e) and F.dtype(e) == F.bool:\n            return F.astype(F.nonzero_1d(F.copy_to(e, graph.device)), graph.idtype)\n        else:\n            return utils.prepare_tensor(graph, e, 'edges[\"{}\"]'.format(etype))\n\n    edges = {graph.to_canonical_etype(etype): e for etype, e in edges.items()}\n    induced_edges = []\n    for cetype in graph.canonical_etypes:\n        eids = edges.get(cetype, F.copy_to(F.tensor([], graph.idtype), graph.device))\n        induced_edges.append(_process_edges(cetype, eids))\n    sgi = graph._graph.edge_subgraph(induced_edges, not relabel_nodes)\n    induced_nodes = sgi.induced_nodes if relabel_nodes else None\n    return _create_hetero_subgraph(graph, sgi, induced_nodes, induced_edges, store_ids=store_ids)\n\nDGLHeteroGraph.edge_subgraph = utils.alias_func(edge_subgraph)\n\ndef in_subgraph(graph, nodes, *, relabel_nodes=False, store_ids=True):\n    \"\"\"Return the subgraph induced on the inbound edges of all the edge types of the\n    given nodes.\n\n    An edge-induced subgraph is equivalent to creating a new graph using the given edges.\n    In addition to extracting the subgraph, DGL also copies the features of the extracted\n    nodes and edges to the resulting graph. The copy is *lazy* and incurs data movement\n    only when needed.\n\n    If the graph is heterogeneous, DGL extracts a subgraph per relation and composes\n    them as the resulting graph. Thus, the resulting graph has the same set of relations\n    as the input one.\n\n    Parameters\n    ----------\n    graph : DGLGraph\n        The input graph.\n    nodes : nodes or dict[str, nodes]\n        The nodes to form the subgraph. The allowed nodes formats are:\n\n        * Int Tensor: Each element is a node ID. The tensor must have the same device type\n          and ID data type as the graph's.\n        * iterable[int]: Each element is a node ID.\n\n        If the graph is homogeneous, one can directly pass the above formats.\n        Otherwise, the argument must be a dictionary with keys being node types\n        and values being the node IDs in the above formats.\n    relabel_nodes : bool, optional\n        If True, it will remove the isolated nodes and relabel the rest nodes in the\n        extracted subgraph.\n    store_ids : bool, optional\n        If True, it will store the raw IDs of the extracted edges in the ``edata`` of the\n        resulting graph under name ``dgl.EID``; if ``relabel_nodes`` is ``True``, it will\n        also store the raw IDs of the extracted nodes in the ``ndata`` of the resulting\n        graph under name ``dgl.NID``.\n\n    Returns\n    -------\n    DGLGraph\n        The subgraph.\n\n    Notes\n    -----\n\n    This function discards the batch information. Please use\n    :func:`dgl.DGLGraph.set_batch_num_nodes`\n    and :func:`dgl.DGLGraph.set_batch_num_edges` on the transformed graph\n    to maintain the information.\n\n    Examples\n    --------\n    The following example uses PyTorch backend.\n\n    >>> import dgl\n    >>> import torch\n\n    Extract a subgraph from a homogeneous graph.\n\n    >>> g = dgl.graph(([0, 1, 2, 3, 4], [1, 2, 3, 4, 0]))  # 5-node cycle\n    >>> g.edata['w'] = torch.arange(10).view(5, 2)\n    >>> sg = dgl.in_subgraph(g, [2, 0])\n    >>> sg\n    Graph(num_nodes=5, num_edges=2,\n          ndata_schemes={}\n          edata_schemes={'w': Scheme(shape=(2,), dtype=torch.int64),\n                         '_ID': Scheme(shape=(), dtype=torch.int64)})\n    >>> sg.edges()\n    (tensor([1, 4]), tensor([2, 0]))\n    >>> sg.edata[dgl.EID]  # original edge IDs\n    tensor([1, 4])\n    >>> sg.edata['w']  # also extract the features\n    tensor([[2, 3],\n            [8, 9]])\n\n    Extract a subgraph with node labeling.\n\n    >>> sg = dgl.in_subgraph(g, [2, 0], relabel_nodes=True)\n    >>> sg\n    Graph(num_nodes=4, num_edges=2,\n          ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64}\n          edata_schemes={'w': Scheme(shape=(2,), dtype=torch.int64),\n                         '_ID': Scheme(shape=(), dtype=torch.int64)})\n    >>> sg.edges()\n    (tensor([1, 3]), tensor([2, 0]))\n    >>> sg.edata[dgl.EID]  # original edge IDs\n    tensor([1, 4])\n    >>> sg.ndata[dgl.NID]  # original node IDs\n    tensor([0, 1, 2, 4])\n\n    Extract a subgraph from a heterogeneous graph.\n\n    >>> g = dgl.heterograph({\n    ...     ('user', 'plays', 'game'): ([0, 1, 1, 2], [0, 0, 2, 1]),\n    ...     ('user', 'follows', 'user'): ([0, 1, 1], [1, 2, 2])})\n    >>> sub_g = g.in_subgraph({'user': [2], 'game': [2]})\n    >>> sub_g\n    Graph(num_nodes={'game': 3, 'user': 3},\n          num_edges={('user', 'plays', 'game'): 1, ('user', 'follows', 'user'): 2},\n          metagraph=[('user', 'game', 'plays'), ('user', 'user', 'follows')])\n\n    See also\n    --------\n    out_subgraph\n    \"\"\"\n    if graph.is_block:\n        raise DGLError('Extracting subgraph of a block graph is not allowed.')\n    if not isinstance(nodes, dict):\n        if len(graph.ntypes) > 1:\n            raise DGLError(\"Must specify node type when the graph is not homogeneous.\")\n        nodes = {graph.ntypes[0] : nodes}\n    nodes = utils.prepare_tensor_dict(graph, nodes, 'nodes')\n    nodes_all_types = []\n    for ntype in graph.ntypes:\n        if ntype in nodes:\n            nodes_all_types.append(F.to_dgl_nd(nodes[ntype]))\n        else:\n            nodes_all_types.append(nd.NULL[graph._idtype_str])\n\n    sgi = _CAPI_DGLInSubgraph(graph._graph, nodes_all_types, relabel_nodes)\n    induced_nodes = sgi.induced_nodes if relabel_nodes else None\n    induced_edges = sgi.induced_edges\n    return _create_hetero_subgraph(graph, sgi, induced_nodes, induced_edges, store_ids=store_ids)\n\nDGLHeteroGraph.in_subgraph = utils.alias_func(in_subgraph)\n\ndef out_subgraph(graph, nodes, *, relabel_nodes=False, store_ids=True):\n    \"\"\"Return the subgraph induced on the outbound edges of all the edge types of the\n    given nodes.\n\n    An edge-induced subgraph is equivalent to creating a new graph using the given edges.\n    In addition to extracting the subgraph, DGL also copies the features of the extracted\n    nodes and edges to the resulting graph. The copy is *lazy* and incurs data movement\n    only when needed.\n\n    If the graph is heterogeneous, DGL extracts a subgraph per relation and composes\n    them as the resulting graph. Thus, the resulting graph has the same set of relations\n    as the input one.\n\n    Parameters\n    ----------\n    graph : DGLGraph\n        The input graph.\n    nodes : nodes or dict[str, nodes]\n        The nodes to form the subgraph. The allowed nodes formats are:\n\n        * Int Tensor: Each element is a node ID. The tensor must have the same device type\n          and ID data type as the graph's.\n        * iterable[int]: Each element is a node ID.\n\n        If the graph is homogeneous, one can directly pass the above formats.\n        Otherwise, the argument must be a dictionary with keys being node types\n        and values being the node IDs in the above formats.\n    relabel_nodes : bool, optional\n        If True, it will remove the isolated nodes and relabel the rest nodes in the\n        extracted subgraph.\n    store_ids : bool, optional\n        If True, it will store the raw IDs of the extracted edges in the ``edata`` of the\n        resulting graph under name ``dgl.EID``; if ``relabel_nodes`` is ``True``, it will\n        also store the raw IDs of the extracted nodes in the ``ndata`` of the resulting\n        graph under name ``dgl.NID``.\n\n    Returns\n    -------\n    DGLGraph\n        The subgraph.\n\n    Notes\n    -----\n\n    This function discards the batch information. Please use\n    :func:`dgl.DGLGraph.set_batch_num_nodes`\n    and :func:`dgl.DGLGraph.set_batch_num_edges` on the transformed graph\n    to maintain the information.\n\n    Examples\n    --------\n    The following example uses PyTorch backend.\n\n    >>> import dgl\n    >>> import torch\n\n    Extract a subgraph from a homogeneous graph.\n\n    >>> g = dgl.graph(([0, 1, 2, 3, 4], [1, 2, 3, 4, 0]))  # 5-node cycle\n    >>> g.edata['w'] = torch.arange(10).view(5, 2)\n    >>> sg = dgl.out_subgraph(g, [2, 0])\n    >>> sg\n    Graph(num_nodes=5, num_edges=2,\n          ndata_schemes={}\n          edata_schemes={'w': Scheme(shape=(2,), dtype=torch.int64),\n                         '_ID': Scheme(shape=(), dtype=torch.int64)})\n    >>> sg.edges()\n    (tensor([2, 0]), tensor([3, 1]))\n    >>> sg.edata[dgl.EID]  # original edge IDs\n    tensor([2, 0])\n    >>> sg.edata['w']  # also extract the features\n    tensor([[4, 5],\n            [0, 1]])\n\n    Extract a subgraph with node labeling.\n\n    >>> sg = dgl.out_subgraph(g, [2, 0], relabel_nodes=True)\n    >>> sg\n    Graph(num_nodes=4, num_edges=2,\n          ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n          edata_schemes={'w': Scheme(shape=(2,), dtype=torch.int64),\n                         '_ID': Scheme(shape=(), dtype=torch.int64)})\n    >>> sg.edges()\n    (tensor([2, 0]), tensor([3, 1]))\n    >>> sg.edata[dgl.EID]  # original edge IDs\n    tensor([2, 0])\n    >>> sg.ndata[dgl.NID]  # original node IDs\n    tensor([0, 1, 2, 3])\n\n    Extract a subgraph from a heterogeneous graph.\n\n    >>> g = dgl.heterograph({\n    ...     ('user', 'plays', 'game'): ([0, 1, 1, 2], [0, 0, 2, 1]),\n    ...     ('user', 'follows', 'user'): ([0, 1, 1], [1, 2, 2])})\n    >>> sub_g = g.out_subgraph({'user': [1]})\n    >>> sub_g\n    Graph(num_nodes={'game': 3, 'user': 3},\n          num_edges={('user', 'plays', 'game'): 2, ('user', 'follows', 'user'): 2},\n          metagraph=[('user', 'game', 'plays'), ('user', 'user', 'follows')])\n\n    See also\n    --------\n    in_subgraph\n    \"\"\"\n    if graph.is_block:\n        raise DGLError('Extracting subgraph of a block graph is not allowed.')\n    if not isinstance(nodes, dict):\n        if len(graph.ntypes) > 1:\n            raise DGLError(\"Must specify node type when the graph is not homogeneous.\")\n        nodes = {graph.ntypes[0] : nodes}\n    nodes = utils.prepare_tensor_dict(graph, nodes, 'nodes')\n    nodes_all_types = []\n    for ntype in graph.ntypes:\n        if ntype in nodes:\n            nodes_all_types.append(F.to_dgl_nd(nodes[ntype]))\n        else:\n            nodes_all_types.append(nd.NULL[graph._idtype_str])\n\n    sgi = _CAPI_DGLOutSubgraph(graph._graph, nodes_all_types, relabel_nodes)\n    induced_nodes = sgi.induced_nodes if relabel_nodes else None\n    induced_edges = sgi.induced_edges\n    return _create_hetero_subgraph(graph, sgi, induced_nodes, induced_edges, store_ids=store_ids)\n\nDGLHeteroGraph.out_subgraph = utils.alias_func(out_subgraph)\n\ndef node_type_subgraph(graph, ntypes):\n    \"\"\"Return the subgraph induced on given node types.\n\n    A node-type-induced subgraph contains all the nodes of the given subset of\n    the node types of a graph and any edges whose endpoints are both in this subset.\n    In addition to extracting the subgraph, DGL also copies the features of the\n    extracted nodes and edges to the resulting graph.\n    The copy is *lazy* and incurs data movement only when needed.\n\n    Parameters\n    ----------\n    graph : DGLGraph\n        The graph to extract subgraphs from.\n    ntypes : list[str]\n        The type names of the nodes in the subgraph.\n\n    Returns\n    -------\n    G : DGLGraph\n        The subgraph.\n\n    Notes\n    -----\n\n    This function discards the batch information. Please use\n    :func:`dgl.DGLGraph.set_batch_num_nodes`\n    and :func:`dgl.DGLGraph.set_batch_num_edges` on the transformed graph\n    to maintain the information.\n\n    Examples\n    --------\n    The following example uses PyTorch backend.\n\n    >>> import dgl\n    >>> import torch\n\n    Instantiate a heterograph.\n\n    >>> g = dgl.heterograph({\n    >>>     ('user', 'plays', 'game'): ([0, 1, 1, 2], [0, 0, 2, 1]),\n    >>>     ('user', 'follows', 'user'): ([0, 1, 1], [1, 2, 2])\n    >>> })\n    >>> # Set node features\n    >>> g.nodes['user'].data['h'] = torch.tensor([[0.], [1.], [2.]])\n\n    Get subgraphs.\n\n    >>> sub_g = g.node_type_subgraph(['user'])\n    >>> print(sub_g)\n    Graph(num_nodes=3, num_edges=3,\n          ndata_schemes={'h': Scheme(shape=(1,), dtype=torch.float32)}\n          edata_schemes={})\n\n    Get the extracted node features.\n\n    >>> sub_g.nodes['user'].data['h']\n    tensor([[0.],\n            [1.],\n            [2.]])\n\n    See Also\n    --------\n    edge_type_subgraph\n    \"\"\"\n    ntid = [graph.get_ntype_id(ntype) for ntype in ntypes]\n    stids, dtids, etids = graph._graph.metagraph.edges('eid')\n    stids, dtids, etids = stids.tonumpy(), dtids.tonumpy(), etids.tonumpy()\n    etypes = []\n    for stid, dtid, etid in zip(stids, dtids, etids):\n        if stid in ntid and dtid in ntid:\n            etypes.append(graph.canonical_etypes[etid])\n    if len(etypes) == 0:\n        raise DGLError('There are no edges among nodes of the specified types.')\n    return edge_type_subgraph(graph, etypes)\n\nDGLHeteroGraph.node_type_subgraph = utils.alias_func(node_type_subgraph)\n\ndef edge_type_subgraph(graph, etypes):\n    \"\"\"Return the subgraph induced on given edge types.\n\n    An edge-type-induced subgraph contains all the edges of the given subset of\n    the edge types of a graph. It also contains all nodes of a particular type\n    if some nodes of the type are incident to these edges.\n    In addition to extracting the subgraph, DGL also copies the features of the\n    extracted nodes and edges to the resulting graph.\n    The copy is *lazy* and incurs data movement only when needed.\n\n    Parameters\n    ----------\n    graph : DGLGraph\n        The graph to extract subgraphs from.\n    etypes : list[str] or list[(str, str, str)]\n        The type names of the edges in the subgraph. The allowed type name\n        formats are:\n\n        * ``(str, str, str)`` for source node type, edge type and destination node type.\n        * or one ``str`` for the edge type name  if the name can uniquely identify a\n          triplet format in the graph.\n\n    Returns\n    -------\n    G : DGLGraph\n        The subgraph.\n\n    Notes\n    -----\n\n    This function discards the batch information. Please use\n    :func:`dgl.DGLGraph.set_batch_num_nodes`\n    and :func:`dgl.DGLGraph.set_batch_num_edges` on the transformed graph\n    to maintain the information.\n\n    Examples\n    --------\n    The following example uses PyTorch backend.\n\n    >>> import dgl\n    >>> import torch\n\n    Instantiate a heterograph.\n\n    >>> g = dgl.heterograph({\n    >>>     ('user', 'plays', 'game'): ([0, 1, 1, 2], [0, 0, 2, 1]),\n    >>>     ('user', 'follows', 'user'): ([0, 1, 1], [1, 2, 2])\n    >>> })\n    >>> # Set edge features\n    >>> g.edges['follows'].data['h'] = torch.tensor([[0.], [1.], [2.]])\n\n    Get subgraphs.\n\n    >>> sub_g = g.edge_type_subgraph(['follows'])\n    >>> sub_g\n    Graph(num_nodes=3, num_edges=3,\n          ndata_schemes={}\n          edata_schemes={'h': Scheme(shape=(1,), dtype=torch.float32)})\n\n    Get the shared edge features.\n\n    >>> sub_g.edges['follows'].data['h']\n    tensor([[0.],\n            [1.],\n            [2.]])\n\n    See Also\n    --------\n    node_type_subgraph\n    \"\"\"\n    etype_ids = [graph.get_etype_id(etype) for etype in etypes]\n    # meta graph is homogeneous graph, still using int64\n    meta_src, meta_dst, _ = graph._graph.metagraph.find_edges(utils.toindex(etype_ids, \"int64\"))\n    rel_graphs = [graph._graph.get_relation_graph(i) for i in etype_ids]\n    meta_src = meta_src.tonumpy()\n    meta_dst = meta_dst.tonumpy()\n    ntypes_invmap = {n: i for i, n in enumerate(set(meta_src) | set(meta_dst))}\n    mapped_meta_src = [ntypes_invmap[v] for v in meta_src]\n    mapped_meta_dst = [ntypes_invmap[v] for v in meta_dst]\n    node_frames = [graph._node_frames[i] for i in ntypes_invmap]\n    edge_frames = [graph._edge_frames[i] for i in etype_ids]\n    induced_ntypes = [graph._ntypes[i] for i in ntypes_invmap]\n    induced_etypes = [graph._etypes[i] for i in etype_ids]   # get the \"name\" of edge type\n    num_nodes_per_induced_type = [graph.number_of_nodes(ntype) for ntype in induced_ntypes]\n\n    metagraph = graph_index.from_edge_list((mapped_meta_src, mapped_meta_dst), True)\n    # num_nodes_per_type should be int64\n    hgidx = heterograph_index.create_heterograph_from_relations(\n        metagraph, rel_graphs, utils.toindex(num_nodes_per_induced_type, \"int64\"))\n    hg = DGLHeteroGraph(hgidx, induced_ntypes, induced_etypes, node_frames, edge_frames)\n    return hg\n\nDGLHeteroGraph.edge_type_subgraph = utils.alias_func(edge_type_subgraph)\n\n#################### Internal functions ####################\n\ndef _create_hetero_subgraph(parent, sgi, induced_nodes, induced_edges, store_ids=True):\n    \"\"\"Internal function to create a subgraph.\n\n    Parameters\n    ----------\n    parent : DGLGraph\n        The parent DGLGraph.\n    sgi : HeteroSubgraphIndex\n        Subgraph object returned by CAPI.\n    induced_nodes : list[Tensor] or None\n        Induced node IDs. Will store it as the dgl.NID ndata unless it\n        is None, which means the induced node IDs are the same as the parent node IDs.\n    induced_edges : list[Tensor] or None\n        Induced edge IDs. Will store it as the dgl.EID ndata unless it\n        is None, which means the induced edge IDs are the same as the parent edge IDs.\n    store_ids : bool\n        If True and induced_nodes is not None, it will store the raw IDs of the extracted\n        nodes in the ``ndata`` of the resulting graph under name ``dgl.NID``.\n        If True and induced_edges is not None, it will store the raw IDs of the extracted\n        edges in the ``edata`` of the resulting graph under name ``dgl.EID``.\n\n    Returns\n    -------\n    DGLGraph\n        Graph\n    \"\"\"\n    node_frames = utils.extract_node_subframes(parent, induced_nodes, store_ids)\n    edge_frames = utils.extract_edge_subframes(parent, induced_edges, store_ids)\n    hsg = DGLHeteroGraph(sgi.graph, parent.ntypes, parent.etypes)\n    utils.set_new_frames(hsg, node_frames=node_frames, edge_frames=edge_frames)\n    return hsg\n\n_init_api(\"dgl.subgraph\")\n", "hunk": "@@ -275,8 +275,9 @@ def edge_subgraph(graph, edges, *, relabel_nodes=True, store_ids=True, **depreca\n     node_subgraph\n     \"\"\"\n     if len(deprecated_kwargs) != 0:\n-        raise DGLError(\"Key word argument preserve_nodes is deprecated. \"\n-                       \"Use relabel_nodes instead.\")\n+        dgl_warning(\n+            \"Key word argument preserve_nodes is deprecated. Use relabel_nodes instead.\")\n+        relabel_nodes = not deprecated_kwargs.get('preserve_nodes')\n     if graph.is_block and relabel_nodes:\n         raise DGLError('Extracting subgraph from a block graph is not allowed.')\n     if not isinstance(edges, Mapping):\n", "comment": "Suggest raise deprecation warning for this release to have a smoother transition.", "ids": ["42402", "b124d1fc169137612ced99ae61d2591484508c5f", "3346537707e8979c8636d6f3475f1d451c841bc1"], "repo": "dmlc/dgl", "ghid": 2929, "old": "     node_subgraph\n     \"\"\"\n     if len(deprecated_kwargs) != 0:\n-        raise DGLError(\"Key word argument preserve_nodes is deprecated. \"\n-                       \"Use relabel_nodes instead.\")\n     if graph.is_block and relabel_nodes:\n         raise DGLError('Extracting subgraph from a block graph is not allowed.')\n     if not isinstance(edges, Mapping):", "new": "     node_subgraph\n     \"\"\"\n     if len(deprecated_kwargs) != 0:\n+        dgl_warning(\n+            \"Key word argument preserve_nodes is deprecated. Use relabel_nodes instead.\")\n+        relabel_nodes = not deprecated_kwargs.get('preserve_nodes')\n     if graph.is_block and relabel_nodes:\n         raise DGLError('Extracting subgraph from a block graph is not allowed.')\n     if not isinstance(edges, Mapping):", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -250,12 +252,12 @@ def forward(self, g):\n     (test_Y == argmax_Y.float()).sum().item() / len(test_Y) * 100))\n \n ###############################################################################\n-# Below is an animation where we plot graphs with the probability a trained model\n-# assigns its ground truth label to it:\n+# The figure here is an animation where you plot graphs with the probability that a trained model\n+# assigns its Amazon SageMaker ground truth label to it.", "oldf": "\"\"\"\n.. currentmodule:: dgl\n\nTutorial: Batched graph classification with DGL\n=====================================\n\n**Author**: `Mufei Li <https://github.com/mufeili>`_,\n`Minjie Wang <https://jermainewang.github.io/>`_,\n`Zheng Zhang <https://shanghai.nyu.edu/academics/faculty/directory/zheng-zhang>`_.\n\nIn this tutorial, you learn how to use DGL to batch multiple graphs of variable size and shape. The \ntutorial also demonstrates training a graph neural network for a simple graph classification task.\n\nGraph classification is an important problem\nwith applications across many fields, such as bioinformatics, chemoinformatics, social\nnetwork analysis, urban computing, and cyber-security. Applying graph neural\nnetworks to this problem has been a popular approach recently. This can be seen in the following reserach references: \n`Ying et al., 2018 <https://arxiv.org/abs/1806.08804>`_,\n`Cangea et al., 2018 <https://arxiv.org/abs/1811.01287>`_,\n`Knyazev et al., 2018 <https://arxiv.org/abs/1811.09595>`_,\n`Bianchi et al., 2019 <https://arxiv.org/abs/1901.01343>`_,\n`Liao et al., 2019 <https://arxiv.org/abs/1901.01484>`_,\n`Gao et al., 2019 <https://openreview.net/forum?id=HJePRoAct7>`_).\n\n\"\"\"\n\n###############################################################################\n# Simple graph classification task\n# --------------------------------\n# In this tutorial, you learn how to perform batched graph classification\n# with DGL. The example task objective is to classify eight types of regular graphs shown here.\n#\n# .. image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/dataset_overview.png\n#     :align: center\n#\n# Implement a synthetic dataset :class:`data.MiniGCDataset` in DGL. The dataset has eight \n# different types of graphs and each class has the same number of graph samples.\n\nfrom dgl.data import MiniGCDataset\nimport matplotlib.pyplot as plt\nimport networkx as nx\n# A dataset with 80 samples, each graph is\n# of size [10, 20]\ndataset = MiniGCDataset(80, 10, 20)\ngraph, label = dataset[0]\nfig, ax = plt.subplots()\nnx.draw(graph.to_networkx(), ax=ax)\nax.set_title('Class: {:d}'.format(label))\nplt.show()\n\n###############################################################################\n# Form a graph mini-batch\n# -----------------------\n# To train neural networks efficiently, a common practice is to batch\n# multiple samples together to form a mini-batch. Batching fixed-shaped tensor\n# inputs is common. For example, batching two images of size 28 x 28\n# gives a tensor of shape 2 x 28 x 28. By contrast, batching graph inputs\n# has two challenges:\n#\n# * Graphs are sparse.\n# * Graphs can have various length. For example, number of nodes and edges.\n#\n# To address this, DGL provides a :func:`dgl.batch` API. It leverages the idea that\n# a batch of graphs can be viewed as a large graph that has many disjointed \n# connected components. Below is a visualization that gives the general idea.\n#\n# .. image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/batch.png\n#     :width: 400pt\n#     :align: center\n#\n# Define the following ``collate`` function to form a mini-batch from a given\n# list of graph and label pairs.\n\nimport dgl\n\ndef collate(samples):\n    # The input `samples` is a list of pairs\n    #  (graph, label).\n    graphs, labels = map(list, zip(*samples))\n    batched_graph = dgl.batch(graphs)\n    return batched_graph, torch.tensor(labels)\n\n###############################################################################\n# The return type of :func:`dgl.batch` is still a graph. In the same way, \n# a batch of tensors is still a tensor. This means that any code that works\n# for one graph immediately works for a batch of graphs. More importantly,\n# because DGL processes messages on all nodes and edges in parallel, this greatly\n# improves efficiency.\n#\n# Graph classifier\n# ----------------\n# Graph classification proceeds as follows.\n#\n# .. image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/graph_classifier.png\n#\n# From a batch of graphs, perform message passing and graph convolution\n# for nodes to communicate with others. After message passing, compute a\n# tensor for graph representation from node (and edge) attributes. This step might \n# be called readout or aggregation. Finally, the graph \n# representations are fed into a classifier :math:`g` to predict the graph labels.\n#\n# Graph convolution\n# -----------------\n# The graph convolution operation is basically the same as that for graph convolutional network (GCN). To learn more, \n# see the GCN `tutorial <https://docs.dgl.ai/tutorials/models/1_gnn/1_gcn.html>`_). The only difference is\n# that we replace :math:`h_{v}^{(l+1)} = \\text{ReLU}\\left(b^{(l)}+\\sum_{u\\in\\mathcal{N}(v)}h_{u}^{(l)}W^{(l)}\\right)` \n# by\n# :math:`h_{v}^{(l+1)} = \\text{ReLU}\\left(b^{(l)}+\\frac{1}{|\\mathcal{N}(v)|}\\sum_{u\\in\\mathcal{N}(v)}h_{u}^{(l)}W^{(l)}\\right)`\n# \n# The replacement of summation by average is to balance nodes with different\n# degrees. This gives a better performance for this experiment.\n#\n# The self edges added in the dataset initialization allows you to\n# include the original node feature :math:`h_{v}^{(l)}` when taking the average.\n\nimport dgl.function as fn\nimport torch\nimport torch.nn as nn\n\n\n# Sends a message of node feature h.\nmsg = fn.copy_src(src='h', out='m')\n\ndef reduce(nodes):\n    \"\"\"Take an average over all neighbor node features hu and use it to\n    overwrite the original node feature.\"\"\"\n    accum = torch.mean(nodes.mailbox['m'], 1)\n    return {'h': accum}\n\nclass NodeApplyModule(nn.Module):\n    \"\"\"Update the node feature hv with ReLU(Whv+b).\"\"\"\n    def __init__(self, in_feats, out_feats, activation):\n        super(NodeApplyModule, self).__init__()\n        self.linear = nn.Linear(in_feats, out_feats)\n        self.activation = activation\n\n    def forward(self, node):\n        h = self.linear(node.data['h'])\n        h = self.activation(h)\n        return {'h' : h}\n\nclass GCN(nn.Module):\n    def __init__(self, in_feats, out_feats, activation):\n        super(GCN, self).__init__()\n        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n\n    def forward(self, g, feature):\n        # Initialize the node features with h.\n        g.ndata['h'] = feature\n        g.update_all(msg, reduce)\n        g.apply_nodes(func=self.apply_mod)\n        return g.ndata.pop('h')\n\n###############################################################################\n# Readout and classification\n# --------------------------\n# For this demonstration, consider initial node features to be their degrees.\n# After two rounds of graph convolution, perform a graph readout by averaging\n# over all node features for each graph in the batch.\n#\n# .. math::\n#\n#    h_g=\\frac{1}{|\\mathcal{V}|}\\sum_{v\\in\\mathcal{V}}h_{v}\n#\n# In DGL, :func:`dgl.mean_nodes` handles this task for a batch of\n# graphs with variable size. You then feed the graph representations into a\n# classifier with one linear layer to obtain pre-softmax logits.\n\nimport torch.nn.functional as F\n\n\nclass Classifier(nn.Module):\n    def __init__(self, in_dim, hidden_dim, n_classes):\n        super(Classifier, self).__init__()\n\n        self.layers = nn.ModuleList([\n            GCN(in_dim, hidden_dim, F.relu),\n            GCN(hidden_dim, hidden_dim, F.relu)])\n        self.classify = nn.Linear(hidden_dim, n_classes)\n\n    def forward(self, g):\n        # For undirected graphs, in_degree is the same as\n        # out_degree.\n        h = g.in_degrees().view(-1, 1).float()\n        for conv in self.layers:\n            h = conv(g, h)\n        g.ndata['h'] = h\n        hg = dgl.mean_nodes(g, 'h')\n        return self.classify(hg)\n\n###############################################################################\n# Setup and training\n# ------------------\n# Create a synthetic dataset of :math:`400` graphs with :math:`10` ~\n# :math:`20` nodes. :math:`320` graphs constitute a training set and\n# :math:`80` graphs constitute a test set.\n\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Create training and test sets.\ntrainset = MiniGCDataset(320, 10, 20)\ntestset = MiniGCDataset(80, 10, 20)\n# Use PyTorch's DataLoader and the collate function\n# defined before.\ndata_loader = DataLoader(trainset, batch_size=32, shuffle=True,\n                         collate_fn=collate)\n\n# Create model\nmodel = Classifier(1, 256, trainset.num_classes)\nloss_func = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nmodel.train()\n\nepoch_losses = []\nfor epoch in range(80):\n    epoch_loss = 0\n    for iter, (bg, label) in enumerate(data_loader):\n        prediction = model(bg)\n        loss = loss_func(prediction, label)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.detach().item()\n    epoch_loss /= (iter + 1)\n    print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n    epoch_losses.append(epoch_loss)\n\n###############################################################################\n# The learning curve of a run is presented below.\n\nplt.title('cross entropy averaged over minibatches')\nplt.plot(epoch_losses)\nplt.show()\n\n###############################################################################\n# The trained model is evaluated on the test set created. To deploy\n# the tutorial, restrict the running time to get a higher\n# accuracy (:math:`80` % ~ :math:`90` %) than the ones printed below.\n\nmodel.eval()\n# Convert a list of tuples to two lists\ntest_X, test_Y = map(list, zip(*testset))\ntest_bg = dgl.batch(test_X)\ntest_Y = torch.tensor(test_Y).float().view(-1, 1)\nprobs_Y = torch.softmax(model(test_bg), 1)\nsampled_Y = torch.multinomial(probs_Y, 1)\nargmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\nprint('Accuracy of sampled predictions on the test set: {:.4f}%'.format(\n    (test_Y == sampled_Y.float()).sum().item() / len(test_Y) * 100))\nprint('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n    (test_Y == argmax_Y.float()).sum().item() / len(test_Y) * 100))\n\n###############################################################################\n# The figure here is an animation where you plot graphs with the probability that a trained model\n# assigns its Amazon SageMaker ground truth label to it.\n#\n# .. image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/test_eval4.gif\n#\n# To understand the node and graph representations that a trained model learned,\n# we use `t-SNE, <https://lvdmaaten.github.io/tsne/>`_ for dimensionality reduction\n# and visualization.\n#\n# .. image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/tsne_node2.png\n#     :align: center\n#\n# .. image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/tsne_graph2.png\n#     :align: center\n#\n# The two small figures on the top separately visualize node representations after one and two\n# layers of graph convolution. The figure on the bottom visualizes\n# the pre-softmax logits for graphs as graph representations.\n#\n# While the visualization does suggest some clustering effects of the node features,\n# you would not expect a perfect result. Node degrees are deterministic for\n# these node features. The graph features are improved when separated.\n#\n# What's next?\n# ------------\n# Graph classification with graph neural networks is still a new field.\n# It's waiting for people to bring more exciting discoveries. The work requires \n# mapping different graphs to different embeddings, while preserving\n# their structural similarity in the embedding space. To learn more about it, see \n# `How Powerful Are Graph Neural Networks? <https://arxiv.org/abs/1810.00826>`_ a research paper  \n# published for the International Conference on Learning Representations 2019.\n#\n# For more examples about batched graph processing, see the following:\n#\n# * Tutorials for `Tree LSTM <https://docs.dgl.ai/tutorials/models/2_small_graph/3_tree-lstm.html>`_ and `Deep Generative Models of Graphs <https://docs.dgl.ai/tutorials/models/3_generative_model/5_dgmg.html>`_\n# * An example implementation of `Junction Tree VAE <https://github.com/dmlc/dgl/tree/master/examples/pytorch/jtnn>`_\n", "hunk": "@@ -252,7 +252,7 @@ print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n     (test_Y == argmax_Y.float()).sum().item() / len(test_Y) * 100))\n \n ###############################################################################\n-# The figure here is an animation where you plot graphs with the probability that a trained model\n+# The animation here plots the probability that a trained model predicts the correct graph type.\n # assigns its Amazon SageMaker ground truth label to it.\n #\n # .. image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/test_eval4.gif\n", "comment": "As @yzh119 has explained, the label is not from Amazon SageMaker. It is a categorical variable from the dataset itself indicating the graph type of each sample. How about \"The animation here plots the probability a trained model predicts the correct graph type.\"?", "ids": ["35851", "61425a544573dbe22c8c204044aa2c93b362df0f", "71c892e48e2d9d6556778dac87a155325909dc58"], "repo": "dmlc/dgl", "ghid": 987, "old": "     (test_Y == argmax_Y.float()).sum().item() / len(test_Y) * 100))\n ###############################################################################\n-# The figure here is an animation where you plot graphs with the probability that a trained model\n # assigns its Amazon SageMaker ground truth label to it.\n #\n # .. image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/test_eval4.gif", "new": "     (test_Y == argmax_Y.float()).sum().item() / len(test_Y) * 100))\n ###############################################################################\n+# The animation here plots the probability that a trained model predicts the correct graph type.\n # assigns its Amazon SageMaker ground truth label to it.\n #\n # .. image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/test_eval4.gif", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -214,6 +214,14 @@ def configure(self, updated):\n             key_size=ctx.options.key_size,\n             passphrase=ctx.options.cert_passphrase.encode(\"utf8\") if ctx.options.cert_passphrase else None,\n         )\n+        if self.certstore.default_ca.has_expired():\n+            ctx.log.warn(\n+                \"The mitmproxy certificate authority has expired!\\n\"\n+                \"Please delete all CA-related files in your ~/.mitmproxy folder.\\n\"\n+                \"The CA will be regenerated automatically after restarting mitmproxy.\\n\"\n+                \"Then make sure all your clients have the new CA installed.\",", "oldf": "import os\nfrom typing import List, Optional, Tuple, TypedDict, Any\n\nfrom OpenSSL import SSL, crypto\nfrom mitmproxy import certs, ctx, exceptions\nfrom mitmproxy.net import tls as net_tls\nfrom mitmproxy.options import CONF_BASENAME\nfrom mitmproxy.proxy import context\nfrom mitmproxy.proxy.layers import tls\n\n# We manually need to specify this, otherwise OpenSSL may select a non-HTTP2 cipher by default.\n# https://ssl-config.mozilla.org/#config=old\nDEFAULT_CIPHERS = (\n    'ECDHE-ECDSA-AES128-GCM-SHA256', 'ECDHE-RSA-AES128-GCM-SHA256', 'ECDHE-ECDSA-AES256-GCM-SHA384',\n    'ECDHE-RSA-AES256-GCM-SHA384', 'ECDHE-ECDSA-CHACHA20-POLY1305', 'ECDHE-RSA-CHACHA20-POLY1305',\n    'DHE-RSA-AES128-GCM-SHA256', 'DHE-RSA-AES256-GCM-SHA384', 'DHE-RSA-CHACHA20-POLY1305', 'ECDHE-ECDSA-AES128-SHA256',\n    'ECDHE-RSA-AES128-SHA256', 'ECDHE-ECDSA-AES128-SHA', 'ECDHE-RSA-AES128-SHA', 'ECDHE-ECDSA-AES256-SHA384',\n    'ECDHE-RSA-AES256-SHA384', 'ECDHE-ECDSA-AES256-SHA', 'ECDHE-RSA-AES256-SHA', 'DHE-RSA-AES128-SHA256',\n    'DHE-RSA-AES256-SHA256', 'AES128-GCM-SHA256', 'AES256-GCM-SHA384', 'AES128-SHA256', 'AES256-SHA256', 'AES128-SHA',\n    'AES256-SHA', 'DES-CBC3-SHA'\n)\n\n\nclass AppData(TypedDict):\n    server_alpn: Optional[bytes]\n    http2: bool\n\n\ndef alpn_select_callback(conn: SSL.Connection, options: List[bytes]) -> Any:\n    app_data: AppData = conn.get_app_data()\n    server_alpn = app_data[\"server_alpn\"]\n    http2 = app_data[\"http2\"]\n    if server_alpn and server_alpn in options:\n        return server_alpn\n    http_alpns = tls.HTTP_ALPNS if http2 else tls.HTTP1_ALPNS\n    for alpn in options:  # client sends in order of preference, so we are nice and respect that.\n        if alpn in http_alpns:\n            return alpn\n    else:\n        return SSL.NO_OVERLAPPING_PROTOCOLS\n\n\nclass TlsConfig:\n    \"\"\"\n    This addon supplies the proxy core with the desired OpenSSL connection objects to negotiate TLS.\n    \"\"\"\n    certstore: certs.CertStore = None  # type: ignore\n\n    # TODO: We should support configuring TLS 1.3 cipher suites (https://github.com/mitmproxy/mitmproxy/issues/4260)\n    # TODO: We should re-use SSL.Context options here, if only for TLS session resumption.\n    #       This may require patches to pyOpenSSL, as some functionality is only exposed on contexts.\n    # TODO: This addon should manage the following options itself, which are current defined in mitmproxy/options.py:\n    #  - upstream_cert\n    #  - add_upstream_certs_to_client_chain\n    #  - ciphers_client\n    #  - ciphers_server\n    #  - key_size\n    #  - certs\n    #  - cert_passphrase\n    #  - ssl_verify_upstream_trusted_ca\n    #  - ssl_verify_upstream_trusted_confdir\n\n    def load(self, loader):\n        loader.add_option(\n            name=\"tls_version_client_min\",\n            typespec=str,\n            default=net_tls.DEFAULT_MIN_VERSION.name,\n            choices=[x.name for x in net_tls.Version],\n            help=f\"Set the minimum TLS version for client connections.\",\n        )\n        loader.add_option(\n            name=\"tls_version_client_max\",\n            typespec=str,\n            default=net_tls.DEFAULT_MAX_VERSION.name,\n            choices=[x.name for x in net_tls.Version],\n            help=f\"Set the maximum TLS version for client connections.\",\n        )\n        loader.add_option(\n            name=\"tls_version_server_min\",\n            typespec=str,\n            default=net_tls.DEFAULT_MIN_VERSION.name,\n            choices=[x.name for x in net_tls.Version],\n            help=f\"Set the minimum TLS version for server connections.\",\n        )\n        loader.add_option(\n            name=\"tls_version_server_max\",\n            typespec=str,\n            default=net_tls.DEFAULT_MAX_VERSION.name,\n            choices=[x.name for x in net_tls.Version],\n            help=f\"Set the maximum TLS version for server connections.\",\n        )\n\n    def tls_clienthello(self, tls_clienthello: tls.ClientHelloData):\n        conn_context = tls_clienthello.context\n        only_non_http_alpns = (\n                conn_context.client.alpn_offers and\n                all(x not in tls.HTTP_ALPNS for x in conn_context.client.alpn_offers)\n        )\n        tls_clienthello.establish_server_tls_first = conn_context.server.tls and (\n                ctx.options.connection_strategy == \"eager\" or\n                ctx.options.add_upstream_certs_to_client_chain or\n                ctx.options.upstream_cert and (\n                        only_non_http_alpns or\n                        not conn_context.client.sni\n                )\n        )\n\n    def tls_start(self, tls_start: tls.TlsStartData):\n        if tls_start.conn == tls_start.context.client:\n            self.create_client_proxy_ssl_conn(tls_start)\n        else:\n            self.create_proxy_server_ssl_conn(tls_start)\n\n    def create_client_proxy_ssl_conn(self, tls_start: tls.TlsStartData) -> None:\n        client: context.Client = tls_start.context.client\n        server: context.Server = tls_start.context.server\n\n        cert, key, chain_file = self.get_cert(tls_start.context)\n\n        if not client.cipher_list and ctx.options.ciphers_client:\n            client.cipher_list = ctx.options.ciphers_client.split(\":\")\n        # don't assign to client.cipher_list, doesn't need to be stored.\n        cipher_list = client.cipher_list or DEFAULT_CIPHERS\n\n        ssl_ctx = net_tls.create_client_proxy_context(\n            min_version=net_tls.Version[ctx.options.tls_version_client_min],\n            max_version=net_tls.Version[ctx.options.tls_version_client_max],\n            cipher_list=cipher_list,\n            cert=cert,\n            key=key,\n            chain_file=chain_file,\n            request_client_cert=False,\n            alpn_select_callback=alpn_select_callback,\n            extra_chain_certs=server.certificate_list,\n            dhparams=self.certstore.dhparams,\n        )\n        tls_start.ssl_conn = SSL.Connection(ssl_ctx)\n        tls_start.ssl_conn.set_app_data(AppData(\n            server_alpn=server.alpn,\n            http2=ctx.options.http2,\n        ))\n        tls_start.ssl_conn.set_accept_state()\n\n    def create_proxy_server_ssl_conn(self, tls_start: tls.TlsStartData) -> None:\n        client: context.Client = tls_start.context.client\n        server: context.Server = tls_start.context.server\n        assert server.address\n\n        if ctx.options.ssl_insecure:\n            verify = net_tls.Verify.VERIFY_NONE\n        else:\n            verify = net_tls.Verify.VERIFY_PEER\n\n        if server.sni is True:\n            server.sni = client.sni or server.address[0].encode()\n        sni = server.sni or None  # make sure that false-y values are None\n\n        if not server.alpn_offers:\n            if client.alpn_offers:\n                if ctx.options.http2:\n                    server.alpn_offers = tuple(client.alpn_offers)\n                else:\n                    server.alpn_offers = tuple(x for x in client.alpn_offers if x != b\"h2\")\n            elif client.tls_established:\n                # We would perfectly support HTTP/1 -> HTTP/2, but we want to keep things on the same protocol version.\n                # There are some edge cases where we want to mirror the regular server's behavior accurately,\n                # for example header capitalization.\n                server.alpn_offers = []\n            elif ctx.options.http2:\n                server.alpn_offers = tls.HTTP_ALPNS\n            else:\n                server.alpn_offers = tls.HTTP1_ALPNS\n\n        if not server.cipher_list and ctx.options.ciphers_server:\n            server.cipher_list = ctx.options.ciphers_server.split(\":\")\n        # don't assign to client.cipher_list, doesn't need to be stored.\n        cipher_list = server.cipher_list or DEFAULT_CIPHERS\n\n        client_cert: Optional[str] = None\n        if ctx.options.client_certs:\n            client_certs = os.path.expanduser(ctx.options.client_certs)\n            if os.path.isfile(client_certs):\n                client_cert = client_certs\n            else:\n                server_name: str = (server.sni or server.address[0].encode(\"idna\")).decode()\n                p = os.path.join(client_certs, f\"{server_name}.pem\")\n                if os.path.isfile(p):\n                    client_cert = p\n\n        ssl_ctx = net_tls.create_proxy_server_context(\n            min_version=net_tls.Version[ctx.options.tls_version_client_min],\n            max_version=net_tls.Version[ctx.options.tls_version_client_max],\n            cipher_list=cipher_list,\n            verify=verify,\n            sni=sni,\n            ca_path=ctx.options.ssl_verify_upstream_trusted_confdir,\n            ca_pemfile=ctx.options.ssl_verify_upstream_trusted_ca,\n            client_cert=client_cert,\n            alpn_protos=server.alpn_offers,\n        )\n\n        tls_start.ssl_conn = SSL.Connection(ssl_ctx)\n        tls_start.ssl_conn.set_tlsext_host_name(server.sni)\n        tls_start.ssl_conn.set_connect_state()\n\n    def configure(self, updated):\n        if \"confdir\" not in updated and \"certs\" not in updated:\n            return\n\n        certstore_path = os.path.expanduser(ctx.options.confdir)\n        self.certstore = certs.CertStore.from_store(\n            path=certstore_path,\n            basename=CONF_BASENAME,\n            key_size=ctx.options.key_size,\n            passphrase=ctx.options.cert_passphrase.encode(\"utf8\") if ctx.options.cert_passphrase else None,\n        )\n        if self.certstore.default_ca.has_expired():\n            ctx.log.warn(\n                \"The mitmproxy certificate authority has expired!\\n\"\n                \"Please delete all CA-related files in your ~/.mitmproxy folder.\\n\"\n                \"The CA will be regenerated automatically after restarting mitmproxy.\\n\"\n                \"Then make sure all your clients have the new CA installed.\",\n            )\n\n        for certspec in ctx.options.certs:\n            parts = certspec.split(\"=\", 1)\n            if len(parts) == 1:\n                parts = [\"*\", parts[0]]\n\n            cert = os.path.expanduser(parts[1])\n            if not os.path.exists(cert):\n                raise exceptions.OptionsError(f\"Certificate file does not exist: {cert}\")\n            try:\n                self.certstore.add_cert_file(\n                    parts[0],\n                    cert,\n                    passphrase=ctx.options.cert_passphrase.encode(\"utf8\") if ctx.options.cert_passphrase else None,\n                )\n            except crypto.Error as e:\n                raise exceptions.OptionsError(f\"Invalid certificate format: {cert}\") from e\n\n    def get_cert(self, conn_context: context.Context) -> Tuple[certs.Cert, SSL.PKey, str]:\n        \"\"\"\n        This function determines the Common Name (CN), Subject Alternative Names (SANs) and Organization Name\n        our certificate should have and then fetches a matching cert from the certstore.\n        \"\"\"\n        altnames: List[bytes] = []\n        organization: Optional[bytes] = None\n\n        # Use upstream certificate if available.\n        if conn_context.server.certificate_list:\n            upstream_cert = conn_context.server.certificate_list[0]\n            if upstream_cert.cn:\n                altnames.append(upstream_cert.cn)\n            altnames.extend(upstream_cert.altnames)\n            if upstream_cert.organization:\n                organization = upstream_cert.organization\n\n        # Add SNI. If not available, try the server address as well.\n        if conn_context.client.sni:\n            altnames.append(conn_context.client.sni)\n        elif conn_context.server.address:\n            altnames.append(conn_context.server.address[0].encode(\"idna\"))\n\n        # As a last resort, add *something* so that we have a certificate to serve.\n        if not altnames:\n            altnames.append(b\"mitmproxy\")\n\n        # only keep first occurrence of each hostname\n        altnames = list(dict.fromkeys(altnames))\n\n        # RFC 2818: If a subjectAltName extension of type dNSName is present, that MUST be used as the identity.\n        # In other words, the Common Name is irrelevant then.\n        return self.certstore.get_cert(altnames[0], altnames, organization)\n", "hunk": "@@ -219,7 +219,7 @@ class TlsConfig:\n                 \"The mitmproxy certificate authority has expired!\\n\"\n                 \"Please delete all CA-related files in your ~/.mitmproxy folder.\\n\"\n                 \"The CA will be regenerated automatically after restarting mitmproxy.\\n\"\n-                \"Then make sure all your clients have the new CA installed.\",\n+                \"See https://docs.mitmproxy.org/stable/concepts-certificates/ for additional help.\",\n             )\n \n         for certspec in ctx.options.certs:\n", "comment": "Let's point them to the docs to use mitm.it et al.", "ids": ["15343", "1655f54817d7ffcd3efdf20d773486d1e60a7b68", "70f1d173e236f5cfd8c9f7d82fcb8ad197f28fa7"], "repo": "mitmproxy/mitmproxy", "ghid": 4372, "old": "                 \"The mitmproxy certificate authority has expired!\\n\"\n                 \"Please delete all CA-related files in your ~/.mitmproxy folder.\\n\"\n                 \"The CA will be regenerated automatically after restarting mitmproxy.\\n\"\n-                \"Then make sure all your clients have the new CA installed.\",\n             )\n         for certspec in ctx.options.certs:", "new": "                 \"The mitmproxy certificate authority has expired!\\n\"\n                 \"Please delete all CA-related files in your ~/.mitmproxy folder.\\n\"\n                 \"The CA will be regenerated automatically after restarting mitmproxy.\\n\"\n+                \"See https://docs.mitmproxy.org/stable/concepts-certificates/ for additional help.\",\n             )\n         for certspec in ctx.options.certs:", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -258,27 +258,32 @@ static EVP_PKEY* load_private_key(\n         SecurityException& exception,\n         PKIDH& pkidh)\n {\n+    EVP_PKEY* key = nullptr;\n+\n     if (file.size() >= 7 && file.compare(0, 7, \"file://\") == 0)\n     {\n-        return detail::FileProvider::load_private_key(certificate, file, password, exception);\n+        key = detail::FileProvider::load_private_key(certificate, file, password, exception);\n     }\n     else if (file.size() >= 7 && file.compare(0, 7, \"pkcs11:\") == 0)\n     {\n-#if HAVE_LIBP11\n         if (!pkidh.pkcs11_provider)\n         {\n             pkidh.pkcs11_provider.reset(new detail::Pkcs11Provider());\n         }\n-        return pkidh.pkcs11_provider->load_private_key(certificate, file, password, exception);\n-#else  // HAVE_LIBP11\n-        static_cast<void>(pkidh);\n-        exception = _SecurityException_(std::string(\"PKCS11 URIs require libp11 \") + file);\n-        return nullptr;\n-#endif // HAVE_LIBP11\n+\n+        key = pkidh.pkcs11_provider->load_private_key(certificate, file, password, exception);\n+\n+        if ( nullptr == key )\n+        {\n+            exception = _SecurityException_(std::string(\"PKCS11 URIs require libp11 \") + file);\n+        }", "oldf": "// Copyright 2016 Proyectos y Sistemas de Mantenimiento SL (eProsima).\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n/*!\n * @file PKIDH.cpp\n */\n\n#include <security/authentication/PKIDH.h>\n#include <security/authentication/PKIIdentityHandle.h>\n#include <fastdds/rtps/security/logging/Logging.h>\n#include <fastdds/dds/log/Log.hpp>\n#include <fastdds/rtps/messages/CDRMessage.h>\n#include <fastdds/rtps/builtin/data/ParticipantProxyData.h>\n\n#include <openssl/opensslv.h>\n\n#include <fastdds/core/policy/ParameterList.hpp>\n\n#if OPENSSL_VERSION_NUMBER >= 0x10100000L\n#define IS_OPENSSL_1_1 1\n#define OPENSSL_CONST const\n#else\n#define IS_OPENSSL_1_1 0\n#define OPENSSL_CONST\n#endif // if OPENSSL_VERSION_NUMBER >= 0x10100000L\n\n#if OPENSSL_VERSION_NUMBER >= 0x10101040L\n#define IS_OPENSSL_1_1_1d 1\n#else\n#define IS_OPENSSL_1_1_1d 0\n#endif // if OPENSSL_VERSION_NUMBER >= 0x10101040L\n\n#include <openssl/pem.h>\n#include <openssl/err.h>\n#include <openssl/obj_mac.h>\n\n#include <security/artifact_providers/FileProvider.hpp>\n#include <security/artifact_providers/Pkcs11Provider.hpp>\n\n#include <cassert>\n#include <algorithm>\n\n#define S1(x) #x\n#define S2(x) S1(x)\n#define LOCATION \" (\" __FILE__ \":\" S2(__LINE__) \")\"\n#define _SecurityException_(str) SecurityException(std::string(str) + LOCATION)\n\nusing namespace eprosima::fastrtps;\nusing namespace eprosima::fastrtps::rtps;\nusing namespace eprosima::fastrtps::rtps::security;\n\nusing ParameterList = eprosima::fastdds::dds::ParameterList;\n\nstatic const unsigned char* BN_deserialize_raw(\n        BIGNUM** bn,\n        const unsigned char* raw_pointer,\n        size_t length,\n        SecurityException& exception)\n{\n    BIGNUM* bnn = BN_new();\n\n    if (bnn != nullptr)\n    {\n        if (BN_bin2bn(raw_pointer, static_cast<int>(length), bnn) != nullptr)\n        {\n            *bn = bnn;\n            return raw_pointer + length;\n        }\n        else\n        {\n            exception = _SecurityException_(\"Cannot deserialize DH\");\n        }\n\n        BN_free(bnn);\n    }\n    else\n    {\n        exception = _SecurityException_(\"OpenSSL library cannot create bignum\");\n    }\n\n    return nullptr;\n}\n\nstatic bool get_signature_algorithm(\n        X509* certificate,\n        std::string& signature_algorithm,\n        SecurityException& exception)\n{\n    bool returnedValue = false;\n    BUF_MEM* ptr = nullptr;\n    OPENSSL_CONST X509_ALGOR* sigalg = nullptr;\n    OPENSSL_CONST ASN1_BIT_STRING* sig = nullptr;\n\n    BIO* out = BIO_new(BIO_s_mem());\n\n    if (out != nullptr)\n    {\n        X509_get0_signature(&sig, &sigalg, certificate);\n\n        if (sigalg != nullptr)\n        {\n            if (i2a_ASN1_OBJECT(out, sigalg->algorithm) > 0)\n            {\n                BIO_get_mem_ptr(out, &ptr);\n\n                if (ptr != nullptr)\n                {\n                    if (strncmp(ptr->data, \"ecdsa-with-SHA256\", ptr->length) == 0)\n                    {\n                        signature_algorithm = ECDSA_SHA256;\n                        returnedValue = true;\n                    }\n                    else if (strncmp(ptr->data, \"sha256WithRSAEncryption\", ptr->length) == 0)\n                    {\n                        signature_algorithm = RSA_SHA256;\n                        returnedValue = true;\n                    }\n                    else if (strncmp(ptr->data, \"sha1WithRSAEncryption\", ptr->length) == 0)\n                    {\n                        signature_algorithm = RSA_SHA256;\n                        returnedValue = true;\n                    }\n                }\n                else\n                {\n                    exception = _SecurityException_(\"OpenSSL library cannot retrieve mem ptr\");\n                }\n            }\n        }\n        else\n        {\n            exception = _SecurityException_(\"OpenSSL library cannot write cert\");\n        }\n\n        BIO_free(out);\n    }\n    else\n    {\n        exception = _SecurityException_(\"OpenSSL library cannot allocate mem\");\n    }\n\n    return returnedValue;\n}\n\n// Auxiliary functions\nstatic X509_STORE* load_identity_ca(\n        const std::string& identity_ca,\n        bool& there_are_crls,\n        std::string& ca_sn,\n        std::string& ca_algo,\n        SecurityException& exception)\n{\n    if (identity_ca.size() >= 7 && identity_ca.compare(0, 7, \"file://\") == 0)\n    {\n        return detail::FileProvider::load_ca(identity_ca, there_are_crls, ca_sn, ca_algo, get_signature_algorithm,\n                       exception);\n    }\n\n    exception = _SecurityException_(std::string(\"Unsupported URI format \") + identity_ca);\n    return nullptr;\n}\n\nstatic X509* load_certificate(\n        const std::string& identity_cert,\n        SecurityException& exception)\n{\n    if (identity_cert.size() >= 7 && identity_cert.compare(0, 7, \"file://\") == 0)\n    {\n        return detail::FileProvider::load_certificate(identity_cert, exception);\n    }\n\n    exception = _SecurityException_(std::string(\"Unsupported URI format \") + identity_cert);\n    return nullptr;\n}\n\nstatic X509* load_certificate(\n        const std::vector<uint8_t>& data)\n{\n    X509* returnedValue = nullptr;\n\n    if (data.size() <= static_cast<size_t>(std::numeric_limits<int>::max()))\n    {\n        BIO* cid = BIO_new_mem_buf(data.data(), static_cast<int>(data.size()));\n\n        if (cid != nullptr)\n        {\n            returnedValue = PEM_read_bio_X509_AUX(cid, NULL, NULL, NULL);\n            BIO_free(cid);\n        }\n    }\n\n    return returnedValue;\n}\n\nstatic bool verify_certificate(\n        X509_STORE* store,\n        X509* cert,\n        const bool there_are_crls)\n{\n    assert(store);\n    assert(cert);\n\n    bool returnedValue = false;\n\n    X509_STORE_CTX* ctx = X509_STORE_CTX_new();\n\n    unsigned long flags = there_are_crls ? X509_V_FLAG_CRL_CHECK : 0;\n    if (X509_STORE_CTX_init(ctx, store, cert, NULL) > 0)\n    {\n        X509_STORE_CTX_set_flags(ctx, flags | /*X509_V_FLAG_X509_STRICT |*/\n                X509_V_FLAG_CHECK_SS_SIGNATURE | X509_V_FLAG_POLICY_CHECK);\n\n        if (X509_verify_cert(ctx) > 0)\n        {\n            returnedValue = true;\n        }\n        else\n        {\n            int errorCode = X509_STORE_CTX_get_error(ctx);\n            if (errorCode == X509_V_OK)\n            {\n                logWarning(SECURITY_AUTHENTICATION, \"Invalidation error of certificate, but no error code returned.\");\n            }\n            else\n            {\n                logWarning(SECURITY_AUTHENTICATION, \"Invalidation error of certificate  (\" << X509_verify_cert_error_string(\n                            errorCode) << \")\");\n            }\n        }\n\n        X509_STORE_CTX_cleanup(ctx);\n    }\n    else\n    {\n        logWarning(SECURITY_AUTHENTICATION, \"Cannot init context for verifying certificate\");\n    }\n\n    X509_STORE_CTX_free(ctx);\n\n    return returnedValue;\n}\n\nstatic EVP_PKEY* load_private_key(\n        X509* certificate,\n        const std::string& file,\n        const std::string& password,\n        SecurityException& exception,\n        PKIDH& pkidh)\n{\n    EVP_PKEY* key = nullptr;\n\n    if (file.size() >= 7 && file.compare(0, 7, \"file://\") == 0)\n    {\n        key = detail::FileProvider::load_private_key(certificate, file, password, exception);\n    }\n    else if (file.size() >= 7 && file.compare(0, 7, \"pkcs11:\") == 0)\n    {\n        if (!pkidh.pkcs11_provider)\n        {\n            pkidh.pkcs11_provider.reset(new detail::Pkcs11Provider());\n        }\n\n        key = pkidh.pkcs11_provider->load_private_key(certificate, file, password, exception);\n\n        if ( nullptr == key )\n        {\n            exception = _SecurityException_(std::string(\"PKCS11 URIs require libp11 \") + file);\n        }\n    }\n    else\n    {\n        exception = _SecurityException_(std::string(\"Unsupported URI format \") + file);\n    }\n\n    return key;\n}\n\nstatic bool store_certificate_in_buffer(\n        X509* certificate,\n        BUF_MEM** ptr,\n        SecurityException& exception)\n{\n    bool returnedValue = false;\n\n    BIO* out = BIO_new(BIO_s_mem());\n\n    if (out != nullptr)\n    {\n        if (PEM_write_bio_X509(out, certificate) > 0 )\n        {\n            BIO_get_mem_ptr(out, ptr);\n\n            if (*ptr != nullptr)\n            {\n                (void)BIO_set_close(out, BIO_NOCLOSE);\n                returnedValue = true;\n            }\n            else\n            {\n                exception = _SecurityException_(\"OpenSSL library cannot retrieve mem ptr\");\n            }\n        }\n        else\n        {\n            exception = _SecurityException_(\"OpenSSL library cannot write cert\");\n        }\n\n        BIO_free(out);\n    }\n    else\n    {\n        exception = _SecurityException_(\"OpenSSL library cannot allocate mem\");\n    }\n\n    return returnedValue;\n}\n\nstatic bool sign_sha256(\n        EVP_PKEY* private_key,\n        const unsigned char* data,\n        const size_t data_length,\n        std::vector<uint8_t>& signature,\n        SecurityException& exception)\n{\n    assert(private_key);\n    assert(data);\n\n    bool returnedValue = false;\n\n#if IS_OPENSSL_1_1\n    EVP_MD_CTX* ctx = EVP_MD_CTX_new();\n#else\n    EVP_MD_CTX* ctx = (EVP_MD_CTX*)malloc(sizeof(EVP_MD_CTX));\n#endif // if IS_OPENSSL_1_1\n    EVP_MD_CTX_init(ctx);\n    EVP_PKEY_CTX* pkey;\n\n    auto md = EVP_sha256();\n    if (EVP_DigestSignInit(ctx, &pkey, md, NULL, private_key) == 1)\n    {\n        // TODO (Miguel) don't do this for ECDSA\n        EVP_PKEY_CTX_set_rsa_padding(pkey, RSA_PKCS1_PSS_PADDING);\n        EVP_PKEY_CTX_set_rsa_mgf1_md(pkey, md);\n        EVP_PKEY_CTX_set_rsa_pss_saltlen(pkey, -1);\n\n        if (EVP_DigestSignUpdate(ctx, data, data_length) == 1)\n        {\n            size_t length = 0;\n            if (EVP_DigestSignFinal(ctx, NULL, &length) == 1 && length > 0)\n            {\n                signature.resize(length);\n\n                if (EVP_DigestSignFinal(ctx, signature.data(), &length) ==  1)\n                {\n                    signature.resize(length);\n                    returnedValue = true;\n                }\n                else\n                {\n                    exception =\n                            _SecurityException_(std::string(\"Cannot finish signature (\") +\n                                    std::to_string(ERR_get_error()) + std::string(\")\"));\n                }\n            }\n            else\n            {\n                exception =\n                        _SecurityException_(std::string(\"Cannot retrieve signature length (\") +\n                                std::to_string(ERR_get_error()) + std::string(\")\"));\n            }\n        }\n        else\n        {\n            exception = _SecurityException_(std::string(\"Cannot sign data (\") + std::to_string(\n                                ERR_get_error()) + std::string(\")\"));\n        }\n    }\n    else\n    {\n        exception = _SecurityException_(std::string(\"Cannot init signature (\") + std::to_string(\n                            ERR_get_error()) + std::string(\")\"));\n    }\n\n#if IS_OPENSSL_1_1\n    EVP_MD_CTX_free(ctx);\n#else\n    EVP_MD_CTX_cleanup(ctx);\n    free(ctx);\n#endif // if IS_OPENSSL_1_1\n\n    return returnedValue;\n}\n\nstatic bool check_sign_sha256(\n        X509* certificate,\n        const unsigned char* data,\n        const size_t data_length,\n        const std::vector<uint8_t>& signature,\n        SecurityException& exception)\n{\n    assert(certificate);\n    assert(data);\n\n    bool returnedValue = false;\n\n    EVP_MD_CTX* ctx =\n#if IS_OPENSSL_1_1\n            EVP_MD_CTX_new();\n#else\n            (EVP_MD_CTX*)malloc(sizeof(EVP_MD_CTX));\n#endif // if IS_OPENSSL_1_1\n    EVP_MD_CTX_init(ctx);\n\n    EVP_PKEY* pubkey = X509_get_pubkey(certificate);\n\n    if (pubkey != nullptr)\n    {\n        auto md = EVP_sha256();\n        EVP_PKEY_CTX* pkey;\n        if (EVP_DigestVerifyInit(ctx, &pkey, md, NULL, pubkey) == 1)\n        {\n            // TODO (Miguel) don't do this for ECDSA\n            EVP_PKEY_CTX_set_rsa_padding(pkey, RSA_PKCS1_PSS_PADDING);\n            EVP_PKEY_CTX_set_rsa_mgf1_md(pkey, md);\n            EVP_PKEY_CTX_set_rsa_pss_saltlen(pkey, -2);\n\n            if (EVP_DigestVerifyUpdate(ctx, data, data_length) == 1)\n            {\n                if (EVP_DigestVerifyFinal(ctx, signature.data(), signature.size()) == 1)\n                {\n                    returnedValue = true;\n                }\n                else\n                {\n                    logWarning(SECURITY_AUTHENTICATION, \"Signature verification error (\" << ERR_get_error() << \")\");\n                }\n            }\n            else\n            {\n                exception =\n                        _SecurityException_(std::string(\"Cannot update signature check (\") +\n                                std::to_string(ERR_get_error()) + std::string(\")\"));\n            }\n\n        }\n        else\n        {\n            exception =\n                    _SecurityException_(std::string(\"Cannot init signature check (\") + std::to_string(\n                                ERR_get_error()) + std::string(\")\"));\n        }\n\n        EVP_PKEY_free(pubkey);\n    }\n    else\n    {\n        exception = _SecurityException_(\"Cannot get public key from certificate\");\n    }\n\n#if IS_OPENSSL_1_1\n    EVP_MD_CTX_free(ctx);\n#else\n    EVP_MD_CTX_cleanup(ctx);\n    free(ctx);\n#endif // if IS_OPENSSL_1_1\n\n    return returnedValue;\n}\n\nstatic X509_CRL* load_crl(\n        const std::string& identity_crl,\n        SecurityException& exception)\n{\n    if (identity_crl.size() >= 7 && identity_crl.compare(0, 7, \"file://\") == 0)\n    {\n        return detail::FileProvider::load_crl(identity_crl, exception);\n    }\n\n    exception = _SecurityException_(std::string(\"Unsupported URI format \") + identity_crl);\n    return nullptr;\n}\n\nstatic bool adjust_participant_key(\n        X509* cert,\n        const GUID_t& candidate_participant_key,\n        GUID_t& adjusted_participant_key,\n        SecurityException& exception)\n{\n    assert(cert != nullptr);\n\n    X509_NAME* cert_sn = X509_get_subject_name(cert);\n    assert(cert_sn != nullptr);\n\n    unsigned char md[SHA256_DIGEST_LENGTH];\n    unsigned int length = 0;\n\n    if (!X509_NAME_digest(cert_sn, EVP_sha256(), md, &length) || length != SHA256_DIGEST_LENGTH)\n    {\n        exception = _SecurityException_(\"OpenSSL library cannot hash sha256\");\n        return false;\n    }\n\n    adjusted_participant_key.guidPrefix.value[0] = 0x80 | (md[0] >> 1);\n    adjusted_participant_key.guidPrefix.value[1] = (md[0] << 7) | (md[1] >> 1);\n    adjusted_participant_key.guidPrefix.value[2] = (md[1] << 7) | (md[2] >> 1);\n    adjusted_participant_key.guidPrefix.value[3] = (md[2] << 7) | (md[3] >> 1);\n    adjusted_participant_key.guidPrefix.value[4] = (md[3] << 7) | (md[4] >> 1);\n    adjusted_participant_key.guidPrefix.value[5] = (md[4] << 7) | (md[5] >> 1);\n\n    unsigned char key[16] = {\n        candidate_participant_key.guidPrefix.value[0],\n        candidate_participant_key.guidPrefix.value[1],\n        candidate_participant_key.guidPrefix.value[2],\n        candidate_participant_key.guidPrefix.value[3],\n        candidate_participant_key.guidPrefix.value[4],\n        candidate_participant_key.guidPrefix.value[5],\n        candidate_participant_key.guidPrefix.value[6],\n        candidate_participant_key.guidPrefix.value[7],\n        candidate_participant_key.guidPrefix.value[8],\n        candidate_participant_key.guidPrefix.value[9],\n        candidate_participant_key.guidPrefix.value[10],\n        candidate_participant_key.guidPrefix.value[11],\n        candidate_participant_key.entityId.value[0],\n        candidate_participant_key.entityId.value[1],\n        candidate_participant_key.entityId.value[2],\n        candidate_participant_key.entityId.value[3]\n    };\n\n    if (!EVP_Digest(&key, 16, md, NULL, EVP_sha256(), NULL))\n    {\n        exception = _SecurityException_(\"OpenSSL library cannot hash sha256\");\n        return false;\n    }\n\n    adjusted_participant_key.guidPrefix.value[6] = md[0];\n    adjusted_participant_key.guidPrefix.value[7] = md[1];\n    adjusted_participant_key.guidPrefix.value[8] = md[2];\n    adjusted_participant_key.guidPrefix.value[9] = md[3];\n    adjusted_participant_key.guidPrefix.value[10] = md[4];\n    adjusted_participant_key.guidPrefix.value[11] = md[5];\n\n    adjusted_participant_key.entityId.value[0] = candidate_participant_key.entityId.value[0];\n    adjusted_participant_key.entityId.value[1] = candidate_participant_key.entityId.value[1];\n    adjusted_participant_key.entityId.value[2] = candidate_participant_key.entityId.value[2];\n    adjusted_participant_key.entityId.value[3] = candidate_participant_key.entityId.value[3];\n\n    return true;\n}\n\nstatic int get_dh_type(\n        const std::string& algorithm)\n{\n    auto raw_alg = algorithm.c_str();\n    if (strcmp(DH_2048_256, raw_alg) == 0)\n    {\n        return EVP_PKEY_DH;\n    }\n    else if (strcmp(ECDH_prime256v1, raw_alg) == 0)\n    {\n        return EVP_PKEY_EC;\n    }\n\n    return 0;\n}\n\nstatic EVP_PKEY* generate_dh_key(\n        int type,\n        SecurityException& exception)\n{\n    EVP_PKEY_CTX* pctx = nullptr;\n    EVP_PKEY* params = nullptr;\n\n    if (type == EVP_PKEY_EC)\n    {\n        pctx = EVP_PKEY_CTX_new_id(EVP_PKEY_EC, NULL);\n        if (pctx != nullptr)\n        {\n            if ((1 != EVP_PKEY_paramgen_init(pctx)) ||\n                    (1 != EVP_PKEY_CTX_set_ec_paramgen_curve_nid(pctx, NID_X9_62_prime256v1)) ||\n                    (1 != EVP_PKEY_paramgen(pctx, &params)))\n            {\n                exception = _SecurityException_(\"Cannot set default parameters: \");\n                EVP_PKEY_CTX_free(pctx);\n                return nullptr;\n            }\n        }\n        else\n        {\n            exception = _SecurityException_(\"Cannot allocate EVP parameters\");\n            return nullptr;\n        }\n    }\n    else if (type == EVP_PKEY_DH)\n    {\n        params = EVP_PKEY_new();\n        if (params != nullptr)\n        {\n            if (1 != EVP_PKEY_set1_DH(params, DH_get_2048_256()))\n            {\n                exception = _SecurityException_(\"Cannot set default parameters: \");\n                EVP_PKEY_free(params);\n                return nullptr;\n            }\n        }\n        else\n        {\n            exception = _SecurityException_(\"Cannot allocate EVP parameters\");\n            return nullptr;\n        }\n    }\n    else\n    {\n        exception = _SecurityException_(\"Wrong DH kind\");\n        return nullptr;\n    }\n\n    EVP_PKEY* keys = nullptr;\n    EVP_PKEY_CTX* kctx = EVP_PKEY_CTX_new(params, NULL);\n\n    if (kctx != nullptr)\n    {\n        if (1 == EVP_PKEY_keygen_init(kctx))\n        {\n            if (1 != EVP_PKEY_keygen(kctx, &keys))\n            {\n                exception = _SecurityException_(\"Cannot generate EVP key\");\n            }\n        }\n        else\n        {\n            exception = _SecurityException_(\"Cannot init EVP key\");\n        }\n\n        EVP_PKEY_CTX_free(kctx);\n    }\n    else\n    {\n        exception = _SecurityException_(\"Cannot create EVP context\");\n    }\n\n    ERR_clear_error();\n    EVP_PKEY_free(params);\n    if (pctx != nullptr)\n    {\n        EVP_PKEY_CTX_free(pctx);\n    }\n    return keys;\n}\n\nstatic bool store_dh_public_key(\n        EVP_PKEY* dhkey,\n        int type,\n        std::vector<uint8_t>& buffer,\n        SecurityException& exception)\n{\n    bool returnedValue = false;\n\n    if (type == EVP_PKEY_DH)\n    {\n        const DH* dh =\n#if IS_OPENSSL_1_1\n                EVP_PKEY_get0_DH(dhkey);\n#else\n                dhkey->pkey.dh;\n#endif // if IS_OPENSSL_1_1\n\n        if (dh != nullptr)\n        {\n#if IS_OPENSSL_1_1\n            const BIGNUM* pub_key = nullptr;\n            const BIGNUM* priv_key = nullptr;\n            DH_get0_key(dh, &pub_key, &priv_key);\n\n#else\n            const BIGNUM* pub_key = dh->pub_key;\n#endif // if IS_OPENSSL_1_1\n\n            int len = BN_num_bytes(pub_key);\n            buffer.resize(len);\n            unsigned char* pointer = buffer.data();\n            if (BN_bn2bin(pub_key, pointer) == len)\n            {\n                returnedValue = true;\n            }\n            else\n            {\n                exception = _SecurityException_(\"Cannot serialize public key\");\n            }\n        }\n        else\n        {\n            exception = _SecurityException_(\"OpenSSL library doesn't retrieve DH\");\n        }\n    }\n    else if (type == EVP_PKEY_EC)\n    {\n        const EC_KEY* ec =\n#if IS_OPENSSL_1_1\n                EVP_PKEY_get0_EC_KEY(dhkey);\n#else\n                dhkey->pkey.ec;\n#endif // if IS_OPENSSL_1_1\n        if (ec != nullptr)\n        {\n            auto grp = EC_KEY_get0_group(ec);\n            auto pub_key = EC_KEY_get0_public_key(ec);\n            auto len = EC_POINT_point2oct(grp, pub_key, EC_KEY_get_conv_form(ec), NULL, 0, NULL);\n            buffer.resize(len);\n            if (EC_POINT_point2oct(grp, pub_key, EC_KEY_get_conv_form(ec), buffer.data(), len, NULL) == len)\n            {\n                returnedValue = true;\n            }\n            else\n            {\n                exception = _SecurityException_(\"Cannot serialize public key\");\n            }\n        }\n        else\n        {\n            exception = _SecurityException_(\"OpenSSL library doesn't retrieve DH\");\n        }\n    }\n    else\n    {\n        exception = _SecurityException_(\"Wrong DH kind\");\n    }\n\n    return returnedValue;\n}\n\nstatic EVP_PKEY* generate_dh_peer_key(\n        const std::vector<uint8_t>& buffer,\n        SecurityException& exception,\n        int alg_kind)\n{\n    if (alg_kind == EVP_PKEY_DH)\n    {\n        DH* dh = DH_get_2048_256();\n\n        if (dh != nullptr)\n        {\n            const unsigned char* pointer = buffer.data();\n\n#if IS_OPENSSL_1_1\n            BIGNUM* pub_key_ptr;\n            BIGNUM** pub_key = &pub_key_ptr;\n#else\n            BIGNUM** pub_key = &dh->pub_key;\n#endif // if IS_OPENSSL_1_1\n\n            if ((pointer = BN_deserialize_raw(pub_key, buffer.data(), buffer.size(), exception)) != nullptr)\n            {\n#if IS_OPENSSL_1_1\n                DH_set0_key(dh, *pub_key, NULL);\n#endif // if IS_OPENSSL_1_1\n                EVP_PKEY* key = EVP_PKEY_new();\n\n                if (key != nullptr)\n                {\n#if IS_OPENSSL_1_1_1d\n                    int type = DH_get0_q(dh) == NULL ? EVP_PKEY_DH : EVP_PKEY_DHX;\n                    if (EVP_PKEY_assign(key, type, dh) > 0)\n#else\n                    if (EVP_PKEY_assign_DH(key, dh) > 0)\n#endif // if IS_OPENSSL_1_1_1d\n                    {\n                        return key;\n                    }\n                    else\n                    {\n                        exception = _SecurityException_(\"OpenSSL library cannot set dh in pkey\");\n                    }\n\n                    EVP_PKEY_free(key);\n                }\n                else\n                {\n                    exception = _SecurityException_(\"OpenSSL library cannot create pkey\");\n                }\n            }\n            else\n            {\n                exception = _SecurityException_(\"Cannot deserialize public key\");\n            }\n        }\n        else\n        {\n            exception = _SecurityException_(\"OpenSSL library cannot create dh\");\n        }\n    }\n    else if (alg_kind == EVP_PKEY_EC)\n    {\n        EC_KEY* ec = EC_KEY_new_by_curve_name(NID_X9_62_prime256v1);\n\n        if (ec != nullptr)\n        {\n            const unsigned char* pointer = buffer.data();\n\n#if IS_OPENSSL_1_1\n            if (EC_KEY_oct2key(ec, pointer, buffer.size(), NULL) > 0)\n#else\n            if (o2i_ECPublicKey(&ec, &pointer, (long) buffer.size()) != nullptr)\n#endif // if IS_OPENSSL_1_1\n            {\n                EVP_PKEY* key = EVP_PKEY_new();\n\n                if (key != nullptr)\n                {\n                    if (EVP_PKEY_assign_EC_KEY(key, ec) > 0)\n                    {\n                        return key;\n                    }\n                    else\n                    {\n                        exception = _SecurityException_(\"OpenSSL library cannot set ec in pkey\");\n                    }\n\n                    EVP_PKEY_free(key);\n                }\n                else\n                {\n                    exception = _SecurityException_(\"OpenSSL library cannot create pkey\");\n                }\n            }\n            else\n            {\n                exception = _SecurityException_(\"Cannot deserialize public key\");\n            }\n\n            EC_KEY_free(ec);\n        }\n        else\n        {\n            exception = _SecurityException_(\"OpenSSL library cannot create ec\");\n        }\n    }\n    else\n    {\n        exception = _SecurityException_(\"Wrong DH kind\");\n    }\n\n    return nullptr;\n}\n\nstatic bool generate_challenge(\n        std::vector<uint8_t>& vector,\n        SecurityException& exception)\n{\n    bool returnedValue = false;\n    BIGNUM* bn = BN_new();\n\n    if (BN_rand(bn, 256, 0 /*BN_RAND_TOP_ONE*/, 0 /*BN_RAND_BOTTOM_ANY*/))\n    {\n        int len = BN_num_bytes(bn);\n        vector.resize(len);\n\n        if (BN_bn2bin(bn, vector.data()) == len)\n        {\n            returnedValue = true;\n        }\n        else\n        {\n            exception = _SecurityException_(\"OpenSSL library cannot store challenge\");\n        }\n    }\n\n    BN_clear_free(bn);\n\n    return returnedValue;\n}\n\nstatic SharedSecretHandle* generate_sharedsecret(\n        EVP_PKEY* private_key,\n        EVP_PKEY* public_key,\n        SecurityException& exception)\n{\n    assert(private_key);\n    assert(public_key);\n\n    SharedSecretHandle* handle = nullptr;\n    EVP_PKEY_CTX* ctx = EVP_PKEY_CTX_new(private_key, NULL);\n\n    if (ctx != nullptr)\n    {\n        if (EVP_PKEY_derive_init(ctx) > 0)\n        {\n            if (EVP_PKEY_derive_set_peer(ctx, public_key) > 0)\n            {\n                size_t length = 0;\n                if (EVP_PKEY_derive(ctx, NULL, &length) > 0)\n                {\n                    SharedSecret::BinaryData data;\n                    data.name(\"SharedSecret\");\n                    data.value().resize(length);\n\n                    if (EVP_PKEY_derive(ctx, data.value().data(), &length) > 0)\n                    {\n                        uint8_t md[32];\n                        if (EVP_Digest(data.value().data(), length, md, NULL, EVP_sha256(), NULL))\n                        {\n                            data.value().assign(md, md + 32);\n                            handle = new SharedSecretHandle();\n                            (*handle)->data_.push_back(std::move(data));\n                        }\n                        else\n                        {\n                            exception = _SecurityException_(\"OpenSSL library failed while getting derived key\");\n                        }\n                    }\n                    else\n                    {\n                        exception = _SecurityException_(\"OpenSSL library cannot get derive\");\n                    }\n                }\n                else\n                {\n                    exception = _SecurityException_(\"OpenSSL library cannot get length\");\n                }\n            }\n            else\n            {\n                exception = _SecurityException_(\"OpenSSL library cannot set peer\");\n            }\n        }\n        else\n        {\n            exception = _SecurityException_(\"OpenSSL library cannot init derive\");\n        }\n\n        EVP_PKEY_CTX_free(ctx);\n    }\n    else\n    {\n        exception = _SecurityException_(\"OpenSSL library cannot allocate context\");\n    }\n\n    return handle;\n}\n\nstatic bool generate_identity_token(\n        PKIIdentityHandle& handle)\n{\n    Property property;\n    IdentityToken& token = handle->identity_token_;\n    token.class_id(\"DDS:Auth:PKI-DH:1.0\");\n\n    property.name(\"dds.cert.sn\");\n    property.value() = handle->cert_sn_;\n    property.propagate(true);\n    token.properties().push_back(std::move(property));\n\n    property.name(\"dds.cert.algo\");\n    property.value() = handle->sign_alg_;\n    property.propagate(true);\n    token.properties().push_back(std::move(property));\n\n    property.name(\"dds.ca.sn\");\n    property.value() = handle->sn;\n    property.propagate(true);\n    token.properties().push_back(std::move(property));\n\n    property.name(\"dds.ca.algo\");\n    property.value() = handle->algo;\n    property.propagate(true);\n    token.properties().push_back(std::move(property));\n\n    return true;\n}\n\nValidationResult_t PKIDH::validate_local_identity(\n        IdentityHandle** local_identity_handle,\n        GUID_t& adjusted_participant_key,\n        const uint32_t /*domain_id*/,\n        const RTPSParticipantAttributes& participant_attr,\n        const GUID_t& candidate_participant_key,\n        SecurityException& exception)\n{\n    assert(local_identity_handle);\n\n    PropertyPolicy auth_properties = PropertyPolicyHelper::get_properties_with_prefix(participant_attr.properties,\n                    \"dds.sec.auth.builtin.PKI-DH.\");\n\n    if (PropertyPolicyHelper::length(auth_properties) == 0)\n    {\n        exception = _SecurityException_(\"Not found any dds.sec.auth.builtin.PKI-DH property\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    std::string* identity_ca = PropertyPolicyHelper::find_property(auth_properties, \"identity_ca\");\n\n    if (identity_ca == nullptr)\n    {\n        exception = _SecurityException_(\"Not found dds.sec.auth.builtin.PKI-DH.identity_ca property\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    std::string* identity_cert = PropertyPolicyHelper::find_property(auth_properties, \"identity_certificate\");\n\n    if (identity_cert == nullptr)\n    {\n        exception = _SecurityException_(\"Not found dds.sec.auth.builtin.PKI-DH.identity_certificate property\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    std::string* identity_crl = PropertyPolicyHelper::find_property(auth_properties, \"identity_crl\");\n\n    std::string* private_key = PropertyPolicyHelper::find_property(auth_properties, \"private_key\");\n\n    if (private_key == nullptr)\n    {\n        exception = _SecurityException_(\"Not found dds.sec.auth.builtin.PKI-DH.private_key property\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    std::string* password = PropertyPolicyHelper::find_property(auth_properties, \"password\");\n    std::string empty_password;\n\n    if (password == nullptr)\n    {\n        password = &empty_password;\n    }\n\n    PKIIdentityHandle* ih = new PKIIdentityHandle();\n\n    (*ih)->store_ = load_identity_ca(*identity_ca, (*ih)->there_are_crls_, (*ih)->sn, (*ih)->algo,\n                    exception);\n\n    if ((*ih)->store_ != nullptr)\n    {\n        ERR_clear_error();\n\n        if (identity_crl != nullptr)\n        {\n            X509_CRL* crl = load_crl(*identity_crl, exception);\n\n            if (crl != nullptr)\n            {\n                X509_STORE_add_crl((*ih)->store_, crl);\n                X509_CRL_free(crl);\n                (*ih)->there_are_crls_ = true;\n            }\n            else\n            {\n                delete ih;\n                EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n                return ValidationResult_t::VALIDATION_FAILED;\n            }\n        }\n\n        ERR_clear_error();\n\n        (*ih)->cert_ = load_certificate(*identity_cert, exception);\n\n        if ((*ih)->cert_ != nullptr)\n        {\n            // Get subject name.\n            X509_NAME* cert_sn = X509_get_subject_name((*ih)->cert_);\n            assert(cert_sn != nullptr);\n            char* cert_sn_str = X509_NAME_oneline(cert_sn, 0, 0);\n            assert(cert_sn_str != nullptr);\n            (*ih)->cert_sn_ = cert_sn_str;\n            OPENSSL_free(cert_sn_str);\n            BIO* cert_sn_rfc2253_str = BIO_new(BIO_s_mem());\n            X509_NAME_print_ex(cert_sn_rfc2253_str, cert_sn, 0, XN_FLAG_RFC2253 & ~ASN1_STRFLGS_ESC_MSB);\n            const int bufsize = 1024;\n            char buffer[bufsize];\n            int length = BIO_read(cert_sn_rfc2253_str, buffer, bufsize);\n            BIO_free(cert_sn_rfc2253_str);\n            (*ih)->cert_sn_rfc2253_.assign(buffer, length);\n\n\n            if (verify_certificate((*ih)->store_, (*ih)->cert_, (*ih)->there_are_crls_))\n            {\n                if (store_certificate_in_buffer((*ih)->cert_, &(*ih)->cert_content_, exception))\n                {\n                    if (get_signature_algorithm((*ih)->cert_, (*ih)->sign_alg_, exception))\n                    {\n                        (*ih)->pkey_ = load_private_key((*ih)->cert_, *private_key, *password, exception, *this);\n\n                        if ((*ih)->pkey_ != nullptr)\n                        {\n                            if (adjust_participant_key((*ih)->cert_, candidate_participant_key,\n                                    adjusted_participant_key, exception))\n                            {\n                                // Generate IdentityToken.\n                                if (generate_identity_token(*ih))\n                                {\n                                    (*ih)->participant_key_ = adjusted_participant_key;\n                                    *local_identity_handle = ih;\n\n                                    return ValidationResult_t::VALIDATION_OK;\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n\n    delete ih;\n\n    ERR_clear_error();\n\n    return ValidationResult_t::VALIDATION_FAILED;\n}\n\nValidationResult_t PKIDH::validate_remote_identity(\n        IdentityHandle** remote_identity_handle,\n        const IdentityHandle& local_identity_handle,\n        const IdentityToken& remote_identity_token,\n        const GUID_t& remote_participant_key,\n        SecurityException& /*exception*/)\n{\n    assert(remote_identity_handle);\n    assert(local_identity_handle.nil() == false);\n\n    ValidationResult_t returnedValue = ValidationResult_t::VALIDATION_FAILED;\n\n    const PKIIdentityHandle& lih = PKIIdentityHandle::narrow(local_identity_handle);\n\n    if (!lih.nil() && !remote_identity_token.is_nil())\n    {\n        // dds.ca.sn\n        const std::string* ca_sn = DataHolderHelper::find_property_value(remote_identity_token, \"dds.ca.sn\");\n\n        // dds.cert.sn\n        // const std::string* cert_sn = DataHolderHelper::find_property_value(remote_identity_token, \"dds.cert.sn\");\n\n        // dds.cert.algo\n        const std::string* cert_algo = DataHolderHelper::find_property_value(remote_identity_token, \"dds.cert.algo\");\n\n        PKIIdentityHandle* rih = new PKIIdentityHandle();\n\n        (*rih)->sn = ca_sn ? *ca_sn : \"\";\n        (*rih)->cert_sn_ = \"\"; // cert_sn ? *cert_sn : \"\";\n        (*rih)->algo = cert_algo ? *cert_algo : \"\";\n        (*rih)->participant_key_ = remote_participant_key;\n        *remote_identity_handle = rih;\n\n        if (lih->participant_key_ < remote_participant_key )\n        {\n            returnedValue = ValidationResult_t::VALIDATION_PENDING_HANDSHAKE_REQUEST;\n        }\n        else\n        {\n            returnedValue = ValidationResult_t::VALIDATION_PENDING_HANDSHAKE_MESSAGE;\n        }\n    }\n\n    return returnedValue;\n}\n\nValidationResult_t PKIDH::begin_handshake_request(\n        HandshakeHandle** handshake_handle,\n        HandshakeMessageToken** handshake_message,\n        const IdentityHandle& initiator_identity_handle,\n        IdentityHandle& replier_identity_handle,\n        const CDRMessage_t& cdr_participant_data,\n        SecurityException& exception)\n{\n    assert(handshake_handle);\n    assert(handshake_message);\n    assert(initiator_identity_handle.nil() == false);\n    assert(replier_identity_handle.nil() == false);\n\n    const PKIIdentityHandle& lih = PKIIdentityHandle::narrow(initiator_identity_handle);\n    PKIIdentityHandle& rih = PKIIdentityHandle::narrow(replier_identity_handle);\n\n    if (lih.nil() || rih.nil())\n    {\n        exception = _SecurityException_(\"Bad precondition\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    if (cdr_participant_data.length == 0)\n    {\n        exception = _SecurityException_(\"Bad precondition\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    unsigned char md[SHA256_DIGEST_LENGTH];\n\n    // New handshake\n    PKIHandshakeHandle* handshake_handle_aux = new PKIHandshakeHandle();\n    (*handshake_handle_aux)->kagree_alg_ = lih->kagree_alg_;\n    (*handshake_handle_aux)->handshake_message_.class_id(\"DDS:Auth:PKI-DH:1.0+Req\");\n\n    BinaryProperty bproperty;\n\n    // c.id\n    bproperty.name(\"c.id\");\n    bproperty.value().assign(lih->cert_content_->data,\n            lih->cert_content_->data + lih->cert_content_->length);\n    bproperty.propagate(true);\n    (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n    // c.perm\n    if (lih->permissions_credential_token_.class_id().compare(\"DDS:Access:PermissionsCredential\") == 0)\n    {\n        const Property* permissions_file = DataHolderHelper::find_property(lih->permissions_credential_token_,\n                        \"dds.perm.cert\");\n\n        if (permissions_file != nullptr)\n        {\n            bproperty.name(\"c.perm\");\n            bproperty.value().assign(permissions_file->value().begin(), permissions_file->value().end());\n            bproperty.propagate(true);\n            (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n        }\n        else\n        {\n            exception = _SecurityException_(\"Cannot find permissions file in permissions credential token\");\n            EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n            delete handshake_handle_aux;\n            return ValidationResult_t::VALIDATION_FAILED;\n        }\n    }\n\n    // c.pdata\n    bproperty.name(\"c.pdata\");\n    bproperty.value().assign(cdr_participant_data.buffer,\n            cdr_participant_data.buffer + cdr_participant_data.length);\n    bproperty.propagate(true);\n    (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n    // c.dsign_algo.\n    bproperty.name(\"c.dsign_algo\");\n    bproperty.value().assign(lih->sign_alg_.begin(),\n            lih->sign_alg_.end());\n    bproperty.propagate(true);\n    (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n    // TODO(Ricardo) Only support right now DH+MODP-2048-256\n    // c.kagree_algo.\n    bproperty.name(\"c.kagree_algo\");\n    bproperty.value().assign(lih->kagree_alg_.begin(),\n            lih->kagree_alg_.end());\n    bproperty.propagate(true);\n    (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n    // hash_c1\n    CDRMessage_t message(static_cast<uint32_t>(BinaryPropertyHelper::serialized_size(\n                (*handshake_handle_aux)->handshake_message_.binary_properties())));\n    message.msg_endian = BIGEND;\n    CDRMessage::addBinaryPropertySeq(&message, (*handshake_handle_aux)->handshake_message_.binary_properties(), false);\n    if (!EVP_Digest(message.buffer, message.length, md, NULL, EVP_sha256(), NULL))\n    {\n        exception = _SecurityException_(\"OpenSSL library cannot hash sha256\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        delete handshake_handle_aux;\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n    bproperty.name(\"hash_c1\");\n    bproperty.value().assign(md, md + SHA256_DIGEST_LENGTH);\n    bproperty.propagate(true);\n    (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n    int kagree_kind = get_dh_type((*handshake_handle_aux)->kagree_alg_);\n\n    // dh1\n    if (((*handshake_handle_aux)->dhkeys_ = generate_dh_key(kagree_kind, exception)) != nullptr)\n    {\n        bproperty.name(\"dh1\");\n        bproperty.propagate(true);\n\n        if (store_dh_public_key((*handshake_handle_aux)->dhkeys_, kagree_kind, bproperty.value(), exception))\n        {\n            (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n            // challenge1\n            bproperty.name(\"challenge1\");\n            bproperty.propagate(true);\n            if (generate_challenge(bproperty.value(), exception))\n            {\n                (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n                (*handshake_handle_aux)->local_identity_handle_ = &lih;\n                (*handshake_handle_aux)->remote_identity_handle_ = &rih;\n                *handshake_handle = handshake_handle_aux;\n                *handshake_message = &(*handshake_handle_aux)->handshake_message_;\n                return ValidationResult_t::VALIDATION_PENDING_HANDSHAKE_MESSAGE;\n            }\n        }\n    }\n\n    delete handshake_handle_aux;\n\n    ERR_clear_error();\n\n    return ValidationResult_t::VALIDATION_FAILED;\n}\n\nValidationResult_t PKIDH::begin_handshake_reply(\n        HandshakeHandle** handshake_handle,\n        HandshakeMessageToken** handshake_message_out,\n        HandshakeMessageToken&& handshake_message_in,\n        IdentityHandle& initiator_identity_handle,\n        const IdentityHandle& replier_identity_handle,\n        const CDRMessage_t& cdr_participant_data,\n        SecurityException& exception)\n{\n    assert(handshake_handle);\n    assert(handshake_message_out);\n    assert(initiator_identity_handle.nil() == false);\n    assert(replier_identity_handle.nil() == false);\n\n    const PKIIdentityHandle& lih = PKIIdentityHandle::narrow(replier_identity_handle);\n    PKIIdentityHandle& rih = PKIIdentityHandle::narrow(initiator_identity_handle);\n\n    if (lih.nil() || rih.nil())\n    {\n        exception = _SecurityException_(\"Bad precondition\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    if (cdr_participant_data.length == 0)\n    {\n        exception = _SecurityException_(\"Bad precondition\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // Check TokenMessage\n    if (handshake_message_in.class_id().compare(\"DDS:Auth:PKI-DH:1.0+Req\") != 0)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", std::string(\"Bad HandshakeMessageToken (\") +\n                handshake_message_in.class_id() + \")\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // Check incomming handshake.\n    // Check c.id\n    const std::vector<uint8_t>* cid = DataHolderHelper::find_binary_property_value(handshake_message_in, \"c.id\");\n    if (cid == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property c.id\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    rih->cert_ = load_certificate(*cid);\n\n    if (rih->cert_ == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot load certificate\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    X509_NAME* cert_sn = X509_get_subject_name(rih->cert_);\n    assert(cert_sn != nullptr);\n    char* cert_sn_str = X509_NAME_oneline(cert_sn, 0, 0);\n    assert(cert_sn_str != nullptr);\n    if (!rih->cert_sn_.empty() && rih->cert_sn_.compare(cert_sn_str) != 0)\n    {\n        OPENSSL_free(cert_sn_str);\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Certificated subject name invalid\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n    rih->cert_sn_.assign(cert_sn_str);\n    OPENSSL_free(cert_sn_str);\n    BIO* cert_sn_rfc2253_str = BIO_new(BIO_s_mem());\n    X509_NAME_print_ex(cert_sn_rfc2253_str, cert_sn, 0, XN_FLAG_RFC2253 & ~ASN1_STRFLGS_ESC_MSB);\n    const int bufsize = 1024;\n    char buffer[bufsize];\n    int str_length = BIO_read(cert_sn_rfc2253_str, buffer, bufsize);\n    BIO_free(cert_sn_rfc2253_str);\n    rih->cert_sn_rfc2253_.assign(buffer, str_length);\n\n    if (!verify_certificate(lih->store_, rih->cert_, lih->there_are_crls_))\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Error verifying certificate\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // c.perm\n    if (lih->permissions_credential_token_.class_id().compare(\"DDS:Access:PermissionsCredential\") == 0)\n    {\n        const std::vector<uint8_t>* perm = DataHolderHelper::find_binary_property_value(handshake_message_in,\n                        \"c.perm\");\n\n        if (perm == nullptr)\n        {\n            WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property c.perm\");\n            return ValidationResult_t::VALIDATION_FAILED;\n        }\n\n        rih->permissions_credential_token_.class_id(\"DDS:Access:PermissionsCredential\");\n        Property permission_file;\n        permission_file.name(\"dds.perm.cert\");\n        permission_file.value().assign(perm->begin(), perm->end());\n        rih->permissions_credential_token_.properties().push_back(std::move(permission_file));\n    }\n\n    const std::vector<uint8_t>* pdata = DataHolderHelper::find_binary_property_value(handshake_message_in, \"c.pdata\");\n\n    if (pdata == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property c.pdata\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    GUID_t participant_guid;\n    CDRMessage_t cdr_pdata(0);\n    cdr_pdata.wraps = true;\n    cdr_pdata.msg_endian = BIGEND;\n    cdr_pdata.length = (uint32_t)pdata->size();\n    cdr_pdata.max_size = (uint32_t)pdata->size();\n    cdr_pdata.buffer = (octet*)pdata->data();\n\n    if (!ParameterList::read_guid_from_cdr_msg(cdr_pdata, fastdds::dds::PID_PARTICIPANT_GUID, participant_guid))\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot deserialize ParticipantProxyData in property c.pdata\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    if ((participant_guid.guidPrefix.value[0] & 0x80) != 0x80)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Bad participant_key's first bit in c.pdata\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    unsigned char md[SHA256_DIGEST_LENGTH];\n    unsigned char hash_c1[SHA256_DIGEST_LENGTH];\n    unsigned int length = 0;\n\n    if (!X509_NAME_digest(cert_sn, EVP_sha256(), md, &length) || length != SHA256_DIGEST_LENGTH)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot generate SHA256 of subject name\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    md[5] &= 0xFE;\n    unsigned char bytes[6]{\n        static_cast<unsigned char>((participant_guid.guidPrefix.value[0] << 1) |\n        (participant_guid.guidPrefix.value[1] >> 7)),\n        static_cast<unsigned char>((participant_guid.guidPrefix.value[1] << 1) |\n        (participant_guid.guidPrefix.value[2] >> 7)),\n        static_cast<unsigned char>((participant_guid.guidPrefix.value[2] << 1) |\n        (participant_guid.guidPrefix.value[3] >> 7)),\n        static_cast<unsigned char>((participant_guid.guidPrefix.value[3] << 1) |\n        (participant_guid.guidPrefix.value[4] >> 7)),\n        static_cast<unsigned char>((participant_guid.guidPrefix.value[4] << 1) |\n        (participant_guid.guidPrefix.value[5] >> 7)),\n        static_cast<unsigned char>(participant_guid.guidPrefix.value[5] << 1)\n    };\n\n    if (memcmp(md, bytes, 6) != 0)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Bad participant_key's 47bits in c.pdata\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // c.dsign_algo\n    const std::vector<uint8_t>* dsign_algo = DataHolderHelper::find_binary_property_value(handshake_message_in,\n                    \"c.dsign_algo\");\n\n    if (dsign_algo == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property c.dsign_algo\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // Check signature algorithm\n    std::string s_dsign_algo(dsign_algo->begin(), dsign_algo->end());\n    if (strcmp(RSA_SHA256, s_dsign_algo.c_str()) != 0 &&\n            strcmp(ECDSA_SHA256, s_dsign_algo.c_str()) != 0)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Not supported signature algorithm (\" + s_dsign_algo + \")\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n    rih->sign_alg_ = std::move(s_dsign_algo);\n\n    // c.kagree_algo\n    const std::vector<uint8_t>* kagree_algo = DataHolderHelper::find_binary_property_value(handshake_message_in,\n                    \"c.kagree_algo\");\n\n    if (kagree_algo == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property c.kagree_algo\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // Check key agreement algorithm\n    std::string s_kagree_algo(kagree_algo->begin(), kagree_algo->end());\n    if (strcmp(DH_2048_256, s_kagree_algo.c_str()) != 0 &&\n            strcmp(ECDH_prime256v1, s_kagree_algo.c_str()) != 0)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", std::string(\"Not supported key agreement algorithm (\") + s_kagree_algo + \")\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n    rih->kagree_alg_ = std::move(s_kagree_algo);\n\n    CDRMessage_t cdrmessage(static_cast<uint32_t>(BinaryPropertyHelper::serialized_size(\n                handshake_message_in.binary_properties())));\n    cdrmessage.msg_endian = BIGEND;\n    CDRMessage::addBinaryPropertySeq(&cdrmessage, handshake_message_in.binary_properties(), \"c.\", false);\n\n    if (!EVP_Digest(cdrmessage.buffer, cdrmessage.length, hash_c1, NULL, EVP_sha256(), NULL))\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot generate SHA256 of request\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n    else\n    {\n        // hash_c1\n        std::vector<uint8_t>* hash_c1_vec =\n                DataHolderHelper::find_binary_property_value(handshake_message_in, \"hash_c1\");\n\n        if (hash_c1_vec != nullptr)\n        {\n            if ((hash_c1_vec->size() == SHA256_DIGEST_LENGTH) &&\n                    (memcmp(hash_c1, hash_c1_vec->data(), SHA256_DIGEST_LENGTH) != 0))\n            {\n                WARNING_SECURITY_LOGGING(\"PKIDH\", \"Wrong hash_c1\");\n            }\n        }\n    }\n\n    // dh1\n    std::vector<uint8_t>* dh1 = DataHolderHelper::find_binary_property_value(handshake_message_in, \"dh1\");\n\n    if (dh1 == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property dh1\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // challenge1\n    std::vector<uint8_t>* challenge1 = DataHolderHelper::find_binary_property_value(handshake_message_in, \"challenge1\");\n\n    if (challenge1 == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property challenge1\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // Generate handshake reply message token.\n    PKIHandshakeHandle* handshake_handle_aux = new PKIHandshakeHandle();\n    (*handshake_handle_aux)->kagree_alg_ = rih->kagree_alg_;\n    (*handshake_handle_aux)->handshake_message_.class_id(\"DDS:Auth:PKI-DH:1.0+Reply\");\n\n    int kagree_kind = get_dh_type((*handshake_handle_aux)->kagree_alg_);\n\n    // Store dh1\n    if (((*handshake_handle_aux)->peerkeys_ = generate_dh_peer_key(*dh1, exception, kagree_kind)) == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot store peer key from dh1\");\n        delete handshake_handle_aux;\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    BinaryProperty bproperty;\n\n    // c.id\n    bproperty.name(\"c.id\");\n    bproperty.value().assign(lih->cert_content_->data,\n            lih->cert_content_->data + lih->cert_content_->length);\n    bproperty.propagate(true);\n    (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n    // c.perm\n    if (lih->permissions_credential_token_.class_id().compare(\"DDS:Access:PermissionsCredential\") == 0)\n    {\n        const Property* permissions_file = DataHolderHelper::find_property(lih->permissions_credential_token_,\n                        \"dds.perm.cert\");\n\n        if (permissions_file != nullptr)\n        {\n            bproperty.name(\"c.perm\");\n            bproperty.value().assign(permissions_file->value().begin(), permissions_file->value().end());\n            bproperty.propagate(true);\n            (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n        }\n        else\n        {\n            exception = _SecurityException_(\"Cannot find permissions file in permissions credential token\");\n            EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n            return ValidationResult_t::VALIDATION_FAILED;\n        }\n    }\n\n    // c.pdata\n    bproperty.name(\"c.pdata\");\n    bproperty.value().assign(cdr_participant_data.buffer,\n            cdr_participant_data.buffer + cdr_participant_data.length);\n    bproperty.propagate(true);\n    (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n    // c.dsign_algo.\n    bproperty.name(\"c.dsign_algo\");\n    bproperty.value().assign(lih->sign_alg_.begin(),\n            lih->sign_alg_.end());\n    bproperty.propagate(true);\n    (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n    // TODO(Ricardo) Only support right now DH+MODP-2048-256\n    // c.kagree_algo.\n    bproperty.name(\"c.kagree_algo\");\n    bproperty.value().assign((*handshake_handle_aux)->kagree_alg_.begin(),\n            (*handshake_handle_aux)->kagree_alg_.end());\n    bproperty.propagate(true);\n    (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n    // hash_c2\n    CDRMessage_t message(static_cast<uint32_t>(BinaryPropertyHelper::serialized_size(\n                (*handshake_handle_aux)->handshake_message_.binary_properties())));\n    message.msg_endian = BIGEND;\n    CDRMessage::addBinaryPropertySeq(&message, (*handshake_handle_aux)->handshake_message_.binary_properties(), false);\n    if (!EVP_Digest(message.buffer, message.length, md, NULL, EVP_sha256(), NULL))\n    {\n        exception = _SecurityException_(\"OpenSSL library cannot hash sha256\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        delete handshake_handle_aux;\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n    bproperty.name(\"hash_c2\");\n    bproperty.value().assign(md, md + SHA256_DIGEST_LENGTH);\n    bproperty.propagate(true);\n    (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n    // dh2\n    if (((*handshake_handle_aux)->dhkeys_ = generate_dh_key(kagree_kind, exception)) != nullptr)\n    {\n        bproperty.name(\"dh2\");\n        bproperty.propagate(true);\n\n        if (store_dh_public_key((*handshake_handle_aux)->dhkeys_, kagree_kind, bproperty.value(), exception))\n        {\n            (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n            // hash_c1\n            bproperty.name(\"hash_c1\");\n            bproperty.value().assign(hash_c1, hash_c1 + SHA256_DIGEST_LENGTH);\n            bproperty.propagate(true);\n            (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n            // dh1\n            bproperty.name(\"dh1\");\n            bproperty.value(std::move(*dh1));\n            bproperty.propagate(true);\n            (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n            // challenge1\n            bproperty.name(\"challenge1\");\n            bproperty.value(std::move(*challenge1));\n            bproperty.propagate(true);\n            (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n            // challenge2\n            bproperty.name(\"challenge2\");\n            bproperty.propagate(true);\n            if (generate_challenge(bproperty.value(), exception))\n            {\n                (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n                // signature\n                CDRMessage_t cdrmessage2(static_cast<uint32_t>(BinaryPropertyHelper::serialized_size(\n                            (*handshake_handle_aux)->handshake_message_.binary_properties())));\n                cdrmessage2.msg_endian = BIGEND;\n                // add sequence length\n                CDRMessage::addUInt32(&cdrmessage2, 6);\n                //add hash_c2\n                CDRMessage::addBinaryProperty(&cdrmessage2,\n                        *DataHolderHelper::find_binary_property((*handshake_handle_aux)->handshake_message_,\n                        \"hash_c2\"));\n                //add challenge2\n                CDRMessage::addBinaryProperty(&cdrmessage2,\n                        *DataHolderHelper::find_binary_property((*handshake_handle_aux)->handshake_message_,\n                        \"challenge2\"));\n                //add dh2\n                CDRMessage::addBinaryProperty(&cdrmessage2,\n                        *DataHolderHelper::find_binary_property((*handshake_handle_aux)->handshake_message_, \"dh2\"));\n                //add challenge1\n                CDRMessage::addBinaryProperty(&cdrmessage2,\n                        *DataHolderHelper::find_binary_property((*handshake_handle_aux)->handshake_message_,\n                        \"challenge1\"));\n                //add dh1\n                CDRMessage::addBinaryProperty(&cdrmessage2,\n                        *DataHolderHelper::find_binary_property((*handshake_handle_aux)->handshake_message_, \"dh1\"));\n                //add hash_c1\n                CDRMessage::addBinaryProperty(&cdrmessage2,\n                        *DataHolderHelper::find_binary_property((*handshake_handle_aux)->handshake_message_,\n                        \"hash_c1\"), false);\n\n                bproperty.name(\"signature\");\n                bproperty.propagate(true);\n                if (sign_sha256(lih->pkey_, cdrmessage2.buffer, cdrmessage2.length, bproperty.value(), exception))\n                {\n                    (*handshake_handle_aux)->handshake_message_.binary_properties().push_back(std::move(bproperty));\n\n                    (*handshake_handle_aux)->local_identity_handle_ = &lih;\n                    (*handshake_handle_aux)->remote_identity_handle_ = &rih;\n                    *handshake_handle = handshake_handle_aux;\n                    *handshake_message_out = &(*handshake_handle_aux)->handshake_message_;\n\n                    return ValidationResult_t::VALIDATION_PENDING_HANDSHAKE_MESSAGE;\n                }\n            }\n        }\n    }\n\n    delete handshake_handle_aux;\n\n    ERR_clear_error();\n\n    return ValidationResult_t::VALIDATION_FAILED;\n}\n\nValidationResult_t PKIDH::process_handshake(\n        HandshakeMessageToken** handshake_message_out,\n        HandshakeMessageToken&& handshake_message_in,\n        HandshakeHandle& handshake_handle,\n        SecurityException& exception)\n{\n    ValidationResult_t returnedValue = ValidationResult_t::VALIDATION_FAILED;\n\n    PKIHandshakeHandle& handshake = PKIHandshakeHandle::narrow(handshake_handle);\n\n    if (!handshake.nil())\n    {\n        if (handshake->handshake_message_.class_id().compare(\"DDS:Auth:PKI-DH:1.0+Req\") == 0)\n        {\n            returnedValue = process_handshake_request(handshake_message_out, std::move(handshake_message_in),\n                            handshake, exception);\n        }\n        else if (handshake->handshake_message_.class_id().compare(\"DDS:Auth:PKI-DH:1.0+Reply\") == 0)\n        {\n            returnedValue = process_handshake_reply(handshake_message_out, std::move(handshake_message_in),\n                            handshake, exception);\n        }\n        else\n        {\n            WARNING_SECURITY_LOGGING(\"PKIDH\",\n                    std::string(\"Handshake message not supported (\") + handshake->handshake_message_.class_id() + \")\");\n        }\n    }\n\n    return returnedValue;\n}\n\nValidationResult_t PKIDH::process_handshake_request(\n        HandshakeMessageToken** handshake_message_out,\n        HandshakeMessageToken&& handshake_message_in,\n        PKIHandshakeHandle& handshake_handle,\n        SecurityException& exception)\n{\n    const PKIIdentityHandle& lih = *handshake_handle->local_identity_handle_;\n    PKIIdentityHandle& rih = *handshake_handle->remote_identity_handle_;\n\n    // Check TokenMessage\n    if (handshake_message_in.class_id().compare(\"DDS:Auth:PKI-DH:1.0+Reply\") != 0)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", std::string(\"Bad HandshakeMessageToken (\") +\n                handshake_message_in.class_id() + \")\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // Check incomming handshake.\n    // Check c.id\n    const std::vector<uint8_t>* cid = DataHolderHelper::find_binary_property_value(handshake_message_in, \"c.id\");\n    if (cid == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property c.id\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    rih->cert_ = load_certificate(*cid);\n\n    if (rih->cert_ == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot load certificate\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    X509_NAME* cert_sn = X509_get_subject_name(rih->cert_);\n    assert(cert_sn != nullptr);\n    char* cert_sn_str = X509_NAME_oneline(cert_sn, 0, 0);\n    assert(cert_sn_str != nullptr);\n    if (!rih->cert_sn_.empty() && rih->cert_sn_.compare(cert_sn_str) != 0)\n    {\n        OPENSSL_free(cert_sn_str);\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Certificated subject name invalid\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n    OPENSSL_free(cert_sn_str);\n    BIO* cert_sn_rfc2253_str = BIO_new(BIO_s_mem());\n    X509_NAME_print_ex(cert_sn_rfc2253_str, cert_sn, 0, XN_FLAG_RFC2253 & ~ASN1_STRFLGS_ESC_MSB);\n    const int bufsize = 1024;\n    char buffer[bufsize];\n    int str_length = BIO_read(cert_sn_rfc2253_str, buffer, bufsize);\n    BIO_free(cert_sn_rfc2253_str);\n    rih->cert_sn_rfc2253_.assign(buffer, str_length);\n\n    if (!verify_certificate(lih->store_, rih->cert_, lih->there_are_crls_))\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Error verifying certificate\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // c.perm\n    if (lih->permissions_credential_token_.class_id().compare(\"DDS:Access:PermissionsCredential\") == 0)\n    {\n        const std::vector<uint8_t>* perm = DataHolderHelper::find_binary_property_value(handshake_message_in,\n                        \"c.perm\");\n\n        if (perm == nullptr)\n        {\n            WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property c.perm\");\n            return ValidationResult_t::VALIDATION_FAILED;\n        }\n\n        rih->permissions_credential_token_.class_id(\"DDS:Access:PermissionsCredential\");\n        Property permission_file;\n        permission_file.name(\"dds.perm.cert\");\n        permission_file.value().assign(perm->begin(), perm->end());\n        rih->permissions_credential_token_.properties().push_back(std::move(permission_file));\n    }\n\n    const std::vector<uint8_t>* pdata = DataHolderHelper::find_binary_property_value(handshake_message_in, \"c.pdata\");\n\n    if (pdata == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property c.pdata\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    GUID_t participant_guid;\n    CDRMessage_t cdr_pdata(0);\n    cdr_pdata.wraps = true;\n    cdr_pdata.msg_endian = BIGEND;\n    cdr_pdata.length = (uint32_t)pdata->size();\n    cdr_pdata.max_size = (uint32_t)pdata->size();\n    cdr_pdata.buffer = (octet*)pdata->data();\n\n    if (!ParameterList::read_guid_from_cdr_msg(cdr_pdata, fastdds::dds::PID_PARTICIPANT_GUID, participant_guid))\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot deserialize ParticipantProxyData in property c.pdata\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    if ((participant_guid.guidPrefix.value[0] & 0x80) != 0x80)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Bad participant_key's first bit in c.pdata\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    unsigned char md[SHA256_DIGEST_LENGTH];\n    unsigned int length = 0;\n\n    if (!X509_NAME_digest(cert_sn, EVP_sha256(), md, &length) || length != SHA256_DIGEST_LENGTH)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot generate SHA256 of subject name\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    md[5] &= 0xFE;\n    unsigned char bytes[6]{\n        static_cast<unsigned char>((participant_guid.guidPrefix.value[0] << 1) |\n        (participant_guid.guidPrefix.value[1] >> 7)),\n        static_cast<unsigned char>((participant_guid.guidPrefix.value[1] << 1) |\n        (participant_guid.guidPrefix.value[2] >> 7)),\n        static_cast<unsigned char>((participant_guid.guidPrefix.value[2] << 1) |\n        (participant_guid.guidPrefix.value[3] >> 7)),\n        static_cast<unsigned char>((participant_guid.guidPrefix.value[3] << 1) |\n        (participant_guid.guidPrefix.value[4] >> 7)),\n        static_cast<unsigned char>((participant_guid.guidPrefix.value[4] << 1) |\n        (participant_guid.guidPrefix.value[5] >> 7)),\n        static_cast<unsigned char>(participant_guid.guidPrefix.value[5] << 1)\n    };\n\n    if (memcmp(md, bytes, 6) != 0)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Bad participant_key's 47bits in c.pdata\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // c.dsign_algo\n    const std::vector<uint8_t>* dsign_algo = DataHolderHelper::find_binary_property_value(handshake_message_in,\n                    \"c.dsign_algo\");\n\n    if (dsign_algo == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property c.dsign_algo\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // Check signature algorithm\n    std::string s_dsign_algo(dsign_algo->begin(), dsign_algo->end());\n    if (s_dsign_algo.compare(RSA_SHA256) != 0 &&\n            s_dsign_algo.compare(ECDSA_SHA256) != 0)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", std::string(\"Not supported signature algorithm (\") + s_dsign_algo + \")\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n    rih->sign_alg_ = std::move(s_dsign_algo);\n\n    // c.kagree_algo\n    const std::vector<uint8_t>* kagree_algo = DataHolderHelper::find_binary_property_value(handshake_message_in,\n                    \"c.kagree_algo\");\n\n    if (kagree_algo == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property c.kagree_algo\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // Check key agreement algorithm\n    std::string s_kagree_algo(kagree_algo->begin(), kagree_algo->end());\n    if (s_kagree_algo.compare(handshake_handle->kagree_alg_) != 0)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", std::string(\"Invalid key agreement algorithm. Received \") +\n                s_kagree_algo + \", expected \" + handshake_handle->kagree_alg_);\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // hash_c2\n    BinaryProperty* hash_c2 = DataHolderHelper::find_binary_property(handshake_message_in, \"hash_c2\");\n\n    if (hash_c2 == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property hash_c2\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    if (hash_c2->value().size() != SHA256_DIGEST_LENGTH)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Wrong size of hash_c2\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    uint32_t digestInLen =\n            static_cast<uint32_t>(BinaryPropertyHelper::serialized_size(handshake_message_in.binary_properties()));\n    CDRMessage_t cdrmessage(digestInLen + 3);\n    cdrmessage.msg_endian = BIGEND;\n    CDRMessage::addBinaryPropertySeq(&cdrmessage, handshake_message_in.binary_properties(), \"c.\", false);\n\n    if (!EVP_Digest(cdrmessage.buffer, cdrmessage.length, md, NULL, EVP_sha256(), NULL))\n    {\n        exception = _SecurityException_(\"Cannot generate SHA256 of request\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    if (memcmp(md, hash_c2->value().data(), SHA256_DIGEST_LENGTH) != 0)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Wrong hash_c2\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // dh2\n    BinaryProperty* dh2 = DataHolderHelper::find_binary_property(handshake_message_in, \"dh2\");\n    if (dh2 == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property dh2\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    int kagree_kind = get_dh_type(s_kagree_algo);\n\n    if ((handshake_handle->peerkeys_ = generate_dh_peer_key(dh2->value(), exception, kagree_kind)) == nullptr)\n    {\n        exception = _SecurityException_(\"Cannot store peer key from dh2\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    BinaryProperty* challenge2 = DataHolderHelper::find_binary_property(handshake_message_in, \"challenge2\");\n\n    if (challenge2 == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property challenge2\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // hash_c1\n    BinaryProperty* hash_c1 = DataHolderHelper::find_binary_property(handshake_message_in, \"hash_c1\");\n\n    if (hash_c1 == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property hash_c1\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    const std::vector<uint8_t>* hash_c1_request = DataHolderHelper::find_binary_property_value(\n        handshake_handle->handshake_message_, \"hash_c1\");\n\n    if (hash_c1_request == nullptr)\n    {\n        exception = _SecurityException_(\"Cannot find property hash_c1 in request message\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    if (hash_c1->value() != *hash_c1_request)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Invalid property hash_c1\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // dh1\n    BinaryProperty* dh1 = DataHolderHelper::find_binary_property(handshake_message_in, \"dh1\");\n\n    if (dh1 == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property dh1\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    const std::vector<uint8_t>* dh1_request = DataHolderHelper::find_binary_property_value(\n        handshake_handle->handshake_message_, \"dh1\");\n\n    if (dh1_request == nullptr)\n    {\n        exception = _SecurityException_(\"Cannot find property dh1 in request message\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    if (dh1->value() != *dh1_request)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Invalid property dh1\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    BinaryProperty* challenge1 = DataHolderHelper::find_binary_property(handshake_message_in, \"challenge1\");\n\n    if (challenge1 == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property challenge1\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    const std::vector<uint8_t>* challenge1_request = DataHolderHelper::find_binary_property_value(\n        handshake_handle->handshake_message_, \"challenge1\");\n\n    if (challenge1_request == nullptr)\n    {\n        exception = _SecurityException_(\"Cannot find property challenge1 in request message\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    if (challenge1->value() != *challenge1_request)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Invalid property challenge1\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    const std::vector<uint8_t>* signature = DataHolderHelper::find_binary_property_value(handshake_message_in,\n                    \"signature\");\n\n    if (signature == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property signature\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // signature\n    CDRMessage_t cdrmessage2(static_cast<uint32_t>(BinaryPropertyHelper::serialized_size(\n                handshake_message_in.binary_properties())));\n    cdrmessage2.msg_endian = BIGEND;\n    // add sequence length\n    CDRMessage::addUInt32(&cdrmessage2, 6);\n    //add hash_c2\n    CDRMessage::addBinaryProperty(&cdrmessage2, *hash_c2);\n    //add challenge2\n    CDRMessage::addBinaryProperty(&cdrmessage2, *challenge2);\n    //add dh2\n    CDRMessage::addBinaryProperty(&cdrmessage2, *dh2);\n    //add challenge1\n    CDRMessage::addBinaryProperty(&cdrmessage2, *challenge1);\n    //add dh1\n    CDRMessage::addBinaryProperty(&cdrmessage2, *dh1);\n    //add hash_c1\n    CDRMessage::addBinaryProperty(&cdrmessage2, *hash_c1, false);\n\n    if (!check_sign_sha256(rih->cert_, cdrmessage2.buffer, cdrmessage2.length, *signature, exception))\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Error verifying signature\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // Generate handshake final message token.\n    HandshakeMessageToken final_message;\n    final_message.binary_properties().clear();\n    final_message.class_id(\"DDS:Auth:PKI-DH:1.0+Final\");\n\n    BinaryProperty bproperty;\n\n    // hash_c1\n    bproperty.name(\"hash_c1\");\n    bproperty.value(std::move(hash_c1->value()));\n    bproperty.propagate(true);\n    final_message.binary_properties().push_back(std::move(bproperty));\n\n    // hash_c2\n    bproperty.name(\"hash_c2\");\n    bproperty.value(std::move(hash_c2->value()));\n    bproperty.propagate(true);\n    final_message.binary_properties().push_back(std::move(bproperty));\n\n    // dh1\n    bproperty.name(\"dh1\");\n    bproperty.value(std::move(dh1->value()));\n    bproperty.propagate(true);\n    final_message.binary_properties().push_back(std::move(bproperty));\n\n    // dh2\n    bproperty.name(\"dh2\");\n    bproperty.value(std::move(dh2->value()));\n    bproperty.propagate(true);\n    final_message.binary_properties().push_back(std::move(bproperty));\n\n    // challenge1\n    bproperty.name(\"challenge1\");\n    bproperty.value(std::move(challenge1->value()));\n    bproperty.propagate(true);\n    final_message.binary_properties().push_back(std::move(bproperty));\n\n    // challenge2\n    bproperty.name(\"challenge2\");\n    bproperty.value(std::move(challenge2->value()));\n    bproperty.propagate(true);\n    final_message.binary_properties().push_back(std::move(bproperty));\n\n    // signature\n    cdrmessage2.length = 0;\n    cdrmessage2.pos = 0;\n    // add sequence length\n    CDRMessage::addUInt32(&cdrmessage2, 6);\n    //add hash_c1\n    CDRMessage::addBinaryProperty(&cdrmessage2, *DataHolderHelper::find_binary_property(final_message, \"hash_c1\"));\n    //add challenge1\n    CDRMessage::addBinaryProperty(&cdrmessage2, *DataHolderHelper::find_binary_property(final_message, \"challenge1\"));\n    //add dh1\n    CDRMessage::addBinaryProperty(&cdrmessage2, *DataHolderHelper::find_binary_property(final_message, \"dh1\"));\n    //add challenge2\n    CDRMessage::addBinaryProperty(&cdrmessage2, *DataHolderHelper::find_binary_property(final_message, \"challenge2\"));\n    //add dh2\n    CDRMessage::addBinaryProperty(&cdrmessage2, *DataHolderHelper::find_binary_property(final_message, \"dh2\"));\n    //add hash_c2\n    CDRMessage::addBinaryProperty(&cdrmessage2, *DataHolderHelper::find_binary_property(final_message, \"hash_c2\"),\n            false);\n\n    bproperty.name(\"signature\");\n    bproperty.propagate(true);\n    if (sign_sha256(lih->pkey_, cdrmessage2.buffer, cdrmessage2.length, bproperty.value(), exception))\n    {\n        final_message.binary_properties().push_back(std::move(bproperty));\n\n        handshake_handle->sharedsecret_ = generate_sharedsecret(handshake_handle->dhkeys_, handshake_handle->peerkeys_,\n                        exception);\n\n        if (handshake_handle->sharedsecret_ != nullptr)\n        {\n            // Save challenge1 y challenge2 in sharedsecret\n            (*handshake_handle->sharedsecret_)->data_.emplace_back(SharedSecret::BinaryData(\"Challenge1\",\n                    *DataHolderHelper::find_binary_property_value(final_message, \"challenge1\")));\n            (*handshake_handle->sharedsecret_)->data_.emplace_back(SharedSecret::BinaryData(\"Challenge2\",\n                    *DataHolderHelper::find_binary_property_value(final_message, \"challenge2\")));\n\n            handshake_handle->handshake_message_ = std::move(final_message);\n            *handshake_message_out = &handshake_handle->handshake_message_;\n\n            return ValidationResult_t::VALIDATION_OK_WITH_FINAL_MESSAGE;\n        }\n    }\n\n    ERR_clear_error();\n\n    return ValidationResult_t::VALIDATION_FAILED;\n}\n\nValidationResult_t PKIDH::process_handshake_reply(\n        HandshakeMessageToken** /*handshake_message_out*/,\n        HandshakeMessageToken&& handshake_message_in,\n        PKIHandshakeHandle& handshake_handle,\n        SecurityException& exception)\n{\n    PKIIdentityHandle& rih = *handshake_handle->remote_identity_handle_;\n\n    // Check TokenMessage\n    if (handshake_message_in.class_id().compare(\"DDS:Auth:PKI-DH:1.0+Final\") != 0)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", std::string(\"Bad HandshakeMessageToken (\") +\n                handshake_message_in.class_id() + \")\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // Check incomming handshake.\n\n    // challenge1 (mandatory)\n    BinaryProperty* challenge1 = DataHolderHelper::find_binary_property(handshake_message_in, \"challenge1\");\n    if (challenge1 == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property challenge1\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    std::vector<uint8_t>* challenge1_reply = DataHolderHelper::find_binary_property_value(\n        handshake_handle->handshake_message_, \"challenge1\");\n    if (challenge1_reply == nullptr)\n    {\n        exception = _SecurityException_(\"Cannot find property challenge1 in reply message\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    if (challenge1->value() != *challenge1_reply)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Invalid challenge1\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // challenge2 (mandatory)\n    BinaryProperty* challenge2 = DataHolderHelper::find_binary_property(handshake_message_in, \"challenge2\");\n    if (challenge2 == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property challenge2\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    std::vector<uint8_t>* challenge2_reply = DataHolderHelper::find_binary_property_value(\n        handshake_handle->handshake_message_, \"challenge2\");\n    if (challenge2_reply == nullptr)\n    {\n        exception = _SecurityException_(\"Cannot find property challenge2 in reply message\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    if (challenge2->value() != *challenge2_reply)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Invalid challenge2\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // signature (mandatory)\n    const std::vector<uint8_t>* signature = DataHolderHelper::find_binary_property_value(handshake_message_in,\n                    \"signature\");\n    if (signature == nullptr)\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Cannot find property signature\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    // hash_c1 (optional)\n    BinaryProperty* hash_c1_reply = DataHolderHelper::find_binary_property(handshake_handle->handshake_message_,\n                    \"hash_c1\");\n    if (hash_c1_reply == nullptr)\n    {\n        exception = _SecurityException_(\"Cannot find property hash_c1 in reply message\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    BinaryProperty* hash_c1 = DataHolderHelper::find_binary_property(handshake_message_in, \"hash_c1\");\n    if (hash_c1 != nullptr)\n    {\n        if (hash_c1->value() != hash_c1_reply->value())\n        {\n            WARNING_SECURITY_LOGGING(\"PKIDH\", \"Invalid hash_c1\");\n        }\n    }\n\n    // hash_c2 (optional)\n    BinaryProperty* hash_c2_reply = DataHolderHelper::find_binary_property(handshake_handle->handshake_message_,\n                    \"hash_c2\");\n    if (hash_c2_reply == nullptr)\n    {\n        exception = _SecurityException_(\"Cannot find property hash_c2 in reply message\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    BinaryProperty* hash_c2 = DataHolderHelper::find_binary_property(handshake_message_in, \"hash_c2\");\n    if (hash_c2 != nullptr)\n    {\n        if (hash_c2->value() != hash_c2_reply->value())\n        {\n            WARNING_SECURITY_LOGGING(\"PKIDH\", \"Invalid hash_c2\");\n        }\n    }\n\n    // dh1 (optional)\n    BinaryProperty* dh1_reply = DataHolderHelper::find_binary_property(handshake_handle->handshake_message_, \"dh1\");\n    if (dh1_reply == nullptr)\n    {\n        exception = _SecurityException_(\"Cannot find property dh1 in reply message\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    BinaryProperty* dh1 = DataHolderHelper::find_binary_property(handshake_message_in, \"dh1\");\n    if (dh1 != nullptr)\n    {\n        if (dh1->value() != dh1_reply->value())\n        {\n            WARNING_SECURITY_LOGGING(\"PKIDH\", \"Invalid dh1\");\n        }\n    }\n\n    // dh2 (optional)\n    BinaryProperty* dh2_reply = DataHolderHelper::find_binary_property(handshake_handle->handshake_message_, \"dh2\");\n    if (dh2_reply == nullptr)\n    {\n        exception = _SecurityException_(\"Cannot find property dh2 in reply message\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    BinaryProperty* dh2 = DataHolderHelper::find_binary_property(handshake_message_in, \"dh2\");\n    if (dh2 != nullptr)\n    {\n        if (dh2->value() != dh2_reply->value())\n        {\n            WARNING_SECURITY_LOGGING(\"PKIDH\", \"Invalid dh2\");\n        }\n    }\n\n    // signature\n    CDRMessage_t cdrmessage(static_cast<uint32_t>(BinaryPropertyHelper::serialized_size(\n                handshake_handle->handshake_message_.binary_properties())));\n    cdrmessage.msg_endian = BIGEND;\n    // add sequence length\n    CDRMessage::addUInt32(&cdrmessage, 6);\n    //add hash_c1\n    CDRMessage::addBinaryProperty(&cdrmessage, *hash_c1_reply);\n    //add challenge1\n    CDRMessage::addBinaryProperty(&cdrmessage, *challenge1);\n    //add dh1\n    CDRMessage::addBinaryProperty(&cdrmessage, *dh1_reply);\n    //add challenge2\n    CDRMessage::addBinaryProperty(&cdrmessage, *challenge2);\n    //add dh2\n    CDRMessage::addBinaryProperty(&cdrmessage, *dh2_reply);\n    //add hash_c2\n    CDRMessage::addBinaryProperty(&cdrmessage, *hash_c2_reply, false);\n\n    if (!check_sign_sha256(rih->cert_, cdrmessage.buffer, cdrmessage.length, *signature, exception))\n    {\n        WARNING_SECURITY_LOGGING(\"PKIDH\", \"Error verifying signature\");\n        return ValidationResult_t::VALIDATION_FAILED;\n    }\n\n    handshake_handle->sharedsecret_ = generate_sharedsecret(handshake_handle->dhkeys_, handshake_handle->peerkeys_,\n                    exception);\n\n    if (handshake_handle->sharedsecret_ != nullptr)\n    {\n        // Save challenge1 y challenge2 in sharedsecret\n        (*handshake_handle->sharedsecret_)->data_.emplace_back(SharedSecret::BinaryData(\"Challenge1\",\n                challenge1->value()));\n        (*handshake_handle->sharedsecret_)->data_.emplace_back(SharedSecret::BinaryData(\"Challenge2\",\n                challenge2->value()));\n\n        return ValidationResult_t::VALIDATION_OK;\n    }\n\n    ERR_clear_error();\n\n    return ValidationResult_t::VALIDATION_FAILED;\n}\n\nSharedSecretHandle* PKIDH::get_shared_secret(\n        const HandshakeHandle& handshake_handle,\n        SecurityException& /*exception*/)\n{\n    const PKIHandshakeHandle& handshake = PKIHandshakeHandle::narrow(handshake_handle);\n\n    if (!handshake.nil())\n    {\n        SharedSecretHandle* sharedsecret = new SharedSecretHandle();\n        (*sharedsecret)->data_ = (*handshake->sharedsecret_)->data_;\n        return sharedsecret;\n    }\n\n    return nullptr;\n}\n\nbool PKIDH::set_listener(\n        AuthenticationListener* /*listener*/,\n        SecurityException& /*exception*/)\n{\n    return false;\n}\n\nbool PKIDH::get_identity_token(\n        IdentityToken** identity_token,\n        const IdentityHandle& handle,\n        SecurityException& /*exception*/)\n{\n    const PKIIdentityHandle& ihandle = PKIIdentityHandle::narrow(handle);\n\n    if (!ihandle.nil())\n    {\n        *identity_token = new IdentityToken(ihandle->identity_token_);\n        return true;\n    }\n\n    return false;\n}\n\nbool PKIDH::return_identity_token(\n        IdentityToken* token,\n        SecurityException& /*exception*/)\n{\n    delete token;\n    return true;\n}\n\nbool PKIDH::return_handshake_handle(\n        HandshakeHandle* handshake_handle,\n        SecurityException& /*exception*/)\n{\n    PKIHandshakeHandle* handle = &PKIHandshakeHandle::narrow(*handshake_handle);\n\n    if (!handle->nil())\n    {\n        delete handle;\n        return true;\n    }\n\n    return false;\n}\n\nbool PKIDH::return_identity_handle(\n        IdentityHandle* identity_handle,\n        SecurityException& /*exception*/)\n{\n    PKIIdentityHandle* handle = &PKIIdentityHandle::narrow(*identity_handle);\n\n    if (!handle->nil())\n    {\n        delete handle;\n        return true;\n    }\n\n    return false;\n}\n\nbool PKIDH::return_sharedsecret_handle(\n        SharedSecretHandle* sharedsecret_handle,\n        SecurityException& /*exception*/)\n{\n    delete sharedsecret_handle;\n    return true;\n}\n\nbool PKIDH::set_permissions_credential_and_token(\n        IdentityHandle& identity_handle,\n        PermissionsCredentialToken& permissions_credential_token,\n        SecurityException& exception)\n{\n    PKIIdentityHandle& ihandle = PKIIdentityHandle::narrow(identity_handle);\n\n    if (!ihandle.nil())\n    {\n        ihandle->permissions_credential_token_ = std::move(permissions_credential_token);\n        return true;\n    }\n    else\n    {\n        exception = _SecurityException_(\"Invalid identity handle\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n    }\n\n    return false;\n}\n\nbool PKIDH::get_authenticated_peer_credential_token(\n        PermissionsCredentialToken** token,\n        const IdentityHandle& identity_handle,\n        SecurityException& exception)\n{\n    const PKIIdentityHandle& handle = PKIIdentityHandle::narrow(identity_handle);\n\n    if (!handle.nil())\n    {\n        *token = new PermissionsCredentialToken(handle->permissions_credential_token_);\n        return true;\n    }\n    else\n    {\n        exception = _SecurityException_(\"Invalid handshake handle\");\n        EMERGENCY_SECURITY_LOGGING(\"PKIDH\", exception.what());\n    }\n\n    return false;\n}\n\nbool PKIDH::return_authenticated_peer_credential_token(\n        PermissionsCredentialToken* token,\n        SecurityException&)\n{\n    delete token;\n    return true;\n}\n", "hunk": "@@ -272,11 +272,6 @@ static EVP_PKEY* load_private_key(\n         }\n \n         key = pkidh.pkcs11_provider->load_private_key(certificate, file, password, exception);\n-\n-        if ( nullptr == key )\n-        {\n-            exception = _SecurityException_(std::string(\"PKCS11 URIs require libp11 \") + file);\n-        }\n     }\n     else\n     {\n", "comment": "Method `pkcs11_provider->load_private_key` is already filling out `exception` in case of error, so there is no need to change the exception message here. ```suggestion ```", "ids": ["23481", "a0f5b9987b732c3b8bb7023f6ea4c8d3a98fd13e", "161e60891ebe48aab1358626a9dcc38008108269"], "repo": "eProsima/Fast-DDS", "ghid": 2381, "old": "         }\n         key = pkidh.pkcs11_provider->load_private_key(certificate, file, password, exception);\n-\n-        if ( nullptr == key )\n-        {\n-            exception = _SecurityException_(std::string(\"PKCS11 URIs require libp11 \") + file);\n-        }\n     }\n     else\n     {", "new": "         }\n         key = pkidh.pkcs11_provider->load_private_key(certificate, file, password, exception);\n     }\n     else\n     {", "lang": "cpp", "norm_lang": "cpp"}
{"old_hunk": "@@ -0,0 +1,252 @@\n+import argparse\n+import os.path as osp\n+import warnings\n+\n+import mmcv\n+import numpy as np\n+import pycocotools.mask as mask_util\n+from mmcv import Config, DictAction\n+\n+from mmdet.core.evaluation import eval_map\n+from mmdet.core.mask.structures import (BitmapMasks, PolygonMasks,\n+                                        polygon_to_bitmap)\n+from mmdet.core.visualization.image import imshow_det_bboxes\n+from mmdet.datasets import build_dataset, retrieve_loading_pipeline\n+\n+\n+def visualize(img,\n+              annotation,\n+              result,\n+              class_names=None,\n+              show=True,\n+              show_mask=True,\n+              wait_time=0,\n+              out_file=None):\n+    if show_mask:\n+        gt_masks = annotation.get('gt_masks', None)\n+        if gt_masks is not None:\n+            if isinstance(gt_masks, BitmapMasks):", "oldf": "import argparse\nimport os.path as osp\nimport warnings\n\nimport mmcv\nimport numpy as np\nimport pycocotools.mask as mask_util\nfrom mmcv import Config, DictAction\n\nfrom mmdet.core.evaluation import eval_map\nfrom mmdet.core.mask.structures import (BitmapMasks, PolygonMasks,\n                                        polygon_to_bitmap)\nfrom mmdet.core.visualization.image import imshow_det_bboxes\nfrom mmdet.datasets import build_dataset, retrieve_loading_pipeline\n\n\ndef visualize(img,\n              annotation,\n              result,\n              class_names=None,\n              show=True,\n              show_mask=True,\n              wait_time=0,\n              out_file=None):\n    if show_mask:\n        gt_masks = annotation.get('gt_masks', None)\n        if gt_masks is not None:\n            if isinstance(gt_masks, BitmapMasks):\n                gt_masks = gt_masks.masks\n            elif isinstance(gt_masks, PolygonMasks):\n                height = gt_masks.height\n                width = gt_masks.width\n                polygon_gt_masks = []\n                for poly_per_obj in gt_masks.masks:\n                    polygon_gt_masks.append(\n                        polygon_to_bitmap(poly_per_obj, height, width))\n                gt_masks = np.stack(polygon_gt_masks).reshape(\n                    -1, height, width)\n            else:\n                warnings.warn('Unsupported data type')\n                gt_masks = None\n    else:\n        gt_masks = None\n    imshow_det_bboxes(\n        img,\n        annotation['bboxes'],\n        annotation['labels'],\n        gt_masks,\n        class_names=class_names,\n        bbox_color=(255, 102, 61),\n        text_color=(255, 102, 61),\n        mask_color=(255, 102, 61),\n        show=False)\n    if isinstance(result, tuple):\n        bbox_result, segm_result = result\n        if isinstance(segm_result, tuple):\n            segm_result = segm_result[0]  # ms rcnn\n    else:\n        bbox_result, segm_result = result, None\n    bboxes = np.vstack(bbox_result)\n    labels = [\n        np.full(bbox.shape[0], i, dtype=np.int32)\n        for i, bbox in enumerate(bbox_result)\n    ]\n    labels = np.concatenate(labels)\n    segms = None\n    if show_mask and segm_result is not None and len(labels) > 0:  # non empty\n        segms = mmcv.concat_list(segm_result)\n        segms = mask_util.decode(segms)\n        segms = segms.transpose(2, 0, 1)\n    imshow_det_bboxes(\n        img,\n        bboxes,\n        labels,\n        segms=segms,\n        class_names=class_names,\n        bbox_color=(72, 101, 241),\n        text_color=(72, 101, 241),\n        mask_color=(72, 101, 241),\n        show=show,\n        wait_time=wait_time,\n        out_file=out_file)\n\n\nclass ResultVisualizer(object):\n    \"\"\"Display and save evaluation results.\n\n    Args:\n        dataset (Dataset): A PyTorch dataset.\n        results (pickle object): pickle object from test results pkl file\n        topk (int): Number of the highest topk and\n            lowest topk after evaluation index sorting. Default: 20\n        show (bool): Whether to show the image. Default: True\n        wait_time (float): Value of waitKey param. Default: 0.\n        show_dir (str or None): The filename to write the image.\n            Default: 'work_dir'\n    \"\"\"\n\n    def __init__(self,\n                 dataset,\n                 results,\n                 topk=20,\n                 show=False,\n                 wait_time=0,\n                 show_dir='work_dir'):\n        self.topk = topk\n        self.dataset = dataset\n        assert self.topk > 0\n        if (self.topk * 2) > len(self.dataset):\n            self.topk = len(dataset) // 2\n        self.results = results\n        self.CLASSES = self.dataset.CLASSES\n        self.show = show\n        self.wait_time = wait_time\n        self.show_dir = show_dir\n\n    def _eval_fn(self, det_result, annotation):\n        # use only bbox det result\n        if isinstance(det_result, tuple):\n            bbox_det_result = [det_result[0]]\n        else:\n            bbox_det_result = [det_result]\n        # mAP\n        iou_thrs = np.linspace(\n            .5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)\n        mean_aps = []\n        for thr in iou_thrs:\n            mean_ap, _ = eval_map(bbox_det_result, [annotation], iou_thr=thr)\n            mean_aps.append(mean_ap)\n        return sum(mean_aps) / len(mean_aps)\n\n    def _save_image_gts_results(self, mAPs, out_dir=None):\n        mmcv.mkdir_or_exist(out_dir)\n\n        for mAP_info in mAPs:\n            index, mAP = mAP_info\n            data_info = self.dataset.prepare_train_img(index)\n\n            # calc save file path\n            filename = data_info['filename']\n            if data_info['img_prefix'] is not None:\n                filename = osp.join(data_info['img_prefix'], filename)\n            else:\n                filename = data_info['filename']\n            fname, name = osp.splitext(osp.basename(filename))\n            save_filename = fname + '_' + str(round(mAP, 3)) + name\n            out_file = osp.join(out_dir, save_filename)\n\n            visualize(\n                data_info['img'],\n                data_info['ann_info'],\n                self.results[index],\n                self.CLASSES,\n                self.show,\n                wait_time=self.wait_time,\n                out_file=out_file)\n\n    def evaluate_and_show(self):\n        _mAPs = {}\n        for i, (result, ) in enumerate(zip(self.results)):\n            # self.dataset[i] should not call directly\n            # because there is a risk of mismatch\n            data_info = self.dataset.prepare_train_img(i)\n            mAP = self._eval_fn(result, data_info['ann_info'])\n            _mAPs[i] = mAP\n\n        # descending select topk image\n        _mAPs = list(sorted(_mAPs.items(), key=lambda kv: kv[1]))\n        good_mAPs = _mAPs[-self.topk:]\n        bad_mAPs = _mAPs[:self.topk]\n\n        good_dir = osp.abspath(osp.join(self.show_dir, 'good'))\n        bad_dir = osp.abspath(osp.join(self.show_dir, 'bad'))\n        self._save_image_gts_results(good_mAPs, good_dir)\n        self._save_image_gts_results(bad_mAPs, bad_dir)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='MMDet eval image prediction result for each')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument(\n        'prediction_path', help='prediction path where test pkl result')\n    parser.add_argument(\n        'show_dir', help='directory where painted images will be saved')\n    parser.add_argument('--show', action='store_true', help='show results')\n    parser.add_argument(\n        '--wait-time',\n        type=float,\n        default=0,\n        help='the interval of show (s), 0 is block')\n    parser.add_argument(\n        'eval',\n        type=str,\n        help='evaluation metrics, which depends on the dataset, e.g., \"bbox\",'\n        ' \"segm\", \"proposal\" for COCO, and \"mAP\", \"recall\" for PASCAL VOC')\n    parser.add_argument(\n        '--topk',\n        default=20,\n        type=int,\n        help='Saved Number of the highest topk '\n        'and lowest topk after index sorting')\n    parser.add_argument(\n        '--cfg-options',\n        nargs='+',\n        action=DictAction,\n        help='override some settings in the used config, the key-value pair '\n        'in xxx=yyy format will be merged into config file. If the value to '\n        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n        'Note that the quotation marks are necessary and that no white space '\n        'is allowed.')\n    parser.add_argument(\n        '--eval-options',\n        nargs='+',\n        action=DictAction,\n        help='custom options for evaluation, the key-value pair in xxx=yyy '\n        'format will be kwargs for dataset.evaluate() function')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    mmcv.check_file_exist(args.prediction_path)\n\n    cfg = Config.fromfile(args.config)\n    if args.cfg_options is not None:\n        cfg.merge_from_dict(args.cfg_options)\n    cfg.data.test.test_mode = True\n    # import modules from string list.\n    if cfg.get('custom_imports', None):\n        from mmcv.utils import import_modules_from_strings\n        import_modules_from_strings(**cfg['custom_imports'])\n\n    cfg.data.test.pop('samples_per_gpu', 0)\n    cfg.data.test.pipeline = retrieve_loading_pipeline(cfg.data.train.pipeline)\n    dataset = build_dataset(cfg.data.test)\n    outputs = mmcv.load(args.prediction_path)\n    result_visualizer = ResultVisualizer(\n        dataset,\n        outputs,\n        topk=args.topk,\n        show=args.show,\n        wait_time=args.wait_time,\n        show_dir=args.show_dir)\n    result_visualizer.evaluate_and_show()\n\n\nif __name__ == '__main__':\n    main()\n", "hunk": "@@ -8,8 +8,7 @@ import pycocotools.mask as mask_util\n from mmcv import Config, DictAction\n \n from mmdet.core.evaluation import eval_map\n-from mmdet.core.mask.structures import (BitmapMasks, PolygonMasks,\n-                                        polygon_to_bitmap)\n+from mmdet.core.mask.structures import BitmapMasks, PolygonMasks\n from mmdet.core.visualization.image import imshow_det_bboxes\n from mmdet.datasets import build_dataset, retrieve_loading_pipeline\n \n", "comment": "We can use the API gt_masks.to_array", "ids": ["22248", "93add8b179bc38e25540157a91b1a5bd22d27b77", "64221355edb350db6c929269e113397988d9f36c"], "repo": "open-mmlab/mmdetection", "ghid": 4441, "old": " from mmcv import Config, DictAction\n from mmdet.core.evaluation import eval_map\n-from mmdet.core.mask.structures import (BitmapMasks, PolygonMasks,\n-                                        polygon_to_bitmap)\n from mmdet.core.visualization.image import imshow_det_bboxes\n from mmdet.datasets import build_dataset, retrieve_loading_pipeline", "new": " from mmcv import Config, DictAction\n from mmdet.core.evaluation import eval_map\n+from mmdet.core.mask.structures import BitmapMasks, PolygonMasks\n from mmdet.core.visualization.image import imshow_det_bboxes\n from mmdet.datasets import build_dataset, retrieve_loading_pipeline", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -46,8 +47,6 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import com.fasterxml.jackson.databind.ObjectMapper;", "oldf": "/*\n * Copyright 2021 Red Hat, Inc. and/or its affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *       http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.kie.kogito.codegen.prediction;\n\nimport java.io.File;\nimport java.nio.file.Path;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport org.drools.compiler.builder.impl.KnowledgeBuilderImpl;\nimport org.drools.core.impl.InternalKnowledgeBase;\nimport org.drools.core.impl.KnowledgeBaseImpl;\nimport org.kie.api.io.Resource;\nimport org.kie.api.io.ResourceType;\nimport org.kie.kogito.codegen.api.ApplicationSection;\nimport org.kie.kogito.codegen.api.GeneratedFile;\nimport org.kie.kogito.codegen.api.GeneratedFileType;\nimport org.kie.kogito.codegen.api.context.KogitoBuildContext;\nimport org.kie.kogito.codegen.api.io.CollectedResource;\nimport org.kie.kogito.codegen.core.AbstractGenerator;\nimport org.kie.kogito.codegen.prediction.config.PredictionConfigGenerator;\nimport org.kie.kogito.codegen.rules.IncrementalRuleCodegen;\nimport org.kie.kogito.pmml.openapi.api.PMMLOASResult;\nimport org.kie.kogito.pmml.openapi.factories.PMMLOASResultFactory;\nimport org.kie.pmml.commons.model.HasNestedModels;\nimport org.kie.pmml.commons.model.HasSourcesMap;\nimport org.kie.pmml.commons.model.KiePMMLFactoryModel;\nimport org.kie.pmml.commons.model.KiePMMLModel;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport static java.util.stream.Collectors.toList;\nimport static org.kie.kogito.codegen.rules.KogitoPackageSources.getReflectConfigFile;\nimport static org.kie.pmml.commons.utils.KiePMMLModelUtils.getSanitizedClassName;\nimport static org.kie.pmml.evaluator.assembler.service.PMMLCompilerService.getKiePMMLModelsFromResourceWithSources;\n\npublic class PredictionCodegen extends AbstractGenerator {\n\n    public static final String DMN_JPMML_CLASS = \"org.kie.dmn.jpmml.DMNjPMMLInvocationEvaluator\";\n    public static final String GENERATOR_NAME = \"predictions\";\n    private static final Logger LOGGER = LoggerFactory.getLogger(PredictionCodegen.class);\n    private static final GeneratedFileType PMML_TYPE = GeneratedFileType.of(\"PMML\", GeneratedFileType.Category.SOURCE);\n    private static final String DECLARED_TYPE_IDENTIFIER = \"org.drools.core.factmodel.GeneratedFact\";\n    private final List<PMMLResource> resources;\n\n    public PredictionCodegen(KogitoBuildContext context, List<PMMLResource> resources) {\n        super(context, GENERATOR_NAME, new PredictionConfigGenerator(context));\n        this.resources = resources;\n    }\n\n    public static PredictionCodegen ofCollectedResources(KogitoBuildContext context,\n            Collection<CollectedResource> resources) {\n        if (context.hasClassAvailable(DMN_JPMML_CLASS)) {\n            LOGGER.info(\"jpmml libraries available on classpath, skipping kogito-pmml parsing and compilation\");\n            return ofPredictions(context, Collections.emptyList());\n        }\n        List<PMMLResource> pmmlResources = resources.stream()\n                .filter(r -> r.resource().getResourceType() == ResourceType.PMML)\n                .flatMap(r -> parsePredictions(r.basePath(), Collections.singletonList(r.resource())).stream())\n                .collect(toList());\n        return ofPredictions(context, pmmlResources);\n    }\n\n    private static PredictionCodegen ofPredictions(KogitoBuildContext context, List<PMMLResource> resources) {\n        return new PredictionCodegen(context, resources);\n    }\n\n    private static List<PMMLResource> parsePredictions(Path path, List<Resource> resources) {\n        final InternalKnowledgeBase knowledgeBase = new KnowledgeBaseImpl(\"PMML\", null);\n        KnowledgeBuilderImpl kbuilderImpl = new KnowledgeBuilderImpl(knowledgeBase);\n        List<PMMLResource> toReturn = new ArrayList<>();\n        resources.forEach(resource -> {\n            List<KiePMMLModel> kiePMMLModels = getKiePMMLModelsFromResourceWithSources(kbuilderImpl, resource);\n            String modelPath = resource.getSourcePath();\n            PMMLResource toAdd = new PMMLResource(kiePMMLModels, path, modelPath);\n            toReturn.add(toAdd);\n        });\n        return toReturn;\n    }\n\n    @Override\n    public Optional<ApplicationSection> section() {\n        return Optional.of(new PredictionModelsGenerator(context(), applicationCanonicalName(), resources));\n    }\n\n    @Override\n    public List<GeneratedFile> generate() {\n        List<GeneratedFile> files = new ArrayList<>();\n        for (PMMLResource resource : resources) {\n            generateModelsFromResource(files, resource);\n        }\n        return files;\n    }\n\n    @Override\n    public int priority() {\n        return 40;\n    }\n\n    private void generateModelsFromResource(List<GeneratedFile> files, PMMLResource resource) {\n        for (KiePMMLModel model : resource.getKiePmmlModels()) {\n            generateModel(files, model, resource);\n        }\n    }\n\n    private void generateModel(List<GeneratedFile> files, KiePMMLModel model, PMMLResource resource) {\n        if (model.getName() == null || model.getName().isEmpty()) {\n            String errorMessage = String.format(\"Model name should not be empty inside %s\",\n                    resource.getModelPath());\n            throw new RuntimeException(errorMessage);\n        }\n\n        generateModelBaseFiles(files, model, resource);\n        generateModelRESTFiles(files, model);\n    }\n\n    private void generateModelBaseFiles(List<GeneratedFile> files, KiePMMLModel model, PMMLResource resource) {\n        if (!(model instanceof HasSourcesMap)) {\n            String errorMessage = String.format(\"Expecting HasSourcesMap instance, retrieved %s inside %s\",\n                    model.getClass().getName(), resource.getModelPath());\n            throw new RuntimeException(errorMessage);\n        }\n\n        Map<String, String> sourceMap = ((HasSourcesMap) model).getSourcesMap();\n        for (Map.Entry<String, String> sourceMapEntry : sourceMap.entrySet()) {\n            String path = sourceMapEntry.getKey().replace('.', File.separatorChar) + \".java\";\n            files.add(new GeneratedFile(PMML_TYPE, path, sourceMapEntry.getValue()));\n        }\n\n        Map<String, String> rulesSourceMap = ((HasSourcesMap) model).getRulesSourcesMap();\n        if (rulesSourceMap != null) {\n            List<String> pojoClasses = new ArrayList<>();\n\n            for (Map.Entry<String, String> rulesSourceMapEntry : rulesSourceMap.entrySet()) {\n                String path = rulesSourceMapEntry.getKey().replace('.', File.separatorChar) + \".java\";\n                files.add(new GeneratedFile(IncrementalRuleCodegen.RULE_TYPE, path, rulesSourceMapEntry.getValue()));\n\n                if (rulesSourceMapEntry.getValue().contains(DECLARED_TYPE_IDENTIFIER)) {\n                    pojoClasses.add(rulesSourceMapEntry.getKey());\n                }\n            }\n\n            if (!pojoClasses.isEmpty()) {\n                org.drools.modelcompiler.builder.GeneratedFile reflectConfigFile =\n                        getReflectConfigFile(model.getKModulePackageName(), pojoClasses);\n                files.add(new GeneratedFile(GeneratedFileType.RESOURCE, reflectConfigFile.getPath(), new String(reflectConfigFile.getData())));\n            }\n        }\n\n        if (model instanceof HasNestedModels) {\n            for (KiePMMLModel nestedModel : ((HasNestedModels) model).getNestedModels()) {\n                generateModelBaseFiles(files, nestedModel, resource);\n            }\n        }\n    }\n\n    private void generateModelRESTFiles(List<GeneratedFile> files, KiePMMLModel model) {\n        if (!context().hasREST() || (model instanceof KiePMMLFactoryModel)) {\n            return;\n        }\n\n        PMMLRestResourceGenerator resourceGenerator = new PMMLRestResourceGenerator(context(), model, applicationCanonicalName());\n        files.add(new GeneratedFile(REST_TYPE, resourceGenerator.generatedFilePath(), resourceGenerator.generate()));\n\n        PMMLOASResult oasResult = PMMLOASResultFactory.getPMMLOASResult(model);\n        try {\n            String jsonContent = new ObjectMapper().writeValueAsString(oasResult.jsonSchemaNode());\n            String jsonFile = String.format(\"%s.json\", getSanitizedClassName(model.getName()));\n            String jsonFilePath = String.format(\"META-INF/resources/%s\", jsonFile);\n            files.add(new GeneratedFile(GeneratedFileType.RESOURCE, jsonFilePath, jsonContent));\n        } catch (Exception e) {\n            LOGGER.warn(\"Failed to write OAS schema\");\n        }\n    }\n}\n", "hunk": "@@ -47,6 +47,8 @@ import org.kie.pmml.commons.model.KiePMMLModel;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.fasterxml.jackson.databind.ObjectMapper;\n+\n import static java.util.stream.Collectors.toList;\n import static org.kie.kogito.codegen.rules.KogitoPackageSources.getReflectConfigFile;\n import static org.kie.pmml.commons.utils.KiePMMLModelUtils.getSanitizedClassName;\n", "comment": "Please compile locally to fix import sorting", "ids": ["19584", "b0df7d0f8b4a58503d7c1891df33ea37f57388ab", "6448ac4cbbf4bfad4b2c43d462fc712be7d9506a"], "repo": "kiegroup/kogito-runtimes", "ghid": 1283, "old": " import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import static java.util.stream.Collectors.toList;\n import static org.kie.kogito.codegen.rules.KogitoPackageSources.getReflectConfigFile;\n import static org.kie.pmml.commons.utils.KiePMMLModelUtils.getSanitizedClassName;", "new": " import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+\n import static java.util.stream.Collectors.toList;\n import static org.kie.kogito.codegen.rules.KogitoPackageSources.getReflectConfigFile;\n import static org.kie.pmml.commons.utils.KiePMMLModelUtils.getSanitizedClassName;", "lang": "java", "norm_lang": "java"}
{"old_hunk": "@@ -40,13 +40,21 @@ using SimTK::Vec3;\n /**\n  * Default constructor.\n  */\n-Station::Station() :\n-   ModelComponent()\n+Station::Station() : Super()", "oldf": "/* -------------------------------------------------------------------------- *\n *                            OpenSim:  Station.cpp                           *\n * -------------------------------------------------------------------------- *\n * The OpenSim API is a toolkit for musculoskeletal modeling and simulation.  *\n * See http://opensim.stanford.edu and the NOTICE file for more information.  *\n * OpenSim is developed at Stanford University and supported by the US        *\n * National Institutes of Health (U54 GM072970, R24 HD065690) and by DARPA    *\n * through the Warrior Web program.                                           *\n *                                                                            *\n * Copyright (c) 2005-2016 Stanford University and the Authors                *\n * Author(s): Ayman Habib, Ajay Seth                                          *\n *                                                                            *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may    *\n * not use this file except in compliance with the License. You may obtain a  *\n * copy of the License at http://www.apache.org/licenses/LICENSE-2.0.         *\n *                                                                            *\n * Unless required by applicable law or agreed to in writing, software        *\n * distributed under the License is distributed on an \"AS IS\" BASIS,          *\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.   *\n * See the License for the specific language governing permissions and        *\n * limitations under the License.                                             *\n * -------------------------------------------------------------------------- */\n\n//=============================================================================\n// INCLUDES\n//=============================================================================\n#include \"Station.h\"\n#include \"Model.h\"\n\n//=============================================================================\n// STATICS\n//=============================================================================\nusing namespace std;\nusing namespace OpenSim;\nusing SimTK::Vec3;\n//=============================================================================\n// CONSTRUCTOR(S) AND DESTRUCTOR\n//=============================================================================\n//_____________________________________________________________________________\n/**\n * Default constructor.\n */\nStation::Station() : Super()\n{\n    setNull();\n    constructInfrastructure();\n}\n\nStation::Station(const PhysicalFrame& frame, const SimTK::Vec3& location)\n    : Super()\n{\n    setNull();\n    constructInfrastructure();\n    setReferenceFrame(frame);\n    set_location(location);\n}\n\n//_____________________________________________________________________________\n/**\n * Destructor.\n */\nStation::~Station()\n{\n}\n\n//_____________________________________________________________________________\n/**\n* Set the data members of this Station to their null values.\n*/\nvoid Station::setNull()\n{\n    setAuthors(\"Ayman Habib\");\n}\n\n//_____________________________________________________________________________\n/**\n* Connect properties to local pointers.\n*/\nvoid Station::constructProperties()\n{\n    //Default location\n    SimTK::Vec3 origin(0.0, 0.0, 0.0);\n    // Location in Body \n    constructProperty_location(origin);\n}\n\n\nvoid Station::constructConnectors()\n{\n    constructConnector<PhysicalFrame>(\"reference_frame\");\n}\n\n/*\n * Return the reference frame with respect to which this station is defined\n*/\nconst PhysicalFrame& Station::getReferenceFrame() const\n{\n    return getConnector<PhysicalFrame>(\"reference_frame\").getConnectee();\n}\n\n/*\n * setReferenceFrame sets the \"reference_frame\" connection\n */\nvoid Station::setReferenceFrame(const OpenSim::PhysicalFrame& aFrame)\n{\n    updConnector<PhysicalFrame>(\"reference_frame\").connect(aFrame);\n}\n\nSimTK::Vec3 Station::findLocationInFrame(const SimTK::State& s,\n        const OpenSim::Frame& aFrame) const\n{\n    // transform location from the station's frame to the other frame\n    return getReferenceFrame().findLocationInAnotherFrame(s, \n                                                get_location(), aFrame);\n}\n\nSimTK::Vec3 Station::calcLocationInGround(const SimTK::State& s) const\n{\n    return getReferenceFrame().getTransformInGround(s)*get_location();\n}\n\nSimTK::Vec3 Station::calcVelocityInGround(const SimTK::State& s) const\n{\n    // compute the local position vector of the station in its reference frame\n    // expressed in ground\n    Vec3 r = getReferenceFrame().getTransformInGround(s).R()*get_location();\n    const SimTK::SpatialVec& V_GF = getReferenceFrame().getVelocityInGround(s);\n\n    // The velocity of the station in ground is a function of its frame's\n    // linear (vF = V_GF[1]) and angular (omegaF = A_GF[0]) velocity, such that\n    // velocity of the station: v = vF + omegaF x r\n    return V_GF[1] + V_GF[0] % r;\n}\n\nSimTK::Vec3 Station::calcAccelerationInGround(const SimTK::State& s) const\n{\n    // The spatial velocity of the reference frame expressed in ground\n    const SimTK::SpatialVec& V_GF = getReferenceFrame().getVelocityInGround(s);\n    // The spatial acceleration of the reference frame expressed in ground\n    const SimTK::SpatialVec& A_GF = getReferenceFrame().getAccelerationInGround(s);\n    // compute the local position vector of the point in its reference frame\n    // expressed in ground\n    Vec3 r = getReferenceFrame().getTransformInGround(s).R()*get_location();\n\n    // The acceleration of the station in ground is a function of its frame's\n    // linear (aF = A_GF[1]) and angular (alpha = A_GF[0]) accelerations and\n    // Corriolois acceleration due to the angular velocity (omega = V_GF[0]) of\n    // its frame, such that: a = aF + alphaF x r + omegaF x (omegaF x r)\n    return A_GF[1] + A_GF[0]%r +  V_GF[0] % (V_GF[0] % r) ;\n}\n", "hunk": "@@ -40,14 +40,14 @@ using SimTK::Vec3;\n /**\n  * Default constructor.\n  */\n-Station::Station() : Super()\n+Station::Station() : Point()\n {\n     setNull();\n     constructInfrastructure();\n }\n \n Station::Station(const PhysicalFrame& frame, const SimTK::Vec3& location)\n-    : Super()\n+    : Point()\n {\n     setNull();\n     constructInfrastructure();\n", "comment": "No need to change, but we don't use this convention in general (of using the `Super` typedef in constructors); we should be consistent.", "ids": ["20477", "0595fa56ea74f009e5e7d803d483200ed5972d31", "564be081fa9499af325500314a853cc0c65ead33"], "repo": "opensim-org/opensim-core", "ghid": 915, "old": " /**\n  * Default constructor.\n  */\n-Station::Station() : Super()\n {\n     setNull();\n     constructInfrastructure();\n }\n Station::Station(const PhysicalFrame& frame, const SimTK::Vec3& location)\n-    : Super()\n {\n     setNull();\n     constructInfrastructure();", "new": " /**\n  * Default constructor.\n  */\n+Station::Station() : Point()\n {\n     setNull();\n     constructInfrastructure();\n }\n Station::Station(const PhysicalFrame& frame, const SimTK::Vec3& location)\n+    : Point()\n {\n     setNull();\n     constructInfrastructure();", "lang": "cpp", "norm_lang": "cpp"}
{"old_hunk": "@@ -59,6 +59,7 @@\n \tEXPECTED_CHANNEL_QUALIFIER(ERROR, 120, \"expected channel reference '':<channel>'' but found ''{0}''\"), //\n \tEXPECTED_CHANNEL_NAME(ERROR, 121, \"expected channel name but found ''{0}''\"), //\n \tILLEGAL_STREAM_NAME(ERROR, 122, \"illegal name for a stream ''{0}''\"), //\n+\tILLEGAL_TASK_NAME(ERROR, 122, \"illegal name for a task ''{0}''\"), //", "oldf": "/*\n * Copyright 2015 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.cloud.data.core.dsl;\n\nimport static org.springframework.cloud.data.core.dsl.DSLMessage.Kind.ERROR;\n\nimport java.text.MessageFormat;\n\n/**\n * Contains all the messages that can be produced during Spring Cloud Data DSL\n * parsing. Each message has a kind (info, warn, error) and a code number.\n * Tests can be written to expect particular code numbers rather than\n * particular text, enabling the message text to more easily be modified\n * and the tests to run successfully in different locales.\n * <p>\n * When a message is formatted, it will have this kind of form\n *\n * <pre class=\"code\">\n * 105E: (pos 34): Expected an argument value but was ' '\n * </pre>\n *\n * </code> The prefix captures the code and the error kind, whilst the position is included if it is known.\n *\n * @author Andy Clement\n */\npublic enum DSLMessage {\n\n\tUNEXPECTED_DATA_AFTER_STREAMDEF(ERROR, 100, \"Found unexpected data after stream definition: ''{0}''\"), //\n\tNO_WHITESPACE_BEFORE_ARG_NAME(ERROR, 101, \"No whitespace allowed between '--' and option name\"), //\n\tNO_WHITESPACE_BEFORE_ARG_EQUALS(ERROR, 102, \"No whitespace allowed after argument name and before '='\"), //\n\tNO_WHITESPACE_BEFORE_ARG_VALUE(ERROR, 103, \"No whitespace allowed after '=' and before option value\"), //\n\tMORE_INPUT(ERROR, 104, \"After parsing a valid stream, there is still more data: ''{0}''\"), EXPECTED_ARGUMENT_VALUE(\n\t\t\tERROR, 105, \"Expected an argument value but was ''{0}''\"), NON_TERMINATING_DOUBLE_QUOTED_STRING(ERROR, 106,\n\t\t\t\"Cannot find terminating \\\" for string\"), //\n\tNON_TERMINATING_QUOTED_STRING(ERROR, 107, \"Cannot find terminating '' for string\"), //\n\tMISSING_CHARACTER(ERROR, 108, \"missing expected character ''{0}''\"), NOT_EXPECTED_TOKEN(ERROR, 111,\n\t\t\t\"Unexpected token.  Expected ''{0}'' but was ''{1}''\"), OOD(ERROR, 112, \"Unexpectedly ran out of input\"), //\n\tUNEXPECTED_ESCAPE_CHAR(ERROR, 114, \"unexpected escape character.\"), //\n\tUNEXPECTED_DATA(ERROR, 115, \"unexpected data in stream definition ''{0}''\"), //\n\tUNRECOGNIZED_STREAM_REFERENCE(ERROR, 116, \"unrecognized stream reference ''{0}''\"), //\n\tUNRECOGNIZED_MODULE_REFERENCE(ERROR, 117, \"unrecognized module reference ''{0}''\"), //\n\tEXPECTED_MODULENAME(ERROR, 118, \"expected module name but found ''{0}''\"), //\n\tEXPECTED_WHITESPACE_AFTER_MODULE_BEFORE_ARGUMENT(ERROR, 119,\n\t\t\t\"expected whitespace after module name and before argument\"), //\n\tEXPECTED_CHANNEL_QUALIFIER(ERROR, 120, \"expected channel reference '':<channel>'' but found ''{0}''\"), //\n\tEXPECTED_CHANNEL_NAME(ERROR, 121, \"expected channel name but found ''{0}''\"), //\n\tILLEGAL_STREAM_NAME(ERROR, 122, \"illegal name for a stream ''{0}''\"), //\n\tILLEGAL_TASK_NAME(ERROR, 122, \"illegal name for a task ''{0}''\"), //\n\tMISSING_VALUE_FOR_VARIABLE(ERROR, 125, \"no value specified for variable ''{0}'' when using substream\"), //\n\tVARIABLE_NOT_TERMINATED(ERROR, 126, \"unable to find variable terminator ''}'' in argument ''{0}''\"), //\n\tAMBIGUOUS_MODULE_NAME(ERROR,\n\t\t\t129,\n\t\t\t\"ambiguous module name ''{0}'' in stream named ''{1}'', appears at both position {2} and {3}\"), //\n\tSTREAM_NAME_MATCHING_MODULE_NAME(ERROR, 130,\n\t\t\t\"Stream name ''{0}'' same as that of its modules' names is not allowed.\"), //\n\tCHANNEL_INDEXING_NOT_ALLOWED(ERROR, 131, \"Channel does not support indexing\"), //\n\tONLY_A_TAP_ON_A_STREAM_OR_JOB_CAN_BE_INDEXED(ERROR, 132, \"Only a tap on a stream or job can be indexed\"), //\n\tEXPECTED_CHANNEL_PREFIX_QUEUE_TOPIC(ERROR,\n\t\t\t133,\n\t\t\t\"Expected channel prefix of ''queue'' or ''topic'' but found ''{0}''\"), //\n\tEXPECTED_CHANNEL_PREFIX_QUEUE_TOPIC_TAP(ERROR,\n\t\t\t134,\n\t\t\t\"Expected channel prefix of ''queue'', ''topic'' or ''tap'' but found ''{0}''\"), //\n\tCANNOT_USE_COMPOSEDMODULE_HERE_AS_IT_DEFINES_SOURCE_CHANNEL(ERROR,\n\t\t\t135,\n\t\t\t\"cannot use composed module ''{0}'' here because it defines a source channel\"), //\n\tCANNOT_USE_COMPOSEDMODULE_HERE_AS_IT_DEFINES_SINK_CHANNEL(ERROR,\n\t\t\t136,\n\t\t\t\"cannot use composed module ''{0}'' here because it defines a sink channel\"), //\n\tCANNOT_USE_COMPOSEDMODULE_HERE_ALREADY_HAS_SOURCE_CHANNEL(ERROR,\n\t\t\t137,\n\t\t\t\"cannot use composed module ''{0}'' here, both that composed module and this stream define a source channel\"), //\n\tCANNOT_USE_COMPOSEDMODULE_HERE_ALREADY_HAS_SINK_CHANNEL(ERROR,\n\t\t\t138,\n\t\t\t\"cannot use composed module ''{0}'' here, both that composed module and this stream define a sink channel\"), //\n\tNO_WHITESPACE_IN_CHANNEL_DEFINITION(ERROR, 139, \"no whitespace allowed between components in a channel name\"), //\n\tNO_WHITESPACE_BETWEEN_LABEL_NAME_AND_COLON(ERROR, 140, \"no whitespace allowed between label name and colon\"), //\n\tTAP_NEEDS_THREE_COMPONENTS(ERROR,\n\t\t\t141,\n\t\t\t\"a tap needs at least three components (tap:queue:[name], tap:topic:[name], tap:stream:[streamname] or tap:job:[jobname])\"), //\n\tNOT_ALLOWED_TO_TAP_THAT(ERROR,\n\t\t\t142,\n\t\t\t\"tapping only allowed on 'stream', 'topic', 'job' or 'queue'. Not allowed on ''{0}''\"), //\n\tDUPLICATE_LABEL(ERROR,\n\t\t\t143,\n\t\t\t\"Label ''{0}'' should be unique but module ''{1}'' (at position {2}) and module ''{3}'' (at position {4}) both use it\"), //\n\tMODULE_REFERENCE_NOT_UNIQUE(ERROR,\n\t\t\t144,\n\t\t\t\"Reference to ''{0}'' is not unique in the target stream ''{1}'', please label the relevant module and use the label, or use a suffix index to indicate which occurrence of the module, e.g. ''{0}.0''\"), //\n\tNO_WHITESPACE_IN_DOTTED_NAME(ERROR,\n\t\t\t145,\n\t\t\t\"No whitespace is allowed between dot and components of a name\"),\n\tNAMED_CHANNELS_UNSUPPORTED_HERE(ERROR, 146, \"A named channel is not supported in this kind of definition\"), ;\n\n\tprivate Kind kind;\n\n\tprivate int code;\n\n\tprivate String message;\n\n\tprivate DSLMessage(Kind kind, int code, String message) {\n\t\tthis.kind = kind;\n\t\tthis.code = code;\n\t\tthis.message = message;\n\t}\n\n\t/**\n\t * Produce a complete message including the prefix, the position (if known) and with the inserts applied to the\n\t * message.\n\t *\n\t * @param pos the position, if less than zero it is ignored and not included in the message\n\t * @param inserts the inserts to put into the formatted message\n\t * @return a formatted message\n\t */\n\tpublic String formatMessage(int pos, Object... inserts) {\n\t\tStringBuilder formattedMessage = new StringBuilder();\n\t\tformattedMessage.append(code);\n\t\t// switch (kind) {\n\t\t// case WARNING:\n\t\t// formattedMessage.append(\"W\");\n\t\t// break;\n\t\t// case INFO:\n\t\t// formattedMessage.append(\"I\");\n\t\t// break;\n\t\t// case ERROR:\n\t\tformattedMessage.append(\"E\");\n\t\t// break;\n\t\t// }\n\t\tformattedMessage.append(\":\");\n\t\tif (pos != -1) {\n\t\t\tformattedMessage.append(\"(pos \").append(pos).append(\"): \");\n\t\t}\n\t\tformattedMessage.append(MessageFormat.format(message, inserts));\n\t\treturn formattedMessage.toString();\n\t}\n\n\tpublic Kind getKind() {\n\t\treturn kind;\n\t}\n\n\tpublic static enum Kind {\n\t\tINFO, WARNING, ERROR\n\t}\n\n}", "hunk": "@@ -59,7 +59,7 @@ public enum DSLMessage {\n \tEXPECTED_CHANNEL_QUALIFIER(ERROR, 120, \"expected channel reference '':<channel>'' but found ''{0}''\"), //\n \tEXPECTED_CHANNEL_NAME(ERROR, 121, \"expected channel name but found ''{0}''\"), //\n \tILLEGAL_STREAM_NAME(ERROR, 122, \"illegal name for a stream ''{0}''\"), //\n-\tILLEGAL_TASK_NAME(ERROR, 122, \"illegal name for a task ''{0}''\"), //\n+\tILLEGAL_TASK_NAME(ERROR, 123, \"illegal name for a task ''{0}''\"), //\n \tMISSING_VALUE_FOR_VARIABLE(ERROR, 125, \"no value specified for variable ''{0}'' when using substream\"), //\n \tVARIABLE_NOT_TERMINATED(ERROR, 126, \"unable to find variable terminator ''}'' in argument ''{0}''\"), //\n \tAMBIGUOUS_MODULE_NAME(ERROR,\n", "comment": "I think the code `122` needs to change as it is already used by `ILLEGAL_STREAM_NAME`", "ids": ["7209", "23b4563ee6a1fb7b873282a25f52622fdcf7ceaa", "0d78db39b6582de956c1b18355eaef4faafa40d5"], "repo": "spring-cloud/spring-cloud-dataflow", "ghid": 26, "old": " \tEXPECTED_CHANNEL_QUALIFIER(ERROR, 120, \"expected channel reference '':<channel>'' but found ''{0}''\"), //\n \tEXPECTED_CHANNEL_NAME(ERROR, 121, \"expected channel name but found ''{0}''\"), //\n \tILLEGAL_STREAM_NAME(ERROR, 122, \"illegal name for a stream ''{0}''\"), //\n-\tILLEGAL_TASK_NAME(ERROR, 122, \"illegal name for a task ''{0}''\"), //\n \tMISSING_VALUE_FOR_VARIABLE(ERROR, 125, \"no value specified for variable ''{0}'' when using substream\"), //\n \tVARIABLE_NOT_TERMINATED(ERROR, 126, \"unable to find variable terminator ''}'' in argument ''{0}''\"), //\n \tAMBIGUOUS_MODULE_NAME(ERROR,", "new": " \tEXPECTED_CHANNEL_QUALIFIER(ERROR, 120, \"expected channel reference '':<channel>'' but found ''{0}''\"), //\n \tEXPECTED_CHANNEL_NAME(ERROR, 121, \"expected channel name but found ''{0}''\"), //\n \tILLEGAL_STREAM_NAME(ERROR, 122, \"illegal name for a stream ''{0}''\"), //\n+\tILLEGAL_TASK_NAME(ERROR, 123, \"illegal name for a task ''{0}''\"), //\n \tMISSING_VALUE_FOR_VARIABLE(ERROR, 125, \"no value specified for variable ''{0}'' when using substream\"), //\n \tVARIABLE_NOT_TERMINATED(ERROR, 126, \"unable to find variable terminator ''}'' in argument ''{0}''\"), //\n \tAMBIGUOUS_MODULE_NAME(ERROR,", "lang": "java", "norm_lang": "java"}
{"old_hunk": "@@ -21,16 +21,13 @@ def GenerateConfig(context):\n   resources = []\n \n   resources.append({\n-      \"name\": \"inventory-database\",\n-      \"type\": \"sqladmin.v1beta4.database\",\n-      \"metadata\": {\n-          \"dependsOn\": [\"inventory-instance\"]", "oldf": "# Copyright 2017 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Creates a Cloud SQL database template for forseti_inventory.\"\"\"\n\n\ndef GenerateConfig(context):\n  \"\"\"Generate configuration.\"\"\"\n\n  resources = []\n\n  resources.append({\n      'name': context.env['name'],\n      'type': 'sqladmin.v1beta4.database',\n      'properties': {\n          'name': context.properties['database-name'],\n          'project': context.env['project'],\n          'instance': '$(ref.{}.name)'.format(context.env['deployment'])\n      }\n  })\n\n  return {'resources': resources}\n", "hunk": "@@ -26,7 +26,7 @@ def GenerateConfig(context):\n       'properties': {\n           'name': context.properties['database-name'],\n           'project': context.env['project'],\n-          'instance': '$(ref.{}.name)'.format(context.env['deployment'])\n+          'instance': '$(ref.cloudsql-instance.name)'\n       }\n   })\n \n", "comment": "Don't we still want DM to wait to deploy this configuration until the instance is established?", "ids": ["24701", "a8bd4afb7bc7d7db1843a26e85683ebcf8d88c3c", "827e5d1c0081d8b2a63d8561623c150cf453dd6d"], "repo": "forseti-security/forseti-security", "ghid": 18, "old": "       'properties': {\n           'name': context.properties['database-name'],\n           'project': context.env['project'],\n-          'instance': '$(ref.{}.name)'.format(context.env['deployment'])\n       }\n   })", "new": "       'properties': {\n           'name': context.properties['database-name'],\n           'project': context.env['project'],\n+          'instance': '$(ref.cloudsql-instance.name)'\n       }\n   })", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -197,10 +198,13 @@ void MessageReceiver::processCDRMsg(\n \n     if (decode_ret == 0)\n     {\n-        // Swap\n-        std::swap(msg, auxiliary_buffer);\n+        // The original CDRMessage buffer (msg) now points to the proprietary temporary buffer crypto_msg_.\n+        // The auxiliary buffer now points to the propietary temporary buffer crypto_submsg_.\n+        // This way each decoded submessage will be process using the crypto_submsg_ buffer.", "oldf": "// Copyright 2016 Proyectos y Sistemas de Mantenimiento SL (eProsima).\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n/**\n * @file MessageReceiver.cpp\n *\n */\n\n#include <fastdds/rtps/messages/MessageReceiver.h>\n\n#include <fastdds/dds/log/Log.hpp>\n\n#include <fastdds/rtps/reader/RTPSReader.h>\n#include <fastdds/rtps/writer/RTPSWriter.h>\n\n#include <fastdds/core/policy/ParameterList.hpp>\n#include <rtps/participant/RTPSParticipantImpl.h>\n\n#include <cassert>\n#include <limits>\n#include <mutex>\n\n#define INFO_SRC_SUBMSG_LENGTH 20\n\n#define IDSTRING \"(ID:\" << std::this_thread::get_id() << \") \" <<\n\nusing ParameterList = eprosima::fastdds::dds::ParameterList;\n\nnamespace eprosima {\nnamespace fastrtps {\nnamespace rtps {\n\nMessageReceiver::MessageReceiver(\n        RTPSParticipantImpl* participant,\n        uint32_t rec_buffer_size)\n    : participant_(participant)\n    , source_version_(c_ProtocolVersion)\n    , source_vendor_id_(c_VendorId_Unknown)\n    , source_guid_prefix_(c_GuidPrefix_Unknown)\n    , dest_guid_prefix_(c_GuidPrefix_Unknown)\n    , have_timestamp_(false)\n    , timestamp_(c_TimeInvalid)\n#if HAVE_SECURITY\n    , crypto_msg_(participant->is_secure() ? rec_buffer_size : 0)\n    , crypto_submsg_(participant->is_secure() ? rec_buffer_size : 0)\n#endif // if HAVE_SECURITY\n{\n    (void)rec_buffer_size;\n    logInfo(RTPS_MSG_IN, \"Created with CDRMessage of size: \" << rec_buffer_size);\n}\n\nMessageReceiver::~MessageReceiver()\n{\n    logInfo(RTPS_MSG_IN, \"\");\n    assert(associated_writers_.empty());\n    assert(associated_readers_.empty());\n}\n\nvoid MessageReceiver::associateEndpoint(\n        Endpoint* to_add)\n{\n    std::lock_guard<std::mutex> guard(mtx_);\n    if (to_add->getAttributes().endpointKind == WRITER)\n    {\n        const auto writer = dynamic_cast<RTPSWriter*>(to_add);\n        for (const auto& it : associated_writers_)\n        {\n            if (it == writer)\n            {\n                return;\n            }\n        }\n\n        associated_writers_.push_back(writer);\n    }\n    else\n    {\n        const auto reader = dynamic_cast<RTPSReader*>(to_add);\n        const auto entityId = reader->getGuid().entityId;\n        // search for set of readers by entity ID\n        const auto readers = associated_readers_.find(entityId);\n        if (readers == associated_readers_.end())\n        {\n            auto vec = std::vector<RTPSReader*>();\n            vec.push_back(reader);\n            associated_readers_.emplace(entityId, vec);\n        }\n        else\n        {\n            for (const auto& it : readers->second)\n            {\n                if (it == reader)\n                {\n                    return;\n                }\n            }\n\n            readers->second.push_back(reader);\n        }\n    }\n}\n\nvoid MessageReceiver::removeEndpoint(\n        Endpoint* to_remove)\n{\n    std::lock_guard<std::mutex> guard(mtx_);\n\n    if (to_remove->getAttributes().endpointKind == WRITER)\n    {\n        auto* var = dynamic_cast<RTPSWriter*>(to_remove);\n        for (auto it = associated_writers_.begin(); it != associated_writers_.end(); ++it)\n        {\n            if (*it == var)\n            {\n                associated_writers_.erase(it);\n                break;\n            }\n        }\n    }\n    else\n    {\n        auto readers = associated_readers_.find(to_remove->getGuid().entityId);\n        if (readers != associated_readers_.end())\n        {\n            auto* var = dynamic_cast<RTPSReader*>(to_remove);\n            for (auto it = readers->second.begin(); it != readers->second.end(); ++it)\n            {\n                if (*it == var)\n                {\n                    readers->second.erase(it);\n                    if (readers->second.empty())\n                    {\n                        associated_readers_.erase(readers);\n                    }\n                    break;\n                }\n            }\n        }\n    }\n}\n\nvoid MessageReceiver::reset()\n{\n    source_version_ = c_ProtocolVersion;\n    source_vendor_id_ = c_VendorId_Unknown;\n    source_guid_prefix_ = c_GuidPrefix_Unknown;\n    dest_guid_prefix_ = c_GuidPrefix_Unknown;\n    have_timestamp_ = false;\n    timestamp_ = c_TimeInvalid;\n}\n\nvoid MessageReceiver::processCDRMsg(\n        const Locator_t& loc,\n        CDRMessage_t* msg)\n{\n    (void)loc;\n\n    if (msg->length < RTPSMESSAGE_HEADER_SIZE)\n    {\n        logWarning(RTPS_MSG_IN, IDSTRING \"Received message too short, ignoring\");\n        return;\n    }\n\n    reset();\n\n    GuidPrefix_t participantGuidPrefix = participant_->getGuid().guidPrefix;\n    dest_guid_prefix_ = participantGuidPrefix;\n\n    msg->pos = 0; //Start reading at 0\n\n    //Once everything is set, the reading begins:\n    if (!checkRTPSHeader(msg))\n    {\n        return;\n    }\n\n#if HAVE_SECURITY\n    security::SecurityManager& security = participant_->security_manager();\n    CDRMessage_t* auxiliary_buffer = &crypto_msg_;\n\n    int decode_ret = security.decode_rtps_message(*msg, *auxiliary_buffer, source_guid_prefix_);\n\n    if (decode_ret < 0)\n    {\n        return;\n    }\n\n    if (decode_ret == 0)\n    {\n        // The original CDRMessage buffer (msg) now points to the proprietary temporary buffer crypto_msg_.\n        // The auxiliary buffer now points to the propietary temporary buffer crypto_submsg_.\n        // This way each decoded submessage will be process using the crypto_submsg_ buffer.\n        msg = auxiliary_buffer;\n        auxiliary_buffer = &crypto_submsg_;\n    }\n#endif // if HAVE_SECURITY\n\n    // Loop until there are no more submessages\n    bool valid;\n    int count = 0;\n    SubmessageHeader_t submsgh; //Current submessage header\n\n    while (msg->pos < msg->length)// end of the message\n    {\n        CDRMessage_t* submessage = msg;\n\n#if HAVE_SECURITY\n        decode_ret = security.decode_rtps_submessage(*msg, *auxiliary_buffer, source_guid_prefix_);\n\n        if (decode_ret < 0)\n        {\n            return;\n        }\n\n        if (decode_ret == 0)\n        {\n            submessage = auxiliary_buffer;\n        }\n#endif // if HAVE_SECURITY\n\n        //First 4 bytes must contain: ID | flags | octets to next header\n        if (!readSubmessageHeader(submessage, &submsgh))\n        {\n            return;\n        }\n\n        valid = true;\n        count++;\n        uint32_t next_msg_pos = submessage->pos;\n        next_msg_pos += (submsgh.submessageLength + 3u) & ~3u;\n        switch (submsgh.submessageId)\n        {\n            case DATA:\n            {\n                if (dest_guid_prefix_ != participantGuidPrefix)\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"Data Submsg ignored, DST is another RTPSParticipant\");\n                }\n                else\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"Data Submsg received, processing.\");\n                    valid = proc_Submsg_Data(submessage, &submsgh);\n                }\n                break;\n            }\n            case DATA_FRAG:\n                if (dest_guid_prefix_ != participantGuidPrefix)\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"DataFrag Submsg ignored, DST is another RTPSParticipant\");\n                }\n                else\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"DataFrag Submsg received, processing.\");\n                    valid = proc_Submsg_DataFrag(submessage, &submsgh);\n                }\n                break;\n            case GAP:\n            {\n                if (dest_guid_prefix_ != participantGuidPrefix)\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"Gap Submsg ignored, DST is another RTPSParticipant...\");\n                }\n                else\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"Gap Submsg received, processing...\");\n                    valid = proc_Submsg_Gap(submessage, &submsgh);\n                }\n                break;\n            }\n            case ACKNACK:\n            {\n                if (dest_guid_prefix_ != participantGuidPrefix)\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"Acknack Submsg ignored, DST is another RTPSParticipant...\");\n                }\n                else\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"Acknack Submsg received, processing...\");\n                    valid = proc_Submsg_Acknack(submessage, &submsgh);\n                }\n                break;\n            }\n            case NACK_FRAG:\n            {\n                if (dest_guid_prefix_ != participantGuidPrefix)\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"NackFrag Submsg ignored, DST is another RTPSParticipant...\");\n                }\n                else\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"NackFrag Submsg received, processing...\");\n                    valid = proc_Submsg_NackFrag(submessage, &submsgh);\n                }\n                break;\n            }\n            case HEARTBEAT:\n            {\n                if (dest_guid_prefix_ != participantGuidPrefix)\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"HB Submsg ignored, DST is another RTPSParticipant...\");\n                }\n                else\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"Heartbeat Submsg received, processing...\");\n                    valid = proc_Submsg_Heartbeat(submessage, &submsgh);\n                }\n                break;\n            }\n            case HEARTBEAT_FRAG:\n            {\n                if (dest_guid_prefix_ != participantGuidPrefix)\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"HBFrag Submsg ignored, DST is another RTPSParticipant...\");\n                }\n                else\n                {\n                    logInfo(RTPS_MSG_IN, IDSTRING \"HeartbeatFrag Submsg received, processing...\");\n                    valid = proc_Submsg_HeartbeatFrag(submessage, &submsgh);\n                }\n                break;\n            }\n            case PAD:\n                logWarning(RTPS_MSG_IN, IDSTRING \"PAD messages not yet implemented, ignoring\");\n                break;\n            case INFO_DST:\n                logInfo(RTPS_MSG_IN, IDSTRING \"InfoDST message received, processing...\");\n                valid = proc_Submsg_InfoDST(submessage, &submsgh);\n                break;\n            case INFO_SRC:\n                logInfo(RTPS_MSG_IN, IDSTRING \"InfoSRC message received, processing...\");\n                valid = proc_Submsg_InfoSRC(submessage, &submsgh);\n                break;\n            case INFO_TS:\n            {\n                logInfo(RTPS_MSG_IN, IDSTRING \"InfoTS Submsg received, processing...\");\n                valid = proc_Submsg_InfoTS(submessage, &submsgh);\n                break;\n            }\n            case INFO_REPLY:\n                break;\n            case INFO_REPLY_IP4:\n                break;\n            default:\n                break;\n        }\n\n        if (!valid || submsgh.is_last)\n        {\n            break;\n        }\n\n        submessage->pos = next_msg_pos;\n    }\n\n    participant_->assert_remote_participant_liveliness(source_guid_prefix_);\n}\n\nbool MessageReceiver::checkRTPSHeader(\n        CDRMessage_t* msg)\n{\n    //check and proccess the RTPS Header\n    if (msg->buffer[0] != 'R' ||  msg->buffer[1] != 'T' ||\n            msg->buffer[2] != 'P' ||  msg->buffer[3] != 'S')\n    {\n        logInfo(RTPS_MSG_IN, IDSTRING \"Msg received with no RTPS in header, ignoring...\");\n        return false;\n    }\n\n    msg->pos += 4;\n\n    //CHECK AND SET protocol version\n    if (msg->buffer[msg->pos] <= c_ProtocolVersion.m_major)\n    {\n        source_version_.m_major = msg->buffer[msg->pos];\n        msg->pos++;\n        source_version_.m_minor = msg->buffer[msg->pos];\n        msg->pos++;\n    }\n    else\n    {\n        logWarning(RTPS_MSG_IN, IDSTRING \"Major RTPS Version not supported\");\n        return false;\n    }\n\n    //Set source vendor id\n    source_vendor_id_[0] = msg->buffer[msg->pos];\n    msg->pos++;\n    source_vendor_id_[1] = msg->buffer[msg->pos];\n    msg->pos++;\n    //set source guid prefix\n    CDRMessage::readData(msg, source_guid_prefix_.value, GuidPrefix_t::size);\n    have_timestamp_ = false;\n    return true;\n}\n\nbool MessageReceiver::readSubmessageHeader(\n        CDRMessage_t* msg,\n        SubmessageHeader_t* smh)\n{\n    if (msg->length - msg->pos < 4)\n    {\n        logWarning(RTPS_MSG_IN, IDSTRING \"SubmessageHeader too short\");\n        return false;\n    }\n\n    smh->submessageId = msg->buffer[msg->pos];\n    msg->pos++;\n    smh->flags = msg->buffer[msg->pos];\n    msg->pos++;\n\n    //Set endianness of message\n    msg->msg_endian = (smh->flags & BIT(0)) != 0 ? LITTLEEND : BIGEND;\n    uint16_t length = 0;\n    CDRMessage::readUInt16(msg, &length);\n    if (msg->pos + length > msg->length)\n    {\n        logWarning(RTPS_MSG_IN, IDSTRING \"SubMsg of invalid length (\" << length <<\n                \") with current msg position/length (\" << msg->pos << \"/\" << msg->length << \")\");\n        return false;\n    }\n\n    if ((length == 0) && (smh->submessageId != INFO_TS) && (smh->submessageId != PAD))\n    {\n        // THIS IS THE LAST SUBMESSAGE\n        smh->submessageLength = msg->length - msg->pos;\n        smh->is_last = true;\n    }\n    else\n    {\n        smh->submessageLength = length;\n        smh->is_last = false;\n    }\n\n    return true;\n}\n\nbool MessageReceiver::willAReaderAcceptMsgDirectedTo(\n        const EntityId_t& readerID)\n{\n    if (associated_readers_.empty())\n    {\n        logWarning(RTPS_MSG_IN, IDSTRING \"Data received when NO readers are listening\");\n        return false;\n    }\n\n    if (readerID != c_EntityId_Unknown)\n    {\n        const auto readers = associated_readers_.find(readerID);\n        if (readers != associated_readers_.end())\n        {\n            return true;\n        }\n    }\n    else\n    {\n        for (const auto& readers : associated_readers_)\n        {\n            if (readers.second.empty())\n            {\n                continue;\n            }\n\n            for (const auto& it : readers.second)\n            {\n                if (it->m_acceptMessagesToUnknownReaders)\n                {\n                    return true;\n                }\n            }\n        }\n    }\n\n    logWarning(RTPS_MSG_IN, IDSTRING \"No Reader accepts this message (directed to: \" << readerID << \")\");\n    return false;\n}\n\ntemplate<typename Functor>\nvoid MessageReceiver::findAllReaders(\n        const EntityId_t& readerID,\n        const Functor& callback)\n{\n    if (readerID != c_EntityId_Unknown)\n    {\n        const auto readers = associated_readers_.find(readerID);\n        if (readers != associated_readers_.end())\n        {\n            for (const auto& it : readers->second)\n            {\n                callback(it);\n            }\n        }\n    }\n    else\n    {\n        for (const auto& readers : associated_readers_)\n        {\n            for (const auto& it : readers.second)\n            {\n                if (it->m_acceptMessagesToUnknownReaders)\n                {\n                    callback(it);\n                }\n            }\n        }\n    }\n}\n\nbool MessageReceiver::proc_Submsg_Data(\n        CDRMessage_t* msg,\n        SubmessageHeader_t* smh)\n{\n    std::lock_guard<std::mutex> guard(mtx_);\n\n    //READ and PROCESS\n    if (smh->submessageLength < RTPSMESSAGE_DATA_MIN_LENGTH)\n    {\n        logInfo(RTPS_MSG_IN, IDSTRING \"Too short submessage received, ignoring\");\n        return false;\n    }\n    //Fill flags bool values\n    bool endiannessFlag = (smh->flags & BIT(0)) != 0;\n    bool inlineQosFlag = (smh->flags & BIT(1)) != 0;\n    bool dataFlag = (smh->flags & BIT(2)) != 0;\n    bool keyFlag = (smh->flags & BIT(3)) != 0;\n    if (keyFlag && dataFlag)\n    {\n        logWarning(RTPS_MSG_IN, IDSTRING \"Message received with Data and Key Flag set, ignoring\");\n        return false;\n    }\n\n    //Assign message endianness\n    if (endiannessFlag)\n    {\n        msg->msg_endian = LITTLEEND;\n    }\n    else\n    {\n        msg->msg_endian = BIGEND;\n    }\n\n    //Extra flags don't matter now. Avoid those bytes\n    msg->pos += 2;\n\n    bool valid = true;\n    int16_t octetsToInlineQos;\n    valid &= CDRMessage::readInt16(msg, &octetsToInlineQos); //it should be 16 in this implementation\n\n    //reader and writer ID\n    EntityId_t readerID;\n    valid &= CDRMessage::readEntityId(msg, &readerID);\n\n    //WE KNOW THE READER THAT THE MESSAGE IS DIRECTED TO SO WE LOOK FOR IT:\n    if (!willAReaderAcceptMsgDirectedTo(readerID))\n    {\n        return false;\n    }\n\n    //FOUND THE READER.\n    //We ask the reader for a cachechange to store the information.\n    CacheChange_t ch;\n    ch.kind = ALIVE;\n    ch.writerGUID.guidPrefix = source_guid_prefix_;\n    valid &= CDRMessage::readEntityId(msg, &ch.writerGUID.entityId);\n\n    //Get sequence number\n    valid &= CDRMessage::readSequenceNumber(msg, &ch.sequenceNumber);\n\n    if (!valid)\n    {\n        return false;\n    }\n\n    if (ch.sequenceNumber <= SequenceNumber_t())\n    {\n        logWarning(RTPS_MSG_IN, IDSTRING \"Invalid message received, bad sequence Number\");\n        return false;\n    }\n\n    //Jump ahead if more parameters are before inlineQos (not in this version, maybe if further minor versions.)\n    if (octetsToInlineQos > RTPSMESSAGE_OCTETSTOINLINEQOS_DATASUBMSG)\n    {\n        msg->pos += (octetsToInlineQos - RTPSMESSAGE_OCTETSTOINLINEQOS_DATASUBMSG);\n        if (msg->pos > msg->length)\n        {\n            logWarning(RTPS_MSG_IN,\n                    IDSTRING \"Invalid jump through msg, msg->pos \" << msg->pos << \" > msg->length \" << msg->length);\n            return false;\n        }\n    }\n\n    uint32_t inlineQosSize = 0;\n\n    if (inlineQosFlag)\n    {\n        if (!ParameterList::updateCacheChangeFromInlineQos(ch, msg, inlineQosSize))\n        {\n            logInfo(RTPS_MSG_IN, IDSTRING \"SubMessage Data ERROR, Inline Qos ParameterList error\");\n            return false;\n        }\n    }\n\n    if (dataFlag || keyFlag)\n    {\n        uint32_t payload_size;\n        payload_size = smh->submessageLength -\n                (RTPSMESSAGE_DATA_EXTRA_INLINEQOS_SIZE + octetsToInlineQos + inlineQosSize);\n\n        if (dataFlag)\n        {\n            uint32_t next_pos = msg->pos + payload_size;\n            if (msg->length >= next_pos && payload_size > 0)\n            {\n                ch.serializedPayload.data = &msg->buffer[msg->pos];\n                ch.serializedPayload.length = payload_size;\n                ch.serializedPayload.max_size = payload_size;\n                msg->pos = next_pos;\n            }\n            else\n            {\n                logWarning(RTPS_MSG_IN, IDSTRING \"Serialized Payload value invalid or larger than maximum allowed size\"\n                        \"(\" << payload_size << \"/\" << (msg->length - msg->pos) << \")\");\n                return false;\n            }\n        }\n        else if (keyFlag)\n        {\n            if (payload_size <= 0)\n            {\n                logWarning(RTPS_MSG_IN, IDSTRING \"Serialized Payload value invalid (\" << payload_size << \")\");\n                return false;\n            }\n\n            if (payload_size <= PARAMETER_KEY_HASH_LENGTH)\n            {\n                memcpy(ch.instanceHandle.value, &msg->buffer[msg->pos], payload_size);\n            }\n            else\n            {\n                logWarning(RTPS_MSG_IN, IDSTRING \"Ignoring Serialized Payload for too large key-only data (\" <<\n                        payload_size << \")\");\n            }\n            msg->pos += payload_size;\n        }\n    }\n\n    // Set sourcetimestamp\n    if (have_timestamp_)\n    {\n        ch.sourceTimestamp = timestamp_;\n    }\n\n    logInfo(RTPS_MSG_IN, IDSTRING \"from Writer \" << ch.writerGUID << \"; possible RTPSReader entities: \" <<\n            associated_readers_.size());\n\n    //Look for the correct reader to add the change\n    findAllReaders(readerID,\n            [&ch](RTPSReader* reader)\n            {\n                reader->processDataMsg(&ch);\n            });\n\n    //TODO(Ricardo) If a exception is thrown (ex, by fastcdr), this line is not executed -> segmentation fault\n    ch.serializedPayload.data = nullptr;\n\n    logInfo(RTPS_MSG_IN, IDSTRING \"Sub Message DATA processed\");\n    return true;\n}\n\nbool MessageReceiver::proc_Submsg_DataFrag(\n        CDRMessage_t* msg,\n        SubmessageHeader_t* smh)\n{\n    std::lock_guard<std::mutex> guard(mtx_);\n\n    //READ and PROCESS\n    if (smh->submessageLength < RTPSMESSAGE_DATA_MIN_LENGTH)\n    {\n        logInfo(RTPS_MSG_IN, IDSTRING \"Too short submessage received, ignoring\");\n        return false;\n    }\n\n    //Fill flags bool values\n    bool endiannessFlag = (smh->flags & BIT(0)) != 0;\n    bool inlineQosFlag = (smh->flags & BIT(1)) != 0;\n    bool keyFlag = (smh->flags & BIT(2)) != 0;\n\n    //Assign message endianness\n    if (endiannessFlag)\n    {\n        msg->msg_endian = LITTLEEND;\n    }\n    else\n    {\n        msg->msg_endian = BIGEND;\n    }\n\n    //Extra flags don't matter now. Avoid those bytes\n    msg->pos += 2;\n\n    bool valid = true;\n    int16_t octetsToInlineQos;\n    valid &= CDRMessage::readInt16(msg, &octetsToInlineQos); //it should be 16 in this implementation\n\n    //reader and writer ID\n    EntityId_t readerID;\n    valid &= CDRMessage::readEntityId(msg, &readerID);\n\n    //WE KNOW THE READER THAT THE MESSAGE IS DIRECTED TO SO WE LOOK FOR IT:\n    if (!willAReaderAcceptMsgDirectedTo(readerID))\n    {\n        return false;\n    }\n\n    //FOUND THE READER.\n    //We ask the reader for a cachechange to store the information.\n    CacheChange_t ch;\n    ch.writerGUID.guidPrefix = source_guid_prefix_;\n    valid &= CDRMessage::readEntityId(msg, &ch.writerGUID.entityId);\n\n    //Get sequence number\n    valid &= CDRMessage::readSequenceNumber(msg, &ch.sequenceNumber);\n\n    if (ch.sequenceNumber <= SequenceNumber_t())\n    {\n        logWarning(RTPS_MSG_IN, IDSTRING \"Invalid message received, bad sequence Number\");\n        return false;\n    }\n\n    // READ FRAGMENT NUMBER\n    uint32_t fragmentStartingNum;\n    valid &= CDRMessage::readUInt32(msg, &fragmentStartingNum);\n\n    // READ FRAGMENTSINSUBMESSAGE\n    uint16_t fragmentsInSubmessage;\n    valid &= CDRMessage::readUInt16(msg, &fragmentsInSubmessage);\n\n    // READ FRAGMENTSIZE\n    uint16_t fragmentSize;\n    valid &= CDRMessage::readUInt16(msg, &fragmentSize);\n\n    // READ SAMPLESIZE\n    uint32_t sampleSize;\n    valid &= CDRMessage::readUInt32(msg, &sampleSize);\n\n    if (!valid)\n    {\n        return false;\n    }\n\n    //Jump ahead if more parameters are before inlineQos (not in this version, maybe if further minor versions.)\n    if (octetsToInlineQos > RTPSMESSAGE_OCTETSTOINLINEQOS_DATAFRAGSUBMSG)\n    {\n        msg->pos += (octetsToInlineQos - RTPSMESSAGE_OCTETSTOINLINEQOS_DATAFRAGSUBMSG);\n        if (msg->pos > msg->length)\n        {\n            logWarning(RTPS_MSG_IN,\n                    IDSTRING \"Invalid jump through msg, msg->pos \" << msg->pos << \" > msg->length \" << msg->length);\n            return false;\n        }\n    }\n\n    uint32_t inlineQosSize = 0;\n\n    if (inlineQosFlag)\n    {\n        if (!ParameterList::updateCacheChangeFromInlineQos(ch, msg, inlineQosSize))\n        {\n            logInfo(RTPS_MSG_IN, IDSTRING \"SubMessage Data ERROR, Inline Qos ParameterList error\");\n            return false;\n        }\n    }\n\n    uint32_t payload_size;\n    payload_size = smh->submessageLength - (RTPSMESSAGE_DATA_EXTRA_INLINEQOS_SIZE + octetsToInlineQos + inlineQosSize);\n\n    // Validations??? XXX TODO\n\n    if (!keyFlag)\n    {\n        uint32_t next_pos = msg->pos + payload_size;\n        if (msg->length >= next_pos && payload_size > 0)\n        {\n            ch.kind = ALIVE;\n            ch.serializedPayload.data = &msg->buffer[msg->pos];\n            ch.serializedPayload.length = payload_size;\n            ch.serializedPayload.max_size = payload_size;\n            ch.setFragmentSize(fragmentSize);\n\n            msg->pos = next_pos;\n        }\n        else\n        {\n            logWarning(RTPS_MSG_IN, IDSTRING \"Serialized Payload value invalid or larger than maximum allowed size \"\n                    \"(\" << payload_size << \"/\" << (msg->length - msg->pos) << \")\");\n            return false;\n        }\n    }\n    else if (keyFlag)\n    {\n        /* XXX TODO\n           Endianness_t previous_endian = msg->msg_endian;\n           if (ch->serializedPayload.encapsulation == PL_CDR_BE)\n           msg->msg_endian = BIGEND;\n           else if (ch->serializedPayload.encapsulation == PL_CDR_LE)\n           msg->msg_endian = LITTLEEND;\n           else\n           {\n           logError(RTPS_MSG_IN, IDSTRING\"Bad encapsulation for KeyHash and status parameter list\");\n           return false;\n           }\n           //uint32_t param_size;\n           if (ParameterList::readParameterListfromCDRMsg(msg, &m_ParamList, ch, false) <= 0)\n           {\n           logInfo(RTPS_MSG_IN, IDSTRING\"SubMessage Data ERROR, keyFlag ParameterList\");\n           return false;\n           }\n           msg->msg_endian = previous_endian;\n         */\n    }\n\n    // Set sourcetimestamp\n    if (have_timestamp_)\n    {\n        ch.sourceTimestamp = timestamp_;\n    }\n\n    //FIXME: DO SOMETHING WITH PARAMETERLIST CREATED.\n    logInfo(RTPS_MSG_IN, IDSTRING \"from Writer \" << ch.writerGUID << \"; possible RTPSReader entities: \" <<\n            associated_readers_.size());\n\n    //Look for the correct reader to add the change\n    findAllReaders(readerID,\n            [&ch, sampleSize, fragmentStartingNum, fragmentsInSubmessage](RTPSReader* reader)\n            {\n                reader->processDataFragMsg(&ch, sampleSize, fragmentStartingNum, fragmentsInSubmessage);\n            });\n\n    ch.serializedPayload.data = nullptr;\n\n    logInfo(RTPS_MSG_IN, IDSTRING \"Sub Message DATA_FRAG processed\");\n\n    return true;\n}\n\nbool MessageReceiver::proc_Submsg_Heartbeat(\n        CDRMessage_t* msg,\n        SubmessageHeader_t* smh)\n{\n    bool endiannessFlag = (smh->flags & BIT(0)) != 0;\n    bool finalFlag = (smh->flags & BIT(1)) != 0;\n    bool livelinessFlag = (smh->flags & BIT(2)) != 0;\n    //Assign message endianness\n    if (endiannessFlag)\n    {\n        msg->msg_endian = LITTLEEND;\n    }\n    else\n    {\n        msg->msg_endian = BIGEND;\n    }\n\n    GUID_t readerGUID;\n    GUID_t writerGUID;\n    readerGUID.guidPrefix = dest_guid_prefix_;\n    CDRMessage::readEntityId(msg, &readerGUID.entityId);\n    writerGUID.guidPrefix = source_guid_prefix_;\n    CDRMessage::readEntityId(msg, &writerGUID.entityId);\n    SequenceNumber_t firstSN;\n    SequenceNumber_t lastSN;\n    CDRMessage::readSequenceNumber(msg, &firstSN);\n    CDRMessage::readSequenceNumber(msg, &lastSN);\n    if (lastSN < firstSN && lastSN != firstSN - 1)\n    {\n        logWarning(RTPS_MSG_IN, IDSTRING \"Invalid Heartbeat received (\" << firstSN << \") - (\" <<\n                lastSN << \"), ignoring\");\n        return false;\n    }\n    uint32_t HBCount;\n    CDRMessage::readUInt32(msg, &HBCount);\n\n    std::lock_guard<std::mutex> guard(mtx_);\n    //Look for the correct reader and writers:\n    findAllReaders(readerGUID.entityId,\n            [&writerGUID, &HBCount, &firstSN, &lastSN, finalFlag, livelinessFlag](RTPSReader* reader)\n            {\n                reader->processHeartbeatMsg(writerGUID, HBCount, firstSN, lastSN, finalFlag, livelinessFlag);\n            });\n\n    return true;\n}\n\nbool MessageReceiver::proc_Submsg_Acknack(\n        CDRMessage_t* msg,\n        SubmessageHeader_t* smh)\n{\n    bool endiannessFlag = (smh->flags & BIT(0)) != 0;\n    bool finalFlag = (smh->flags & BIT(1)) != 0;\n    //Assign message endianness\n    if (endiannessFlag)\n    {\n        msg->msg_endian = LITTLEEND;\n    }\n    else\n    {\n        msg->msg_endian = BIGEND;\n    }\n    GUID_t readerGUID;\n    GUID_t writerGUID;\n    readerGUID.guidPrefix = source_guid_prefix_;\n    CDRMessage::readEntityId(msg, &readerGUID.entityId);\n    writerGUID.guidPrefix = dest_guid_prefix_;\n    CDRMessage::readEntityId(msg, &writerGUID.entityId);\n\n\n    SequenceNumberSet_t SNSet = CDRMessage::readSequenceNumberSet(msg);\n    uint32_t Ackcount;\n    CDRMessage::readUInt32(msg, &Ackcount);\n\n    std::lock_guard<std::mutex> guard(mtx_);\n    //Look for the correct writer to use the acknack\n    for (RTPSWriter* it : associated_writers_)\n    {\n        bool result;\n        if (it->process_acknack(writerGUID, readerGUID, Ackcount, SNSet, finalFlag, result))\n        {\n            if (!result)\n            {\n                logInfo(RTPS_MSG_IN, IDSTRING \"Acknack msg to NOT stateful writer \");\n            }\n            return result;\n        }\n    }\n    logInfo(RTPS_MSG_IN, IDSTRING \"Acknack msg to UNKNOWN writer (I loooked through \"\n            << associated_writers_.size() << \" writers in this ListenResource)\");\n    return false;\n}\n\nbool MessageReceiver::proc_Submsg_Gap(\n        CDRMessage_t* msg,\n        SubmessageHeader_t* smh)\n{\n    bool endiannessFlag = (smh->flags & BIT(0)) != 0;\n    //Assign message endianness\n    if (endiannessFlag)\n    {\n        msg->msg_endian = LITTLEEND;\n    }\n    else\n    {\n        msg->msg_endian = BIGEND;\n    }\n\n    GUID_t writerGUID;\n    GUID_t readerGUID;\n    readerGUID.guidPrefix = dest_guid_prefix_;\n    CDRMessage::readEntityId(msg, &readerGUID.entityId);\n    writerGUID.guidPrefix = source_guid_prefix_;\n    CDRMessage::readEntityId(msg, &writerGUID.entityId);\n    SequenceNumber_t gapStart;\n    CDRMessage::readSequenceNumber(msg, &gapStart);\n    SequenceNumberSet_t gapList = CDRMessage::readSequenceNumberSet(msg);\n    if (gapStart <= SequenceNumber_t(0, 0))\n    {\n        return false;\n    }\n\n    std::lock_guard<std::mutex> guard(mtx_);\n    findAllReaders(readerGUID.entityId,\n            [&writerGUID, &gapStart, &gapList](RTPSReader* reader)\n            {\n                reader->processGapMsg(writerGUID, gapStart, gapList);\n            });\n\n    return true;\n}\n\nbool MessageReceiver::proc_Submsg_InfoTS(\n        CDRMessage_t* msg,\n        SubmessageHeader_t* smh)\n{\n    bool endiannessFlag = (smh->flags & BIT(0)) != 0;\n    bool timeFlag = (smh->flags & BIT(1)) != 0;\n    //Assign message endianness\n    if (endiannessFlag)\n    {\n        msg->msg_endian = LITTLEEND;\n    }\n    else\n    {\n        msg->msg_endian = BIGEND;\n    }\n    if (!timeFlag)\n    {\n        have_timestamp_ = true;\n        CDRMessage::readTimestamp(msg, &timestamp_);\n    }\n    else\n    {\n        have_timestamp_ = false;\n    }\n\n    return true;\n}\n\nbool MessageReceiver::proc_Submsg_InfoDST(\n        CDRMessage_t* msg,\n        SubmessageHeader_t* smh)\n{\n    bool endiannessFlag = (smh->flags & BIT(0)) != 0u;\n    //bool timeFlag = smh->flags & BIT(1) ? true : false;\n    //Assign message endianness\n    if (endiannessFlag)\n    {\n        msg->msg_endian = LITTLEEND;\n    }\n    else\n    {\n        msg->msg_endian = BIGEND;\n    }\n    GuidPrefix_t guidP;\n    CDRMessage::readData(msg, guidP.value, GuidPrefix_t::size);\n    if (guidP != c_GuidPrefix_Unknown)\n    {\n        dest_guid_prefix_ = guidP;\n        logInfo(RTPS_MSG_IN, IDSTRING \"DST RTPSParticipant is now: \" << dest_guid_prefix_);\n    }\n    return true;\n}\n\nbool MessageReceiver::proc_Submsg_InfoSRC(\n        CDRMessage_t* msg,\n        SubmessageHeader_t* smh)\n{\n    bool endiannessFlag = (smh->flags & BIT(0)) != 0;\n    //bool timeFlag = smh->flags & BIT(1) ? true : false;\n    //Assign message endianness\n    if (endiannessFlag)\n    {\n        msg->msg_endian = LITTLEEND;\n    }\n    else\n    {\n        msg->msg_endian = BIGEND;\n    }\n    if (smh->submessageLength == INFO_SRC_SUBMSG_LENGTH)\n    {\n        //AVOID FIRST 4 BYTES:\n        msg->pos += 4;\n        CDRMessage::readOctet(msg, &source_version_.m_major);\n        CDRMessage::readOctet(msg, &source_version_.m_minor);\n        CDRMessage::readData(msg, &source_vendor_id_[0], 2);\n        CDRMessage::readData(msg, source_guid_prefix_.value, GuidPrefix_t::size);\n        logInfo(RTPS_MSG_IN, IDSTRING \"SRC RTPSParticipant is now: \" << source_guid_prefix_);\n        return true;\n    }\n    return false;\n}\n\nbool MessageReceiver::proc_Submsg_NackFrag(\n        CDRMessage_t* msg,\n        SubmessageHeader_t* smh)\n{\n    bool endiannessFlag = (smh->flags & BIT(0)) != 0;\n    //Assign message endianness\n    if (endiannessFlag)\n    {\n        msg->msg_endian = LITTLEEND;\n    }\n    else\n    {\n        msg->msg_endian = BIGEND;\n    }\n\n    GUID_t readerGUID;\n    GUID_t writerGUID;\n    readerGUID.guidPrefix = source_guid_prefix_;\n    CDRMessage::readEntityId(msg, &readerGUID.entityId);\n    writerGUID.guidPrefix = dest_guid_prefix_;\n    CDRMessage::readEntityId(msg, &writerGUID.entityId);\n\n    SequenceNumber_t writerSN;\n    CDRMessage::readSequenceNumber(msg, &writerSN);\n\n    FragmentNumberSet_t fnState;\n    CDRMessage::readFragmentNumberSet(msg, &fnState);\n\n    uint32_t Ackcount;\n    CDRMessage::readUInt32(msg, &Ackcount);\n\n    std::lock_guard<std::mutex> guard(mtx_);\n    //Look for the correct writer to use the acknack\n    for (RTPSWriter* it : associated_writers_)\n    {\n        bool result;\n        if (it->process_nack_frag(writerGUID, readerGUID, Ackcount, writerSN, fnState, result))\n        {\n            if (!result)\n            {\n                logInfo(RTPS_MSG_IN, IDSTRING \"Acknack msg to NOT stateful writer \");\n            }\n            return result;\n        }\n    }\n    logInfo(RTPS_MSG_IN, IDSTRING \"Acknack msg to UNKNOWN writer (I looked through \"\n            << associated_writers_.size() << \" writers in this ListenResource)\");\n    return false;\n}\n\nbool MessageReceiver::proc_Submsg_HeartbeatFrag(\n        CDRMessage_t* msg,\n        SubmessageHeader_t* smh)\n{\n    bool endiannessFlag = (smh->flags & BIT(0)) != 0;\n    //Assign message endianness\n    if (endiannessFlag)\n    {\n        msg->msg_endian = LITTLEEND;\n    }\n    else\n    {\n        msg->msg_endian = BIGEND;\n    }\n\n    GUID_t readerGUID;\n    GUID_t writerGUID;\n    readerGUID.guidPrefix = dest_guid_prefix_;\n    CDRMessage::readEntityId(msg, &readerGUID.entityId);\n    writerGUID.guidPrefix = source_guid_prefix_;\n    CDRMessage::readEntityId(msg, &writerGUID.entityId);\n\n    SequenceNumber_t writerSN;\n    CDRMessage::readSequenceNumber(msg, &writerSN);\n\n    FragmentNumber_t lastFN;\n    CDRMessage::readUInt32(msg, static_cast<uint32_t*>(&lastFN));\n\n    uint32_t HBCount;\n    CDRMessage::readUInt32(msg, &HBCount);\n\n    // XXX TODO VALIDATE DATA?\n\n    //Look for the correct reader and writers:\n    /* XXX TODO\n       std::lock_guard<std::mutex> guard(mtx_);\n       for (std::vector<RTPSReader*>::iterator it = associated_readers_.begin();\n            it != associated_readers_.end(); ++it)\n       {\n           if ((*it)->acceptMsgDirectedTo(readerGUID.entityId))\n           {\n           (*it)->processHeartbeatMsg(writerGUID, HBCount, firstSN, lastSN, finalFlag, livelinessFlag);\n           }\n       }\n     */\n    return true;\n}\n\n} // namespace rtps\n} /* namespace rtps */\n} /* namespace eprosima */\n", "hunk": "@@ -200,7 +200,7 @@ void MessageReceiver::processCDRMsg(\n     {\n         // The original CDRMessage buffer (msg) now points to the proprietary temporary buffer crypto_msg_.\n         // The auxiliary buffer now points to the propietary temporary buffer crypto_submsg_.\n-        // This way each decoded submessage will be process using the crypto_submsg_ buffer.\n+        // This way each decoded sub-message will be processed using the crypto_submsg_ buffer.\n         msg = auxiliary_buffer;\n         auxiliary_buffer = &crypto_submsg_;\n     }\n", "comment": "```suggestion // This way each decoded sub-message will be processed using the crypto_submsg_ buffer. ```", "ids": ["20987", "cda01c2c75b149429a140937e8a21a64347b1b73", "f5e59486524bab4a35e359f16aaad6680629845c"], "repo": "eProsima/Fast-DDS", "ghid": 1644, "old": "     {\n         // The original CDRMessage buffer (msg) now points to the proprietary temporary buffer crypto_msg_.\n         // The auxiliary buffer now points to the propietary temporary buffer crypto_submsg_.\n-        // This way each decoded submessage will be process using the crypto_submsg_ buffer.\n         msg = auxiliary_buffer;\n         auxiliary_buffer = &crypto_submsg_;\n     }", "new": "     {\n         // The original CDRMessage buffer (msg) now points to the proprietary temporary buffer crypto_msg_.\n         // The auxiliary buffer now points to the propietary temporary buffer crypto_submsg_.\n+        // This way each decoded sub-message will be processed using the crypto_submsg_ buffer.\n         msg = auxiliary_buffer;\n         auxiliary_buffer = &crypto_submsg_;\n     }", "lang": "cpp", "norm_lang": "cpp"}
{"old_hunk": "@@ -491,6 +491,11 @@ def get_thread_group(concurrency=None, rampup=0, hold=0, iterations=None,\n         if hold or (rampup and not iterations):\n             scheduler = True\n \n+        if isinstance(rampup, numeric_types) and isinstance(hold, numeric_types):", "oldf": "\"\"\"\nModule holds base stuff regarding JMX format\n\nCopyright 2015 BlazeMeter Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\"\"\"\nimport logging\nimport os\nimport traceback\n\nfrom cssselect import GenericTranslator\n\nfrom bzt import TaurusInternalException, TaurusConfigError\nfrom bzt.engine import Scenario, BetterDict\nfrom bzt.six import etree, iteritems, string_types, parse, text_type, numeric_types\n\n\ndef cond_int(val):\n    if isinstance(val, float):\n        return int(val)\n\n    return val\n\n\nclass JMX(object):\n    \"\"\"\n    A class to manipulate and generate JMX test plans for JMeter\n\n    :param original: path to existing JMX to load. If it is None, then creates\n    empty test plan\n    \"\"\"\n    TEST_PLAN_SEL = \"jmeterTestPlan>hashTree>hashTree\"\n    THR_GROUP_SEL = TEST_PLAN_SEL + \">hashTree[type=tg]\"\n    THR_TIMER = \"kg.apc.jmeter.timers.VariableThroughputTimer\"\n\n    def __init__(self, original=None, test_plan_name=\"BZT Generated Test Plan\"):\n        self.log = logging.getLogger(self.__class__.__name__)\n        if original:\n            self.load(original)\n        else:\n            root = etree.Element(\"jmeterTestPlan\")\n            self.tree = etree.ElementTree(root)\n\n            test_plan = etree.Element(\"TestPlan\", guiclass=\"TestPlanGui\",\n                                      testname=test_plan_name,\n                                      testclass=\"TestPlan\")\n\n            htree = etree.Element(\"hashTree\")\n            htree.append(test_plan)\n            htree.append(etree.Element(\"hashTree\"))\n            self.append(\"jmeterTestPlan\", htree)\n\n            element_prop = self._get_arguments_panel(\"TestPlan.user_defined_variables\")\n            self.append(\"jmeterTestPlan>hashTree>TestPlan\", element_prop)\n\n    def load(self, original):\n        \"\"\"\n        Load existing JMX file\n\n        :param original: JMX file path\n        :raise TaurusInternalException: in case of XML parsing error\n        \"\"\"\n        try:\n            self.tree = etree.ElementTree()\n            self.tree.parse(original)\n        except BaseException as exc:\n            msg = \"XML parsing failed for file %s: %s\"\n            raise TaurusInternalException(msg % (original, exc))\n\n    def get(self, selector):\n        \"\"\"\n        Returns tree elements by CSS selector\n\n        :type selector: str\n        :return:\n        \"\"\"\n        expression = GenericTranslator().css_to_xpath(selector)\n        nodes = self.tree.xpath(expression)\n        return nodes\n\n    def append(self, selector, node):\n        \"\"\"\n        Add node to container specified by selector. If multiple nodes will\n        match the selector, first of them will be used as container.\n\n        :param selector: CSS selector for container\n        :param node: Element instance to add\n        :raise TaurusInternalException: if container was not found\n        \"\"\"\n        container = self.get(selector)\n        if not len(container):\n            msg = \"Failed to find TestPlan node in file: %s\"\n            raise TaurusInternalException(msg % selector)\n\n        container[0].append(node)\n\n    def save(self, filename):\n        \"\"\"\n        Save JMX into file\n\n        :param filename:\n        \"\"\"\n        self.log.debug(\"Saving JMX to: %s\", filename)\n        with open(filename, \"wb\") as fhd:\n            self.tree.write(fhd, pretty_print=True, encoding=\"UTF-8\", xml_declaration=True)\n\n    @staticmethod\n    def _flag(flag_name, bool_value):\n        \"\"\"\n        Generates element for JMX flag node\n\n        :param flag_name:\n        :param bool_value:\n        :return:\n        \"\"\"\n        elm = etree.Element(flag_name)\n        elm.text = \"true\" if bool_value else \"false\"\n        return elm\n\n    @staticmethod\n    def __jtl_writer(filename, label, flags):\n        \"\"\"\n        Generates JTL writer\n\n        :param filename:\n        :return:\n        \"\"\"\n        jtl = etree.Element(\"stringProp\", {\"name\": \"filename\"})\n        jtl.text = filename\n\n        name = etree.Element(\"name\")\n        name.text = \"saveConfig\"\n        value = etree.Element(\"value\")\n        value.set(\"class\", \"SampleSaveConfiguration\")\n\n        for key, val in iteritems(flags):\n            value.append(JMX._flag(key, val))\n        obj_prop = etree.Element(\"objProp\")\n        obj_prop.append(name)\n        obj_prop.append(value)\n\n        listener = etree.Element(\"ResultCollector\",\n                                 testname=label,\n                                 testclass=\"ResultCollector\",\n                                 guiclass=\"SimpleDataWriter\")\n        listener.append(jtl)\n        listener.append(obj_prop)\n        return listener\n\n    @staticmethod\n    def new_kpi_listener(filename):\n        \"\"\"\n        Generates listener for writing basic KPI data in CSV format\n\n        :param filename:\n        :return:\n        \"\"\"\n        flags = {\n            \"xml\": False,\n            \"fieldNames\": True,\n            \"time\": True,\n            \"timestamp\": True,\n            \"latency\": True,\n            \"success\": True,\n            \"label\": True,\n            \"code\": True,\n            \"message\": True,\n            \"threadName\": True,\n            \"dataType\": False,\n            \"encoding\": False,\n            \"assertions\": False,\n            \"subresults\": False,\n            \"responseData\": False,\n            \"samplerData\": False,\n            \"responseHeaders\": False,\n            \"requestHeaders\": False,\n            \"responseDataOnError\": False,\n            \"saveAssertionResultsFailureMessage\": False,\n            \"bytes\": True,\n            \"hostname\": True,\n            \"threadCounts\": True,\n            \"url\": False\n        }\n\n        return JMX.__jtl_writer(filename, \"KPI Writer\", flags)\n\n    @staticmethod\n    def new_xml_listener(filename, is_full, user_flags):\n        \"\"\"\n\n        :param is_full: bool\n        :param filename: str\n        :param user_flags: BetterDict\n        :return:\n        \"\"\"\n        default_flags = {\n            \"xml\": True,\n            \"fieldNames\": True,\n            \"time\": True,\n            \"timestamp\": True,\n            \"latency\": True,\n            \"success\": True,\n            \"label\": True,\n            \"code\": True,\n            \"message\": True,\n            \"threadName\": True,\n            \"dataType\": True,\n            \"encoding\": True,\n            \"assertions\": True,\n            \"subresults\": True,\n            \"responseData\": False,\n            \"samplerData\": False,\n            \"responseHeaders\": True,\n            \"requestHeaders\": True,\n            \"responseDataOnError\": True,\n            \"saveAssertionResultsFailureMessage\": True,\n            \"bytes\": True,\n            \"threadCounts\": True,\n            \"url\": True\n        }\n        flags = BetterDict()\n        flags.merge(default_flags)\n        flags.merge(user_flags)\n\n        if is_full:\n            writer = JMX.__jtl_writer(filename, \"Trace Writer\", flags)\n        else:\n            writer = JMX.__jtl_writer(filename, \"Errors Writer\", flags)\n            writer.append(JMX._bool_prop(\"ResultCollector.error_logging\", True))\n\n        return writer\n\n    @staticmethod\n    def _get_arguments_panel(name):\n        \"\"\"\n        Generates ArgumentsPanel node\n\n        :param name:\n        :return:\n        \"\"\"\n        return etree.Element(\"elementProp\", name=name, elementType=\"Arguments\",\n                             guiclass=\"ArgumentsPanel\", testclass=\"Arguments\")\n\n    @staticmethod\n    def _get_http_request(url, label, method, timeout, body, keepalive, files=(), encoding=None, follow_redirects=True,\n                          use_random_host_ip=False, host_ips=()):\n        \"\"\"\n        Generates HTTP request\n        :type method: str\n        :type label: str\n        :type url: str\n        :rtype: lxml.etree.Element\n        \"\"\"\n        proxy = etree.Element(\"HTTPSamplerProxy\", guiclass=\"HttpTestSampleGui\", testclass=\"HTTPSamplerProxy\")\n        proxy.set(\"testname\", label)\n\n        args = JMX._get_arguments_panel(\"HTTPsampler.Arguments\")\n\n        if isinstance(body, string_types):\n            JMX.__add_body_from_string(args, body, proxy)\n        elif isinstance(body, dict):\n            JMX.__add_body_from_script(args, body, proxy)\n        elif body:\n            msg = \"Cannot handle 'body' option of type %s: %s\"\n            raise TaurusInternalException(msg % (type(body), body))\n\n        parsed_url = parse.urlparse(url)\n        JMX.__add_hostnameport_2sampler(parsed_url, proxy, url)\n\n        path = parsed_url.path\n        if parsed_url.query:\n            path += \"?\" + parsed_url.query\n\n        proxy.append(JMX._string_prop(\"HTTPSampler.path\", path))\n        proxy.append(JMX._string_prop(\"HTTPSampler.method\", method))\n        proxy.append(JMX._bool_prop(\"HTTPSampler.use_keepalive\", keepalive))\n        proxy.append(JMX._bool_prop(\"HTTPSampler.follow_redirects\", follow_redirects))\n        proxy.append(JMX._bool_prop(\"HTTPSampler.auto_redirects\", False))\n\n        if timeout is not None:\n            proxy.append(JMX._string_prop(\"HTTPSampler.connect_timeout\", timeout))\n            proxy.append(JMX._string_prop(\"HTTPSampler.response_timeout\", timeout))\n\n        if encoding is not None:\n            proxy.append(JMX._string_prop(\"HTTPSampler.contentEncoding\", encoding))\n\n        if files:\n            proxy.append(JMX._bool_prop(\"HTTPSampler.DO_MULTIPART_POST\", True))\n            proxy.append(JMX._bool_prop(\"HTTPSampler.BROWSER_COMPATIBLE_MULTIPART\", True))\n\n            files_prop = JMX._element_prop(\"HTTPsampler.Files\", \"HTTPFileArgs\")\n            files_coll = JMX._collection_prop(\"HTTPFileArgs.files\")\n            for file_dict in files:\n                file_elem = JMX._element_prop(file_dict['path'], \"HTTPFileArg\")\n                file_elem.append(JMX._string_prop(\"File.path\", file_dict['path']))\n                file_elem.append(JMX._string_prop(\"File.paramname\", file_dict[\"param\"]))\n                file_elem.append(JMX._string_prop(\"File.mimetype\", file_dict['mime-type']))\n                files_coll.append(file_elem)\n            files_prop.append(files_coll)\n            proxy.append(files_prop)\n\n        if use_random_host_ip and host_ips:\n            if len(host_ips) > 1:\n                expr = \"${__chooseRandom(%s,randomAddr)}\" % \",\".join(host_ips)\n            else:\n                expr = host_ips[0]\n            proxy.append(JMX._string_prop(\"HTTPSampler.ipSource\", expr))\n\n        return proxy\n\n    @staticmethod\n    def __add_body_from_string(args, body, proxy):\n        proxy.append(JMX._bool_prop(\"HTTPSampler.postBodyRaw\", True))\n        coll_prop = JMX._collection_prop(\"Arguments.arguments\")\n        header = JMX._element_prop(\"elementProp\", \"HTTPArgument\")\n        try:\n            header.append(JMX._string_prop(\"Argument.value\", body))\n        except ValueError:\n            logging.warning(\"Failed to set body: %s\", traceback.format_exc())\n            header.append(JMX._string_prop(\"Argument.value\", \"BINARY-STUB\"))\n        coll_prop.append(header)\n        args.append(coll_prop)\n        proxy.append(args)\n\n    @staticmethod\n    def __add_body_from_script(args, body, proxy):\n        http_args_coll_prop = JMX._collection_prop(\"Arguments.arguments\")\n        for arg_name, arg_value in body.items():\n            if not (isinstance(arg_value, string_types) or isinstance(arg_value, numeric_types)):\n                msg = 'Body field \"%s: %s\" requires \"Content-Type: application/json\" header'\n                raise TaurusInternalException(msg % (arg_name, arg_value))\n            try:\n                http_element_prop = JMX._element_prop(arg_name, \"HTTPArgument\")\n            except ValueError:\n                logging.warning(\"Failed to get element property: %s\", traceback.format_exc())\n                http_element_prop = JMX._element_prop('BINARY-STUB', \"HTTPArgument\")\n\n            try:\n                http_element_prop.append(JMX._string_prop(\"Argument.name\", arg_name))\n            except ValueError:\n                logging.warning(\"Failed to set arg name: %s\", traceback.format_exc())\n                http_element_prop.append(JMX._string_prop(\"Argument.name\", \"BINARY-STUB\"))\n\n            try:\n                http_element_prop.append(\n                    JMX._string_prop(\"Argument.value\", arg_value if arg_value is not None else ''))\n            except ValueError:\n                logging.warning(\"Failed to set arg name: %s\", traceback.format_exc())\n                http_element_prop.append(JMX._string_prop(\"Argument.value\", \"BINARY-STUB\"))\n\n            http_element_prop.append(JMX._bool_prop(\"HTTPArgument.always_encode\", True))\n            use_equals = arg_value is not None\n            http_element_prop.append(JMX._bool_prop(\"HTTPArgument.use_equals\", arg_value is not None))\n            http_element_prop.append(JMX._string_prop(\"Argument.metadata\", '=' if use_equals else ''))\n            http_args_coll_prop.append(http_element_prop)\n        args.append(http_args_coll_prop)\n        proxy.append(args)\n\n    @staticmethod\n    def __add_hostnameport_2sampler(parsed_url, proxy, url):\n        if parsed_url.scheme:\n            proxy.append(JMX._string_prop(\"HTTPSampler.protocol\", parsed_url.scheme))\n        if parsed_url.netloc:\n            netloc_parts = parsed_url.netloc.split(':')\n            if netloc_parts[0]:\n                proxy.append(JMX._string_prop(\"HTTPSampler.domain\", netloc_parts[0]))\n\n            if len(netloc_parts) > 1 and netloc_parts[1]:\n                proxy.append(JMX._string_prop(\"HTTPSampler.port\", netloc_parts[1]))\n            else:\n                try:\n                    if parsed_url.port:\n                        proxy.append(JMX._string_prop(\"HTTPSampler.port\", parsed_url.port))\n                    else:\n                        proxy.append(JMX._string_prop(\"HTTPSampler.port\", \"\"))\n                except ValueError:\n                    logging.debug(\"Non-parsable port: %s\", url)\n                    proxy.append(JMX._string_prop(\"HTTPSampler.port\", \"\"))\n\n    @staticmethod\n    def _element_prop(name, element_type):\n        \"\"\"\n        Generates element property node\n\n        :param name:\n        :param element_type:\n        :return:\n        \"\"\"\n        res = etree.Element(\"elementProp\", name=name, elementType=element_type)\n        return res\n\n    @staticmethod\n    def _collection_prop(name):\n        \"\"\"\n        Adds Collection prop\n        :param name:\n        :return:\n        \"\"\"\n        res = etree.Element(\"collectionProp\", name=name)\n        return res\n\n    @staticmethod\n    def _string_prop(name, value):\n        \"\"\"\n        Generates string property node\n\n        :param name:\n        :param value:\n        :return:\n        \"\"\"\n        res = etree.Element(\"stringProp\", name=name)\n        res.text = text_type(value)\n        return res\n\n    @staticmethod\n    def _long_prop(name, value):\n        \"\"\"\n        Generates long property node\n\n        :param name:\n        :param value:\n        :return:\n        \"\"\"\n        res = etree.Element(\"longProp\", name=name)\n        res.text = text_type(value)\n        return res\n\n    @staticmethod\n    def _bool_prop(name, value):\n        \"\"\"\n        Generates boolean property\n\n        :param name:\n        :param value:\n        :return:\n        \"\"\"\n        res = etree.Element(\"boolProp\", name=name)\n        res.text = 'true' if value else 'false'\n        return res\n\n    @staticmethod\n    def int_prop(name, value):\n        \"\"\"\n        JMX int property\n        :param name:\n        :param value:\n        :return:\n        \"\"\"\n        res = etree.Element(\"intProp\", name=name)\n        res.text = text_type(value)\n        return res\n\n    @staticmethod\n    def get_thread_group(concurrency=None, rampup=0, hold=0, iterations=None,\n                         testname=\"ThreadGroup\", on_error=\"continue\"):\n        \"\"\"\n        Generates ThreadGroup\n\n        :param concurrency:\n        :param rampup:\n        :param hold:\n        :param iterations:\n        :param testname:\n        :param on_error:\n        :return:\n        \"\"\"\n        if not rampup:\n            rampup = 0\n\n        rampup = cond_int(rampup)\n        hold = cond_int(hold)\n\n        if not concurrency:\n            concurrency = 1\n\n        if not iterations:\n            iterations = -1\n\n        scheduler = False\n        if hold or (rampup and not iterations):\n            scheduler = True\n\n        if isinstance(rampup, numeric_types) and isinstance(hold, numeric_types):\n            duration = hold + rampup\n        else:\n            duration = 0\n\n        trg = etree.Element(\"ThreadGroup\", guiclass=\"ThreadGroupGui\",\n                            testclass=\"ThreadGroup\", testname=testname)\n        if on_error is not None:\n            trg.append(JMX._string_prop(\"ThreadGroup.on_sample_error\", on_error))\n        loop = etree.Element(\"elementProp\",\n                             name=\"ThreadGroup.main_controller\",\n                             elementType=\"LoopController\",\n                             guiclass=\"LoopControlPanel\",\n                             testclass=\"LoopController\")\n        loop.append(JMX._bool_prop(\"LoopController.continue_forever\", False))  # always false except of root LC\n        loop.append(JMX._string_prop(\"LoopController.loops\", iterations))\n        trg.append(loop)\n\n        trg.append(JMX._string_prop(\"ThreadGroup.num_threads\", concurrency))\n        trg.append(JMX._string_prop(\"ThreadGroup.ramp_time\", rampup))\n        trg.append(JMX._string_prop(\"ThreadGroup.start_time\", \"\"))\n        trg.append(JMX._string_prop(\"ThreadGroup.end_time\", \"\"))\n        trg.append(JMX._bool_prop(\"ThreadGroup.scheduler\", scheduler))\n        trg.append(JMX._string_prop(\"ThreadGroup.duration\", duration))\n\n        return trg\n\n    def get_rps_shaper(self):\n        \"\"\"\n\n        :return: etree.Element\n        \"\"\"\n\n        throughput_timer_element = etree.Element(self.THR_TIMER,\n                                                 guiclass=self.THR_TIMER + \"Gui\",\n                                                 testclass=self.THR_TIMER,\n                                                 testname=\"jp@gc - Throughput Shaping Timer\",\n                                                 enabled=\"true\")\n        shaper_load_prof = self._collection_prop(\"load_profile\")\n        throughput_timer_element.append(shaper_load_prof)\n\n        return throughput_timer_element\n\n    def add_rps_shaper_schedule(self, shaper_etree, start_rps, end_rps, duration):\n        \"\"\"\n        Adds schedule to rps shaper\n        :param shaper_etree:\n        :param start_rps:\n        :param end_rps:\n        :param duration:\n        :return:\n        \"\"\"\n        shaper_collection = shaper_etree.find(\".//collectionProp[@name='load_profile']\")\n        coll_prop = self._collection_prop(\"1817389797\")\n        start_rps_prop = self._string_prop(\"49\", cond_int(start_rps))\n        end_rps_prop = self._string_prop(\"1567\", cond_int(end_rps))\n        duration_prop = self._string_prop(\"53\", cond_int(duration))\n        coll_prop.append(start_rps_prop)\n        coll_prop.append(end_rps_prop)\n        coll_prop.append(duration_prop)\n        shaper_collection.append(coll_prop)\n\n    @staticmethod\n    def add_user_def_vars_elements(udv_dict, testname=\"Variables from Taurus\"):\n        \"\"\"\n        :type testname: str\n        :type udv_dict: dict[str,str]\n        :rtype: etree.Element\n        \"\"\"\n\n        udv_element = etree.Element(\"Arguments\", guiclass=\"ArgumentsPanel\", testclass=\"Arguments\",\n                                    testname=testname)\n        udv_collection_prop = JMX._collection_prop(\"Arguments.arguments\")\n\n        for var_name in sorted(udv_dict.keys(), key=str):\n            udv_element_prop = JMX._element_prop(str(var_name), \"Argument\")\n            udv_arg_name_prop = JMX._string_prop(\"Argument.name\", var_name)\n            udv_arg_value_prop = JMX._string_prop(\"Argument.value\", udv_dict[var_name])\n            udv_arg_desc_prop = JMX._string_prop(\"Argument.desc\", \"\")\n            udv_arg_meta_prop = JMX._string_prop(\"Argument.metadata\", \"=\")\n            udv_element_prop.append(udv_arg_name_prop)\n            udv_element_prop.append(udv_arg_value_prop)\n            udv_element_prop.append(udv_arg_desc_prop)\n            udv_element_prop.append(udv_arg_meta_prop)\n            udv_collection_prop.append(udv_element_prop)\n\n        udv_element.append(udv_collection_prop)\n        return udv_element\n\n    @staticmethod\n    def get_concurrency_thread_group(\n            concurrency=None, rampup=0, hold=0, steps=None, on_error=\"continue\", testname=\"ConcurrencyThreadGroup\"):\n        \"\"\"\n        :return: etree element, Concurrency Thread Group\n        \"\"\"\n        if not rampup:\n            rampup = 0\n\n        if not concurrency:\n            concurrency = 1\n\n        if steps is None:  # zero means infinity of steps\n            steps = 0\n\n        name = 'com.blazemeter.jmeter.threads.concurrency.ConcurrencyThreadGroup'\n        concurrency_thread_group = etree.Element(\n            name, guiclass=name + \"Gui\", testclass=name, testname=testname, enabled=\"true\")\n        virtual_user_controller = etree.Element(\n            \"elementProp\",\n            name=\"ThreadGroup.main_controller\",\n            elementType=\"com.blazemeter.jmeter.control.VirtualUserController\")\n        concurrency_thread_group.append(virtual_user_controller)\n        concurrency_thread_group.append(JMX._string_prop(\"ThreadGroup.on_sample_error\", on_error))\n        concurrency_thread_group.append(JMX._string_prop(\"TargetLevel\", str(concurrency)))\n        concurrency_thread_group.append(JMX._string_prop(\"RampUp\", str(cond_int(rampup))))\n        concurrency_thread_group.append(JMX._string_prop(\"Steps\", steps))\n        concurrency_thread_group.append(JMX._string_prop(\"Hold\", str(cond_int(hold))))\n        concurrency_thread_group.append(JMX._string_prop(\"LogFilename\", \"\"))\n        concurrency_thread_group.append(JMX._string_prop(\"Iterations\", \"\"))\n        concurrency_thread_group.append(JMX._string_prop(\"Unit\", \"S\"))\n\n        return concurrency_thread_group\n\n    @staticmethod\n    def get_dns_cache_mgr():\n        \"\"\"\n        Adds dns cache element with defaults parameters\n\n        :return:\n        \"\"\"\n        dns_element = etree.Element(\"DNSCacheManager\", guiclass=\"DNSCachePanel\", testclass=\"DNSCacheManager\",\n                                    testname=\"DNS Cache Manager\")\n        dns_element.append(JMX._collection_prop(\"DNSCacheManager.servers\"))\n        dns_element.append(JMX._bool_prop(\"DNSCacheManager.clearEachIteration\", False))\n        dns_element.append(JMX._bool_prop(\"DNSCacheManager.isCustomResolver\", False))\n        return dns_element\n\n    @staticmethod\n    def _get_header_mgr(hdict):\n        \"\"\"\n\n        :type hdict: dict[str,str]\n        :rtype: lxml.etree.Element\n        \"\"\"\n        mgr = etree.Element(\"HeaderManager\", guiclass=\"HeaderPanel\", testclass=\"HeaderManager\", testname=\"Headers\")\n\n        coll_prop = etree.Element(\"collectionProp\", name=\"HeaderManager.headers\")\n        for hname, hval in iteritems(hdict):\n            header = etree.Element(\"elementProp\", name=\"\", elementType=\"Header\")\n            header.append(JMX._string_prop(\"Header.name\", hname))\n            header.append(JMX._string_prop(\"Header.value\", hval))\n            coll_prop.append(header)\n        mgr.append(coll_prop)\n        return mgr\n\n    @staticmethod\n    def _get_cache_mgr():\n        \"\"\"\n        :rtype: lxml.etree.Element\n        \"\"\"\n        mgr = etree.Element(\"CacheManager\", guiclass=\"CacheManagerGui\", testclass=\"CacheManager\", testname=\"Cache\")\n        mgr.append(JMX._bool_prop(\"clearEachIteration\", True))\n        mgr.append(JMX._bool_prop(\"useExpires\", True))\n        return mgr\n\n    @staticmethod\n    def _get_cookie_mgr(scenario=None):\n        \"\"\"\n        :rtype: lxml.etree.Element\n        \"\"\"\n        mgr = etree.Element(\"CookieManager\", guiclass=\"CookiePanel\", testclass=\"CookieManager\", testname=\"Cookies\")\n        mgr.append(JMX._bool_prop(\"CookieManager.clearEachIteration\", True))\n        mgr.append(JMX._string_prop(\"CookieManager.implementation\",\n                                    \"org.apache.jmeter.protocol.http.control.HC4CookieHandler\"))\n\n        if scenario:\n            cookies = scenario.get(Scenario.COOKIES, [])\n            if cookies:\n                cookies_coll = JMX._collection_prop(\"CookieManager.cookies\")\n                mgr.append(cookies_coll)\n                for cookie in cookies:\n                    if not isinstance(cookie, dict):\n                        raise TaurusConfigError(\"Cookie must be dictionary: %s\" % cookie)\n                    c_name = cookie.get(\"name\", TaurusConfigError(\"Name of cookie isn't found: %s\" % cookie))\n                    c_value = cookie.get(\"value\", TaurusConfigError(\"Value of cookie isn't found: %s\" % cookie))\n                    c_domain = cookie.get(\"domain\", TaurusConfigError(\"Domain of cookie isn't found: %s\" % cookie))\n                    c_path = cookie.get(\"path\", \"\")\n                    c_secure = cookie.get(\"secure\", False)\n\n                    # follow params are hardcoded in JMeter\n                    c_expires = 0\n                    c_path_specified = True\n                    c_domain_specified = True\n\n                    c_elem = etree.Element(\"elementProp\", name=c_name, elementType=\"Cookie\", testname=c_name)\n                    c_elem.append(JMX._string_prop(\"Cookie.value\", c_value))\n                    c_elem.append(JMX._string_prop(\"Cookie.domain\", c_domain))\n                    c_elem.append(JMX._string_prop(\"Cookie.path\", c_path))\n                    c_elem.append(JMX._bool_prop(\"Cookie.secure\", c_secure))\n                    c_elem.append(JMX._long_prop(\"Cookie.expires\", c_expires))\n                    c_elem.append(JMX._bool_prop(\"Cookie.path_specified\", c_path_specified))\n                    c_elem.append(JMX._bool_prop(\"Cookie.domain_specified\", c_domain_specified))\n\n                    cookies_coll.append(c_elem)\n\n        return mgr\n\n    @staticmethod\n    def _get_http_defaults(default_address=None, timeout=None, retrieve_resources=None, concurrent_pool_size=4,\n                           content_encoding=None, resources_regex=None):\n        \"\"\"\n        :rtype: lxml.etree.Element\n        \"\"\"\n        cfg = etree.Element(\"ConfigTestElement\", guiclass=\"HttpDefaultsGui\",\n                            testclass=\"ConfigTestElement\", testname=\"Defaults\")\n\n        if retrieve_resources:\n            cfg.append(JMX._bool_prop(\"HTTPSampler.image_parser\", True))\n            cfg.append(JMX._bool_prop(\"HTTPSampler.concurrentDwn\", True))\n            if concurrent_pool_size:\n                cfg.append(JMX._string_prop(\"HTTPSampler.concurrentPool\", concurrent_pool_size))\n\n        params = etree.Element(\"elementProp\",\n                               name=\"HTTPsampler.Arguments\",\n                               elementType=\"Arguments\",\n                               guiclass=\"HTTPArgumentsPanel\",\n                               testclass=\"Arguments\", testname=\"user_defined\")\n        cfg.append(params)\n        if default_address:\n            parsed_url = parse.urlsplit(default_address)\n            if parsed_url.scheme:\n                cfg.append(JMX._string_prop(\"HTTPSampler.protocol\", parsed_url.scheme))\n\n            if parsed_url.netloc:\n                netloc = parsed_url.netloc\n                if ':' in netloc:\n                    index = netloc.rfind(':')\n                    cfg.append(JMX._string_prop(\"HTTPSampler.port\", netloc[index + 1:]))\n                    netloc = netloc[:index]\n\n                cfg.append(JMX._string_prop(\"HTTPSampler.domain\", netloc))\n\n        if timeout:\n            cfg.append(JMX._string_prop(\"HTTPSampler.connect_timeout\", timeout))\n            cfg.append(JMX._string_prop(\"HTTPSampler.response_timeout\", timeout))\n\n        if content_encoding:\n            cfg.append(JMX._string_prop(\"HTTPSampler.contentEncoding\", content_encoding))\n\n        if resources_regex:\n            cfg.append(JMX._string_prop(\"HTTPSampler.embedded_url_re\", resources_regex))\n\n        return cfg\n\n    @staticmethod\n    def _get_dur_assertion(timeout):\n        \"\"\"\n\n        :type timeout: int\n        :return:\n        \"\"\"\n        element = etree.Element(\"DurationAssertion\", guiclass=\"DurationAssertionGui\",\n                                testclass=\"DurationAssertion\", testname=\"Timeout Check\")\n        element.append(JMX._string_prop(\"DurationAssertion.duration\", timeout))\n        return element\n\n    @staticmethod\n    def _get_constant_timer(delay):\n        \"\"\"\n\n        :type delay: int\n        :rtype: lxml.etree.Element\n        \"\"\"\n        element = etree.Element(\"ConstantTimer\", guiclass=\"ConstantTimerGui\",\n                                testclass=\"ConstantTimer\", testname=\"Think-Time\")\n        element.append(JMX._string_prop(\"ConstantTimer.delay\", delay))\n        return element\n\n    @staticmethod\n    def _get_extractor(varname, headers, regexp, template, match_no, default='NOT_FOUND'):\n        \"\"\"\n\n        :type varname: str\n        :type regexp: str\n        :type template: str|int\n        :type match_no: int\n        :type default: str\n        :rtype: lxml.etree.Element\n        \"\"\"\n        if isinstance(template, int):\n            template = '$%s$' % template\n\n        if headers.lower() == 'headers':\n            headers = 'true'\n        elif headers.lower() == 'http-code':\n            headers = 'code'\n        elif headers.lower() == 'url':\n            headers = 'URL'\n        else:\n            headers = 'body'\n\n        element = etree.Element(\"RegexExtractor\", guiclass=\"RegexExtractorGui\",\n                                testclass=\"RegexExtractor\", testname=\"Get %s\" % varname, enabled=\"true\")\n        element.append(JMX._string_prop(\"RegexExtractor.useHeaders\", headers))\n        element.append(JMX._string_prop(\"RegexExtractor.refname\", varname))\n        element.append(JMX._string_prop(\"RegexExtractor.regex\", regexp))\n        element.append(JMX._string_prop(\"Sample.scope\", \"parent\"))\n        element.append(JMX._string_prop(\"RegexExtractor.template\", template))\n        element.append(JMX._string_prop(\"RegexExtractor.default\", default))\n        element.append(JMX._string_prop(\"RegexExtractor.match_number\", match_no))\n        return element\n\n    @staticmethod\n    def _get_jquerycss_extractor(varname, selector, attribute, match_no, default=\"NOT_FOUND\"):\n        \"\"\"\n\n        :type varname: str\n        :type regexp: str\n        :type match_no: int\n        :type default: str\n        :rtype: lxml.etree.Element\n        \"\"\"\n\n        element = etree.Element(\"HtmlExtractor\", guiclass=\"HtmlExtractorGui\", testclass=\"HtmlExtractor\",\n                                testname=\"Get %s\" % varname)\n        element.append(JMX._string_prop(\"HtmlExtractor.refname\", varname))\n        element.append(JMX._string_prop(\"HtmlExtractor.expr\", selector))\n        element.append(JMX._string_prop(\"HtmlExtractor.attribute\", attribute))\n        element.append(JMX._string_prop(\"HtmlExtractor.match_number\", match_no))\n        element.append(JMX._string_prop(\"HtmlExtractor.default\", default))\n        return element\n\n    @staticmethod\n    def _get_json_extractor(varname, jsonpath, default='NOT_FOUND', from_variable=None):\n        \"\"\"\n\n        :type varname: str\n        :type default: str\n        :rtype: lxml.etree.Element\n        \"\"\"\n        package = \"com.atlantbh.jmeter.plugins.jsonutils.jsonpathextractor\"\n        element = etree.Element(\"%s.JSONPathExtractor\" % package,\n                                guiclass=\"%s.gui.JSONPathExtractorGui\" % package,\n                                testclass=\"%s.JSONPathExtractor\" % package,\n                                testname=\"Get %s\" % varname)\n        element.append(JMX._string_prop(\"VAR\", varname))\n        element.append(JMX._string_prop(\"JSONPATH\", jsonpath))\n        element.append(JMX._string_prop(\"DEFAULT\", default))\n        if from_variable:\n            element.append(JMX._string_prop(\"VARIABLE\", from_variable))\n            element.append(JMX._string_prop(\"SUBJECT\", \"VAR\"))\n        return element\n\n    @staticmethod\n    def _get_json_path_assertion(jsonpath, expected_value, json_validation, expect_null, invert, regexp=True):\n        \"\"\"\n        :type jsonpath: str\n        :type expected_value: str\n        :type json_validation: bool\n        :type expect_null: bool\n        :type invert: bool\n        :type regexp: bool\n        :return: lxml.etree.Element\n        \"\"\"\n        package = \"com.atlantbh.jmeter.plugins.jsonutils.jsonpathassertion\"\n        element = etree.Element(\"%s.JSONPathAssertion\" % package,\n                                guiclass=\"%s.gui.JSONPathAssertionGui\" % package,\n                                testclass=\"%s.JSONPathAssertion\" % package,\n                                testname=\"JSon path assertion\")\n        element.append(JMX._string_prop(\"JSON_PATH\", jsonpath))\n        element.append(JMX._string_prop(\"EXPECTED_VALUE\", expected_value))\n        element.append(JMX._bool_prop(\"JSONVALIDATION\", json_validation))\n        element.append(JMX._bool_prop(\"EXPECT_NULL\", expect_null))\n        element.append(JMX._bool_prop(\"INVERT\", invert))\n        element.append(JMX._bool_prop(\"ISREGEX\", regexp))\n\n        return element\n\n    @staticmethod\n    def _get_xpath_extractor(varname, xpath, default, validate_xml, ignore_whitespace, use_tolerant_parser):\n        \"\"\"\n        :type varname: str\n        :type xpath: str\n        :type default: str\n        :type validate_xml: bool\n        :type ignore_whitespace: bool\n        :type use_tolerant_parser: bool\n        :rtype: lxml.etree.Element\n        \"\"\"\n        element = etree.Element(\"XPathExtractor\",\n                                guiclass=\"XPathExtractorGui\",\n                                testclass=\"XPathExtractor\",\n                                testname=\"Get %s\" % varname)\n        element.append(JMX._string_prop(\"XPathExtractor.refname\", varname))\n        element.append(JMX._string_prop(\"XPathExtractor.xpathQuery\", xpath))\n        element.append(JMX._string_prop(\"XPathExtractor.default\", default))\n        element.append(JMX._bool_prop(\"XPathExtractor.validate\", validate_xml))\n        element.append(JMX._bool_prop(\"XPathExtractor.whitespace\", ignore_whitespace))\n        element.append(JMX._bool_prop(\"XPathExtractor.tolerant\", use_tolerant_parser))\n        return element\n\n    @staticmethod\n    def _get_xpath_assertion(xpath, validate_xml, ignore_whitespace, use_tolerant_parser, invert):\n        \"\"\"\n        :type xpath: str\n        :type validate_xml: bool\n        :type ignore_whitespace: bool\n        :type use_tolerant_parser: bool\n        :return: lxml.etree.Element\n        \"\"\"\n        element = etree.Element(\"XPathAssertion\",\n                                guiclass=\"XPathAssertionGui\",\n                                testclass=\"XPathAssertion\",\n                                testname=\"XPath Assertion\")\n\n        element.append(JMX._string_prop(\"XPath.xpath\", xpath))\n        element.append(JMX._bool_prop(\"XPath.validate\", validate_xml))\n        element.append(JMX._bool_prop(\"XPath.whitespace\", ignore_whitespace))\n        element.append(JMX._bool_prop(\"XPath.tolerant\", use_tolerant_parser))\n        element.append(JMX._bool_prop(\"XPath.negate\", invert))\n\n        return element\n\n    @staticmethod\n    def _get_resp_assertion(field, contains, is_regexp, is_invert, assume_success=False):\n        \"\"\"\n\n        :type field: str\n        :type contains: list[str]\n        :type is_regexp: bool\n        :type is_invert:  bool\n        :rtype: lxml.etree.Element\n        \"\"\"\n        tname = \"Assert %s %s\" % (\"hasn't\" if is_invert else \"has\",\n                                  \"[\" + \", \".join('\"' + text_type(x) + '\"' for x in contains) + \"]\")\n        element = etree.Element(\"ResponseAssertion\", guiclass=\"AssertionGui\",\n                                testclass=\"ResponseAssertion\", testname=tname)\n        if field == Scenario.FIELD_HEADERS:\n            fld = \"Assertion.response_headers\"\n        elif field == Scenario.FIELD_RESP_CODE:\n            fld = \"Assertion.response_code\"\n        else:\n            fld = \"Assertion.response_data\"\n\n        if is_regexp:\n            if is_invert:\n                mtype = 6  # not contains\n            else:\n                mtype = 2  # contains\n        else:\n            if is_invert:\n                mtype = 20  # not substring\n            else:\n                mtype = 16  # substring\n\n        element.append(JMX._string_prop(\"Assertion.test_field\", fld))\n        element.append(JMX._string_prop(\"Assertion.test_type\", mtype))\n        element.append(JMX._bool_prop(\"Assertion.assume_success\", assume_success))\n\n        coll_prop = etree.Element(\"collectionProp\", name=\"Asserion.test_strings\")\n        for string in contains:\n            coll_prop.append(JMX._string_prop(\"\", string))\n        element.append(coll_prop)\n\n        return element\n\n    @staticmethod\n    def _get_jsr223_element(language, script_file, parameters, execute, script_text=None):\n        if execute == \"before\":\n            element = etree.Element(\"JSR223PreProcessor\", guiclass=\"TestBeanGUI\",\n                                    testclass=\"JSR223PreProcessor\", testname=\"JSR223 PreProcessor\")\n        else:\n            element = etree.Element(\"JSR223PostProcessor\", guiclass=\"TestBeanGUI\",\n                                    testclass=\"JSR223PostProcessor\", testname=\"JSR223 PostProcessor\")\n        element.append(JMX._string_prop(\"filename\", script_file if script_file else ''))\n        element.append(JMX._string_prop(\"script\", script_text if script_text else ''))\n        element.append(JMX._string_prop(\"parameters\", parameters))\n        element.append(JMX._string_prop(\"scriptLanguage\", language))\n        return element\n\n    @staticmethod\n    def _get_csv_config(path, delimiter, is_quoted, loop, variable_names):\n        \"\"\"\n\n        :type path: str\n        :type delimiter: str\n        :type is_quoted: bool\n        :return:\n        \"\"\"\n        element = etree.Element(\"CSVDataSet\", guiclass=\"TestBeanGUI\",\n                                testclass=\"CSVDataSet\", testname=\"CSV %s\" % os.path.basename(path))\n        element.append(JMX._string_prop(\"filename\", path))\n        element.append(JMX._string_prop(\"delimiter\", delimiter))\n        element.append(JMX._bool_prop(\"quotedData\", is_quoted))\n        element.append(JMX._bool_prop(\"recycle\", loop))\n        element.append(JMX._bool_prop(\"stopThread\", not loop))\n        element.append(JMX._string_prop(\"variableNames\", variable_names))\n\n        return element\n\n    def set_enabled(self, sel, state):\n        \"\"\"\n        Toggle items by selector\n\n        :type sel: str\n        :type state: bool\n        \"\"\"\n        items = self.get(sel)\n        self.log.debug(\"Enable %s elements %s: %s\", state, sel, items)\n        for item in items:\n            item.set(\"enabled\", 'true' if state else 'false')\n\n    def set_text(self, sel, text):\n        \"\"\"\n        Set text value\n\n        :type sel: str\n        :type text: str\n        \"\"\"\n        items = self.get(sel)\n        res = 0\n        for item in items:\n            item.text = text_type(text)\n            res += 1\n\n        return res\n\n    @staticmethod\n    def _get_simple_controller(name):\n        return etree.Element(\"GenericController\", guiclass=\"LogicControllerGui\", testclass=\"GenericController\",\n                             testname=name)\n\n    def _add_results_tree(self):\n        dbg_tree = etree.Element(\"ResultCollector\",\n                                 testname=\"View Results Tree\",\n                                 testclass=\"ResultCollector\",\n                                 guiclass=\"ViewResultsFullVisualizer\")\n        self.append(self.TEST_PLAN_SEL, dbg_tree)\n        self.append(self.TEST_PLAN_SEL, etree.Element(\"hashTree\"))\n\n    @staticmethod\n    def _get_results_tree():\n        dbg_tree = etree.Element(\"ResultCollector\",\n                                 testname=\"View Results Tree\",\n                                 testclass=\"ResultCollector\",\n                                 guiclass=\"ViewResultsFullVisualizer\")\n        return dbg_tree\n\n    @staticmethod\n    def _get_if_controller(condition):\n        controller = etree.Element(\"IfController\", guiclass=\"IfControllerPanel\", testclass=\"IfController\",\n                                   testname=\"If Controller\")\n        controller.append(JMX._string_prop(\"IfController.condition\", condition))\n        return controller\n\n    @staticmethod\n    def _get_loop_controller(loops):\n        if loops == 'forever':\n            iterations = -1\n        else:\n            iterations = loops\n        controller = etree.Element(\"LoopController\", guiclass=\"LoopControlPanel\", testclass=\"LoopController\",\n                                   testname=\"Loop Controller\")\n        controller.append(JMX._bool_prop(\"LoopController.continue_forever\", False))  # always false except of root LC\n        controller.append(JMX._string_prop(\"LoopController.loops\", str(iterations)))\n        return controller\n\n    @staticmethod\n    def _get_foreach_controller(input_var, loop_var):\n        # TODO: useSeparator option\n        controller = etree.Element(\"ForeachController\", guiclass=\"ForeachControlPanel\", testclass=\"ForeachController\",\n                                   testname=\"ForEach Controller\")\n        controller.append(JMX._string_prop(\"ForeachController.inputVal\", input_var))\n        controller.append(JMX._string_prop(\"ForeachController.returnVal\", loop_var))\n        controller.append(JMX._bool_prop(\"ForeachController.useSeparator\", True))\n        return controller\n\n    @staticmethod\n    def _get_while_controller(condition):\n        controller = etree.Element(\"WhileController\", guiclass=\"WhileControllerGui\", testclass=\"WhileController\",\n                                   testname=\"While Controller\")\n        controller.append(JMX._string_prop(\"WhileController.condition\", condition))\n        return controller\n\n    @staticmethod\n    def _get_transaction_controller(transaction_name, force_parent_sample=False):\n        controller = etree.Element(\"TransactionController\", guiclass=\"TransactionControllerGui\",\n                                   testclass=\"TransactionController\", testname=transaction_name)\n        controller.append(JMX._bool_prop(\"TransactionController.parent\", force_parent_sample))\n        return controller\n\n    @staticmethod\n    def _get_functional_mode_prop(enabled):\n        return JMX._bool_prop(\"TestPlan.functional_mode\", enabled)\n\n    @staticmethod\n    def _get_action_block(action_index, target_index, duration_ms):\n        action = etree.Element(\"TestAction\", guiclass=\"TestActionGui\", testclass=\"TestAction\", testname=\"Test Action\")\n        action.append(JMX.int_prop(\"ActionProcessor.action\", action_index))\n        action.append(JMX.int_prop(\"ActionProcessor.target\", target_index))\n        action.append(JMX._string_prop(\"ActionProcessor.duration\", str(duration_ms)))\n        return action\n", "hunk": "@@ -494,7 +494,7 @@ class JMX(object):\n         if isinstance(rampup, numeric_types) and isinstance(hold, numeric_types):\n             duration = hold + rampup\n         else:\n-            duration = 0\n+            duration = 1\n \n         trg = etree.Element(\"ThreadGroup\", guiclass=\"ThreadGroupGui\",\n                             testclass=\"ThreadGroup\", testname=testname)\n", "comment": "what if one of them is property and one is not?", "ids": ["14517", "1bff7d12fb6111fa99a647ce17bc08bb36b6e777", "f755b034a500d03fb7319bf6daf0376ac8920c27"], "repo": "Blazemeter/taurus", "ghid": 643, "old": "         if isinstance(rampup, numeric_types) and isinstance(hold, numeric_types):\n             duration = hold + rampup\n         else:\n-            duration = 0\n         trg = etree.Element(\"ThreadGroup\", guiclass=\"ThreadGroupGui\",\n                             testclass=\"ThreadGroup\", testname=testname)", "new": "         if isinstance(rampup, numeric_types) and isinstance(hold, numeric_types):\n             duration = hold + rampup\n         else:\n+            duration = 1\n         trg = etree.Element(\"ThreadGroup\", guiclass=\"ThreadGroupGui\",\n                             testclass=\"ThreadGroup\", testname=testname)", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -470,6 +474,8 @@ def draw_networkx_nodes(\n         labelleft=False,\n     )\n \n+    ax.margins(margins[0], margins[1])\n+", "oldf": "\"\"\"\n**********\nMatplotlib\n**********\n\nDraw networks with matplotlib.\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> nx.draw(G)\n\nSee Also\n--------\n - :doc:`matplotlib <matplotlib:index>`\n - :func:`matplotlib.pyplot.scatter`\n - :obj:`matplotlib.patches.FancyArrowPatch`\n\"\"\"\nfrom numbers import Number\nimport networkx as nx\nfrom networkx.drawing.layout import (\n    shell_layout,\n    circular_layout,\n    kamada_kawai_layout,\n    spectral_layout,\n    spring_layout,\n    random_layout,\n    planar_layout,\n)\nimport warnings\n\n__all__ = [\n    \"draw\",\n    \"draw_networkx\",\n    \"draw_networkx_nodes\",\n    \"draw_networkx_edges\",\n    \"draw_networkx_labels\",\n    \"draw_networkx_edge_labels\",\n    \"draw_circular\",\n    \"draw_kamada_kawai\",\n    \"draw_random\",\n    \"draw_spectral\",\n    \"draw_spring\",\n    \"draw_planar\",\n    \"draw_shell\",\n]\n\n\ndef draw(G, pos=None, ax=None, **kwds):\n    \"\"\"Draw the graph G with Matplotlib.\n\n    Draw the graph as a simple representation with no node\n    labels or edge labels and using the full Matplotlib figure area\n    and no axis labels by default.  See draw_networkx() for more\n    full-featured drawing that allows title, axis labels etc.\n\n    Parameters\n    ----------\n    G : graph\n        A networkx graph\n\n    pos : dictionary, optional\n        A dictionary with nodes as keys and positions as values.\n        If not specified a spring layout positioning will be computed.\n        See :py:mod:`networkx.drawing.layout` for functions that\n        compute node positions.\n\n    ax : Matplotlib Axes object, optional\n        Draw the graph in specified Matplotlib axes.\n\n    kwds : optional keywords\n        See networkx.draw_networkx() for a description of optional keywords.\n\n    Examples\n    --------\n    >>> G = nx.dodecahedral_graph()\n    >>> nx.draw(G)\n    >>> nx.draw(G, pos=nx.spring_layout(G))  # use spring layout\n\n    See Also\n    --------\n    draw_networkx\n    draw_networkx_nodes\n    draw_networkx_edges\n    draw_networkx_labels\n    draw_networkx_edge_labels\n\n    Notes\n    -----\n    This function has the same name as pylab.draw and pyplot.draw\n    so beware when using `from networkx import *`\n\n    since you might overwrite the pylab.draw function.\n\n    With pyplot use\n\n    >>> import matplotlib.pyplot as plt\n    >>> G = nx.dodecahedral_graph()\n    >>> nx.draw(G)  # networkx draw()\n    >>> plt.draw()  # pyplot draw()\n\n    Also see the NetworkX drawing examples at\n    https://networkx.org/documentation/latest/auto_examples/index.html\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    if ax is None:\n        cf = plt.gcf()\n    else:\n        cf = ax.get_figure()\n    cf.set_facecolor(\"w\")\n    if ax is None:\n        if cf._axstack() is None:\n            ax = cf.add_axes((0, 0, 1, 1))\n        else:\n            ax = cf.gca()\n\n    if \"with_labels\" not in kwds:\n        kwds[\"with_labels\"] = \"labels\" in kwds\n\n    draw_networkx(G, pos=pos, ax=ax, **kwds)\n    ax.set_axis_off()\n    plt.draw_if_interactive()\n    return\n\n\ndef draw_networkx(G, pos=None, arrows=True, with_labels=True, **kwds):\n    \"\"\"Draw the graph G using Matplotlib.\n\n    Draw the graph with Matplotlib with options for node positions,\n    labeling, titles, and many other drawing features.\n    See draw() for simple drawing without labels or axes.\n\n    Parameters\n    ----------\n    G : graph\n        A networkx graph\n\n    pos : dictionary, optional\n        A dictionary with nodes as keys and positions as values.\n        If not specified a spring layout positioning will be computed.\n        See :py:mod:`networkx.drawing.layout` for functions that\n        compute node positions.\n\n    arrows : bool (default=True)\n        For directed graphs, if True draw arrowheads.\n        Note: Arrows will be the same color as edges.\n\n    arrowstyle : str (default='-\\|>')\n        For directed graphs, choose the style of the arrowsheads.\n        See `matplotlib.patches.ArrowStyle` for more options.\n\n    arrowsize : int (default=10)\n        For directed graphs, choose the size of the arrow head's length and\n        width. See `matplotlib.patches.FancyArrowPatch` for attribute\n        `mutation_scale` for more info.\n\n    with_labels :  bool (default=True)\n        Set to True to draw labels on the nodes.\n\n    ax : Matplotlib Axes object, optional\n        Draw the graph in the specified Matplotlib axes.\n\n    nodelist : list (default=list(G))\n        Draw only specified nodes\n\n    edgelist : list (default=list(G.edges()))\n        Draw only specified edges\n\n    node_size : scalar or array (default=300)\n        Size of nodes.  If an array is specified it must be the\n        same length as nodelist.\n\n    node_color : color or array of colors (default='#1f78b4')\n        Node color. Can be a single color or a sequence of colors with the same\n        length as nodelist. Color can be string or rgb (or rgba) tuple of\n        floats from 0-1. If numeric values are specified they will be\n        mapped to colors using the cmap and vmin,vmax parameters. See\n        matplotlib.scatter for more details.\n\n    node_shape :  string (default='o')\n        The shape of the node.  Specification is as matplotlib.scatter\n        marker, one of 'so^>v<dph8'.\n\n    alpha : float or None (default=None)\n        The node and edge transparency\n\n    cmap : Matplotlib colormap, optional\n        Colormap for mapping intensities of nodes\n\n    vmin,vmax : float, optional\n        Minimum and maximum for node colormap scaling\n\n    linewidths : scalar or sequence (default=1.0)\n        Line width of symbol border\n\n    width : float or array of floats (default=1.0)\n        Line width of edges\n\n    edge_color : color or array of colors (default='k')\n        Edge color. Can be a single color or a sequence of colors with the same\n        length as edgelist. Color can be string or rgb (or rgba) tuple of\n        floats from 0-1. If numeric values are specified they will be\n        mapped to colors using the edge_cmap and edge_vmin,edge_vmax parameters.\n\n    edge_cmap : Matplotlib colormap, optional\n        Colormap for mapping intensities of edges\n\n    edge_vmin,edge_vmax : floats, optional\n        Minimum and maximum for edge colormap scaling\n\n    style : string (default=solid line)\n        Edge line style e.g.: '-', '--', '-.', ':'\n        or words like 'solid' or 'dashed'.\n        (See `matplotlib.patches.FancyArrowPatch`: `linestyle`)\n\n    labels : dictionary (default=None)\n        Node labels in a dictionary of text labels keyed by node\n\n    font_size : int (default=12 for nodes, 10 for edges)\n        Font size for text labels\n\n    font_color : string (default='k' black)\n        Font color string\n\n    font_weight : string (default='normal')\n        Font weight\n\n    font_family : string (default='sans-serif')\n        Font family\n\n    label : string, optional\n        Label for graph legend\n\n    kwds : optional keywords\n        See networkx.draw_networkx_nodes(), networkx.draw_networkx_edges(), and\n        networkx.draw_networkx_labels() for a description of optional keywords.\n\n    Notes\n    -----\n    For directed graphs, arrows  are drawn at the head end.  Arrows can be\n    turned off with keyword arrows=False.\n\n    Examples\n    --------\n    >>> G = nx.dodecahedral_graph()\n    >>> nx.draw(G)\n    >>> nx.draw(G, pos=nx.spring_layout(G))  # use spring layout\n\n    >>> import matplotlib.pyplot as plt\n    >>> limits = plt.axis(\"off\")  # turn off axis\n\n    Also see the NetworkX drawing examples at\n    https://networkx.org/documentation/latest/auto_examples/index.html\n\n    See Also\n    --------\n    draw\n    draw_networkx_nodes\n    draw_networkx_edges\n    draw_networkx_labels\n    draw_networkx_edge_labels\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    valid_node_kwds = (\n        \"nodelist\",\n        \"node_size\",\n        \"node_color\",\n        \"node_shape\",\n        \"alpha\",\n        \"cmap\",\n        \"vmin\",\n        \"vmax\",\n        \"ax\",\n        \"linewidths\",\n        \"edgecolors\",\n        \"label\",\n    )\n\n    valid_edge_kwds = (\n        \"edgelist\",\n        \"width\",\n        \"edge_color\",\n        \"style\",\n        \"alpha\",\n        \"arrowstyle\",\n        \"arrowsize\",\n        \"edge_cmap\",\n        \"edge_vmin\",\n        \"edge_vmax\",\n        \"ax\",\n        \"label\",\n        \"node_size\",\n        \"nodelist\",\n        \"node_shape\",\n        \"connectionstyle\",\n        \"min_source_margin\",\n        \"min_target_margin\",\n    )\n\n    valid_label_kwds = (\n        \"labels\",\n        \"font_size\",\n        \"font_color\",\n        \"font_family\",\n        \"font_weight\",\n        \"alpha\",\n        \"bbox\",\n        \"ax\",\n        \"horizontalalignment\",\n        \"verticalalignment\",\n    )\n\n    valid_kwds = valid_node_kwds + valid_edge_kwds + valid_label_kwds\n\n    if any([k not in valid_kwds for k in kwds]):\n        invalid_args = \", \".join([k for k in kwds if k not in valid_kwds])\n        raise ValueError(f\"Received invalid argument(s): {invalid_args}\")\n\n    node_kwds = {k: v for k, v in kwds.items() if k in valid_node_kwds}\n    edge_kwds = {k: v for k, v in kwds.items() if k in valid_edge_kwds}\n    label_kwds = {k: v for k, v in kwds.items() if k in valid_label_kwds}\n\n    if pos is None:\n        pos = nx.drawing.spring_layout(G)  # default to spring layout\n\n    draw_networkx_nodes(G, pos, **node_kwds)\n    draw_networkx_edges(G, pos, arrows=arrows, **edge_kwds)\n    if with_labels:\n        draw_networkx_labels(G, pos, **label_kwds)\n    plt.draw_if_interactive()\n\n\ndef draw_networkx_nodes(\n    G,\n    pos,\n    nodelist=None,\n    node_size=300,\n    node_color=\"#1f78b4\",\n    node_shape=\"o\",\n    alpha=None,\n    cmap=None,\n    vmin=None,\n    vmax=None,\n    ax=None,\n    linewidths=None,\n    edgecolors=None,\n    label=None,\n    margins=None,\n):\n    \"\"\"Draw the nodes of the graph G.\n\n    This draws only the nodes of the graph G.\n\n    Parameters\n    ----------\n    G : graph\n        A networkx graph\n\n    pos : dictionary\n        A dictionary with nodes as keys and positions as values.\n        Positions should be sequences of length 2.\n\n    ax : Matplotlib Axes object, optional\n        Draw the graph in the specified Matplotlib axes.\n\n    nodelist : list (default list(G))\n        Draw only specified nodes\n\n    node_size : scalar or array (default=300)\n        Size of nodes.  If an array it must be the same length as nodelist.\n\n    node_color : color or array of colors (default='#1f78b4')\n        Node color. Can be a single color or a sequence of colors with the same\n        length as nodelist. Color can be string or rgb (or rgba) tuple of\n        floats from 0-1. If numeric values are specified they will be\n        mapped to colors using the cmap and vmin,vmax parameters. See\n        matplotlib.scatter for more details.\n\n    node_shape :  string (default='o')\n        The shape of the node.  Specification is as matplotlib.scatter\n        marker, one of 'so^>v<dph8'.\n\n    alpha : float or array of floats (default=None)\n        The node transparency.  This can be a single alpha value,\n        in which case it will be applied to all the nodes of color. Otherwise,\n        if it is an array, the elements of alpha will be applied to the colors\n        in order (cycling through alpha multiple times if necessary).\n\n    cmap : Matplotlib colormap (default=None)\n        Colormap for mapping intensities of nodes\n\n    vmin,vmax : floats or None (default=None)\n        Minimum and maximum for node colormap scaling\n\n    linewidths : [None | scalar | sequence] (default=1.0)\n        Line width of symbol border\n\n    edgecolors : [None | scalar | sequence] (default = node_color)\n        Colors of node borders\n\n    label : [None | string]\n        Label for legend\n\n    margins : [None | list] (default=None)\n        Horizontal and vertical plot margins. List should contain exactly two elements.\n\n    Returns\n    -------\n    matplotlib.collections.PathCollection\n        `PathCollection` of the nodes.\n\n    Examples\n    --------\n    >>> G = nx.dodecahedral_graph()\n    >>> nodes = nx.draw_networkx_nodes(G, pos=nx.spring_layout(G))\n\n    Also see the NetworkX drawing examples at\n    https://networkx.org/documentation/latest/auto_examples/index.html\n\n    See Also\n    --------\n    draw\n    draw_networkx\n    draw_networkx_edges\n    draw_networkx_labels\n    draw_networkx_edge_labels\n    \"\"\"\n    from collections.abc import Iterable\n    import numpy as np\n    import matplotlib as mpl\n    import matplotlib.collections  # call as mpl.collections\n    import matplotlib.pyplot as plt\n\n    if ax is None:\n        ax = plt.gca()\n\n    if nodelist is None:\n        nodelist = list(G)\n\n    if len(nodelist) == 0:  # empty nodelist, no drawing\n        return mpl.collections.PathCollection(None)\n\n    try:\n        xy = np.asarray([pos[v] for v in nodelist])\n    except KeyError as e:\n        raise nx.NetworkXError(f\"Node {e} has no position.\") from e\n\n    if isinstance(alpha, Iterable):\n        node_color = apply_alpha(node_color, alpha, nodelist, cmap, vmin, vmax)\n        alpha = None\n\n    node_collection = ax.scatter(\n        xy[:, 0],\n        xy[:, 1],\n        s=node_size,\n        c=node_color,\n        marker=node_shape,\n        cmap=cmap,\n        vmin=vmin,\n        vmax=vmax,\n        alpha=alpha,\n        linewidths=linewidths,\n        edgecolors=edgecolors,\n        label=label,\n    )\n    ax.tick_params(\n        axis=\"both\",\n        which=\"both\",\n        bottom=False,\n        left=False,\n        labelbottom=False,\n        labelleft=False,\n    )\n\n    ax.margins(margins[0], margins[1])\n\n    node_collection.set_zorder(2)\n    return node_collection\n\n\ndef draw_networkx_edges(\n    G,\n    pos,\n    edgelist=None,\n    width=1.0,\n    edge_color=\"k\",\n    style=\"solid\",\n    alpha=None,\n    arrowstyle=None,\n    arrowsize=10,\n    edge_cmap=None,\n    edge_vmin=None,\n    edge_vmax=None,\n    ax=None,\n    arrows=True,\n    label=None,\n    node_size=300,\n    nodelist=None,\n    node_shape=\"o\",\n    connectionstyle=\"arc3\",\n    min_source_margin=0,\n    min_target_margin=0,\n):\n    \"\"\"Draw the edges of the graph G.\n\n    This draws only the edges of the graph G.\n\n    Parameters\n    ----------\n    G : graph\n        A networkx graph\n\n    pos : dictionary\n        A dictionary with nodes as keys and positions as values.\n        Positions should be sequences of length 2.\n\n    edgelist : collection of edge tuples (default=G.edges())\n        Draw only specified edges\n\n    width : float or array of floats (default=1.0)\n        Line width of edges\n\n    edge_color : color or array of colors (default='k')\n        Edge color. Can be a single color or a sequence of colors with the same\n        length as edgelist. Color can be string or rgb (or rgba) tuple of\n        floats from 0-1. If numeric values are specified they will be\n        mapped to colors using the edge_cmap and edge_vmin,edge_vmax parameters.\n\n    style : string (default=solid line)\n        Edge line style e.g.: '-', '--', '-.', ':'\n        or words like 'solid' or 'dashed'.\n        (See `matplotlib.patches.FancyArrowPatch`: `linestyle`)\n\n    alpha : float or None (default=None)\n        The edge transparency\n\n    edge_cmap : Matplotlib colormap, optional\n        Colormap for mapping intensities of edges\n\n    edge_vmin,edge_vmax : floats, optional\n        Minimum and maximum for edge colormap scaling\n\n    ax : Matplotlib Axes object, optional\n        Draw the graph in the specified Matplotlib axes.\n\n    arrows : bool, optional (default=True)\n        For directed graphs, if True set default to drawing arrowheads.\n        Otherwise set default to no arrowheads. Ignored if `arrowstyle` is set.\n\n        Note: Arrows will be the same color as edges.\n\n    arrowstyle : str (default='-\\|>' if directed else '-')\n        For directed graphs and `arrows==True` defaults to '-\\|>',\n        otherwise defaults to '-'.\n\n        See `matplotlib.patches.ArrowStyle` for more options.\n\n    arrowsize : int (default=10)\n        For directed graphs, choose the size of the arrow head's length and\n        width. See `matplotlib.patches.FancyArrowPatch` for attribute\n        `mutation_scale` for more info.\n\n    connectionstyle : string (default=\"arc3\")\n        Pass the connectionstyle parameter to create curved arc of rounding\n        radius rad. For example, connectionstyle='arc3,rad=0.2'.\n        See `matplotlib.patches.ConnectionStyle` and\n        `matplotlib.patches.FancyArrowPatch` for more info.\n\n    node_size : scalar or array (default=300)\n        Size of nodes. Though the nodes are not drawn with this function, the\n        node size is used in determining edge positioning.\n\n    nodelist : list, optional (default=G.nodes())\n       This provides the node order for the `node_size` array (if it is an array).\n\n    node_shape :  string (default='o')\n        The marker used for nodes, used in determining edge positioning.\n        Specification is as a `matplotlib.markers` marker, e.g. one of 'so^>v<dph8'.\n\n    label : None or string\n        Label for legend\n\n    min_source_margin : int (default=0)\n        The minimum margin (gap) at the begining of the edge at the source.\n\n    min_target_margin : int (default=0)\n        The minimum margin (gap) at the end of the edge at the target.\n\n    Returns\n    -------\n    list of matplotlib.patches.FancyArrowPatch\n        `FancyArrowPatch` instances of the directed edges\n\n    Notes\n    -----\n    For directed graphs, arrows are drawn at the head end.  Arrows can be\n    turned off with keyword arrows=False or by passing an arrowstyle without\n    an arrow on the end.\n\n    Be sure to include `node_size` as a keyword argument; arrows are\n    drawn considering the size of nodes.\n\n    Examples\n    --------\n    >>> G = nx.dodecahedral_graph()\n    >>> edges = nx.draw_networkx_edges(G, pos=nx.spring_layout(G))\n\n    >>> G = nx.DiGraph()\n    >>> G.add_edges_from([(1, 2), (1, 3), (2, 3)])\n    >>> arcs = nx.draw_networkx_edges(G, pos=nx.spring_layout(G))\n    >>> alphas = [0.3, 0.4, 0.5]\n    >>> for i, arc in enumerate(arcs):  # change alpha values of arcs\n    ...     arc.set_alpha(alphas[i])\n\n    Also see the NetworkX drawing examples at\n    https://networkx.org/documentation/latest/auto_examples/index.html\n\n    See Also\n    --------\n    draw\n    draw_networkx\n    draw_networkx_nodes\n    draw_networkx_labels\n    draw_networkx_edge_labels\n\n    \"\"\"\n    import numpy as np\n    import matplotlib as mpl\n    import matplotlib.colors  # call as mpl.colors\n    import matplotlib.patches  # call as mpl.patches\n    import matplotlib.path  # call as mpl.path\n    import matplotlib.pyplot as plt\n\n    if arrowstyle is None:\n        if G.is_directed() and arrows:\n            arrowstyle = \"-|>\"\n        else:\n            arrowstyle = \"-\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    if edgelist is None:\n        edgelist = list(G.edges())\n\n    if len(edgelist) == 0:  # no edges!\n        return []\n\n    if nodelist is None:\n        nodelist = list(G.nodes())\n\n    # FancyArrowPatch handles color=None different from LineCollection\n    if edge_color is None:\n        edge_color = \"k\"\n\n    # set edge positions\n    edge_pos = np.asarray([(pos[e[0]], pos[e[1]]) for e in edgelist])\n\n    # Check if edge_color is an array of floats and map to edge_cmap.\n    # This is the only case handled differently from matplotlib\n    if (\n        np.iterable(edge_color)\n        and (len(edge_color) == len(edge_pos))\n        and np.alltrue([isinstance(c, Number) for c in edge_color])\n    ):\n        if edge_cmap is not None:\n            assert isinstance(edge_cmap, mpl.colors.Colormap)\n        else:\n            edge_cmap = plt.get_cmap()\n        if edge_vmin is None:\n            edge_vmin = min(edge_color)\n        if edge_vmax is None:\n            edge_vmax = max(edge_color)\n        color_normal = mpl.colors.Normalize(vmin=edge_vmin, vmax=edge_vmax)\n        edge_color = [edge_cmap(color_normal(e)) for e in edge_color]\n\n    # Note: Waiting for someone to implement arrow to intersection with\n    # marker.  Meanwhile, this works well for polygons with more than 4\n    # sides and circle.\n\n    def to_marker_edge(marker_size, marker):\n        if marker in \"s^>v<d\":  # `large` markers need extra space\n            return np.sqrt(2 * marker_size) / 2\n        else:\n            return np.sqrt(marker_size) / 2\n\n    # Draw arrows with `matplotlib.patches.FancyarrowPatch`\n    arrow_collection = []\n    mutation_scale = arrowsize  # scale factor of arrow head\n\n    # compute view\n    minx = np.amin(np.ravel(edge_pos[:, :, 0]))\n    maxx = np.amax(np.ravel(edge_pos[:, :, 0]))\n    miny = np.amin(np.ravel(edge_pos[:, :, 1]))\n    maxy = np.amax(np.ravel(edge_pos[:, :, 1]))\n    w = maxx - minx\n    h = maxy - miny\n\n    base_connection_style = mpl.patches.ConnectionStyle(connectionstyle)\n\n    # Fallback for self-loop scale. Left outside of _connectionstyle so it is\n    # only computed once\n    max_nodesize = np.array(node_size).max()\n\n    def _connectionstyle(posA, posB, *args, **kwargs):\n        # check if we need to do a self-loop\n        if np.all(posA == posB):\n            # Self-loops are scaled by view extent, except in cases the extent\n            # is 0, e.g. for a single node. In this case, fall back to scaling\n            # by the maximum node size\n            selfloop_ht = 0.005 * max_nodesize if h == 0 else h\n            # this is called with _screen space_ values so covert back\n            # to data space\n            data_loc = ax.transData.inverted().transform(posA)\n            v_shift = 0.1 * selfloop_ht\n            h_shift = v_shift * 0.5\n            # put the top of the loop first so arrow is not hidden by node\n            path = [\n                # 1\n                data_loc + np.asarray([0, v_shift]),\n                # 4 4 4\n                data_loc + np.asarray([h_shift, v_shift]),\n                data_loc + np.asarray([h_shift, 0]),\n                data_loc,\n                # 4 4 4\n                data_loc + np.asarray([-h_shift, 0]),\n                data_loc + np.asarray([-h_shift, v_shift]),\n                data_loc + np.asarray([0, v_shift]),\n            ]\n\n            ret = mpl.path.Path(ax.transData.transform(path), [1, 4, 4, 4, 4, 4, 4])\n        # if not, fall back to the user specified behavior\n        else:\n            ret = base_connection_style(posA, posB, *args, **kwargs)\n\n        return ret\n\n    # FancyArrowPatch doesn't handle color strings\n    arrow_colors = mpl.colors.colorConverter.to_rgba_array(edge_color, alpha)\n    for i, (src, dst) in enumerate(edge_pos):\n        x1, y1 = src\n        x2, y2 = dst\n        shrink_source = 0  # space from source to tail\n        shrink_target = 0  # space from  head to target\n        if np.iterable(node_size):  # many node sizes\n            source, target = edgelist[i][:2]\n            source_node_size = node_size[nodelist.index(source)]\n            target_node_size = node_size[nodelist.index(target)]\n            shrink_source = to_marker_edge(source_node_size, node_shape)\n            shrink_target = to_marker_edge(target_node_size, node_shape)\n        else:\n            shrink_source = shrink_target = to_marker_edge(node_size, node_shape)\n\n        if shrink_source < min_source_margin:\n            shrink_source = min_source_margin\n\n        if shrink_target < min_target_margin:\n            shrink_target = min_target_margin\n\n        if len(arrow_colors) == len(edge_pos):\n            arrow_color = arrow_colors[i]\n        elif len(arrow_colors) == 1:\n            arrow_color = arrow_colors[0]\n        else:  # Cycle through colors\n            arrow_color = arrow_colors[i % len(arrow_colors)]\n\n        if np.iterable(width):\n            if len(width) == len(edge_pos):\n                line_width = width[i]\n            else:\n                line_width = width[i % len(width)]\n        else:\n            line_width = width\n\n        arrow = mpl.patches.FancyArrowPatch(\n            (x1, y1),\n            (x2, y2),\n            arrowstyle=arrowstyle,\n            shrinkA=shrink_source,\n            shrinkB=shrink_target,\n            mutation_scale=mutation_scale,\n            color=arrow_color,\n            linewidth=line_width,\n            connectionstyle=_connectionstyle,\n            linestyle=style,\n            zorder=1,\n        )  # arrows go behind nodes\n\n        arrow_collection.append(arrow)\n        ax.add_patch(arrow)\n\n    # update view\n    padx, pady = 0.05 * w, 0.05 * h\n    corners = (minx - padx, miny - pady), (maxx + padx, maxy + pady)\n    ax.update_datalim(corners)\n    ax.autoscale_view()\n\n    ax.tick_params(\n        axis=\"both\",\n        which=\"both\",\n        bottom=False,\n        left=False,\n        labelbottom=False,\n        labelleft=False,\n    )\n\n    return arrow_collection\n\n\ndef draw_networkx_labels(\n    G,\n    pos,\n    labels=None,\n    font_size=12,\n    font_color=\"k\",\n    font_family=\"sans-serif\",\n    font_weight=\"normal\",\n    alpha=None,\n    bbox=None,\n    horizontalalignment=\"center\",\n    verticalalignment=\"center\",\n    ax=None,\n    clip_on=True,\n):\n    \"\"\"Draw node labels on the graph G.\n\n    Parameters\n    ----------\n    G : graph\n        A networkx graph\n\n    pos : dictionary\n        A dictionary with nodes as keys and positions as values.\n        Positions should be sequences of length 2.\n\n    labels : dictionary (default={n: n for n in G})\n        Node labels in a dictionary of text labels keyed by node.\n        Node-keys in labels should appear as keys in `pos`.\n        If needed use: `{n:lab for n,lab in labels.items() if n in pos}`\n\n    font_size : int (default=12)\n        Font size for text labels\n\n    font_color : string (default='k' black)\n        Font color string\n\n    font_weight : string (default='normal')\n        Font weight\n\n    font_family : string (default='sans-serif')\n        Font family\n\n    alpha : float or None (default=None)\n        The text transparency\n\n    bbox : Matplotlib bbox, (default is Matplotlib's ax.text default)\n        Specify text box properties (e.g. shape, color etc.) for node labels.\n\n    horizontalalignment : string (default='center')\n        Horizontal alignment {'center', 'right', 'left'}\n\n    verticalalignment : string (default='center')\n        Vertical alignment {'center', 'top', 'bottom', 'baseline', 'center_baseline'}\n\n    ax : Matplotlib Axes object, optional\n        Draw the graph in the specified Matplotlib axes.\n\n    clip_on : bool (default=True)\n        Turn on clipping of node labels at axis boundaries\n\n    Returns\n    -------\n    dict\n        `dict` of labels keyed on the nodes\n\n    Examples\n    --------\n    >>> G = nx.dodecahedral_graph()\n    >>> labels = nx.draw_networkx_labels(G, pos=nx.spring_layout(G))\n\n    Also see the NetworkX drawing examples at\n    https://networkx.org/documentation/latest/auto_examples/index.html\n\n    See Also\n    --------\n    draw\n    draw_networkx\n    draw_networkx_nodes\n    draw_networkx_edges\n    draw_networkx_edge_labels\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    if ax is None:\n        ax = plt.gca()\n\n    if labels is None:\n        labels = {n: n for n in G.nodes()}\n\n    text_items = {}  # there is no text collection so we'll fake one\n    for n, label in labels.items():\n        (x, y) = pos[n]\n        if not isinstance(label, str):\n            label = str(label)  # this makes \"1\" and 1 labeled the same\n        t = ax.text(\n            x,\n            y,\n            label,\n            size=font_size,\n            color=font_color,\n            family=font_family,\n            weight=font_weight,\n            alpha=alpha,\n            horizontalalignment=horizontalalignment,\n            verticalalignment=verticalalignment,\n            transform=ax.transData,\n            bbox=bbox,\n            clip_on=clip_on,\n        )\n        text_items[n] = t\n\n    ax.tick_params(\n        axis=\"both\",\n        which=\"both\",\n        bottom=False,\n        left=False,\n        labelbottom=False,\n        labelleft=False,\n    )\n\n    return text_items\n\n\ndef draw_networkx_edge_labels(\n    G,\n    pos,\n    edge_labels=None,\n    label_pos=0.5,\n    font_size=10,\n    font_color=\"k\",\n    font_family=\"sans-serif\",\n    font_weight=\"normal\",\n    alpha=None,\n    bbox=None,\n    horizontalalignment=\"center\",\n    verticalalignment=\"center\",\n    ax=None,\n    rotate=True,\n    clip_on=True,\n):\n    \"\"\"Draw edge labels.\n\n    Parameters\n    ----------\n    G : graph\n        A networkx graph\n\n    pos : dictionary\n        A dictionary with nodes as keys and positions as values.\n        Positions should be sequences of length 2.\n\n    edge_labels : dictionary (default={})\n        Edge labels in a dictionary of labels keyed by edge two-tuple.\n        Only labels for the keys in the dictionary are drawn.\n\n    label_pos : float (default=0.5)\n        Position of edge label along edge (0=head, 0.5=center, 1=tail)\n\n    font_size : int (default=10)\n        Font size for text labels\n\n    font_color : string (default='k' black)\n        Font color string\n\n    font_weight : string (default='normal')\n        Font weight\n\n    font_family : string (default='sans-serif')\n        Font family\n\n    alpha : float or None (default=None)\n        The text transparency\n\n    bbox : Matplotlib bbox, optional\n        Specify text box properties (e.g. shape, color etc.) for edge labels.\n        Default is {boxstyle='round', ec=(1.0, 1.0, 1.0), fc=(1.0, 1.0, 1.0)}.\n\n    horizontalalignment : string (default='center')\n        Horizontal alignment {'center', 'right', 'left'}\n\n    verticalalignment : string (default='center')\n        Vertical alignment {'center', 'top', 'bottom', 'baseline', 'center_baseline'}\n\n    ax : Matplotlib Axes object, optional\n        Draw the graph in the specified Matplotlib axes.\n\n    rotate : bool (deafult=True)\n        Rotate edge labels to lie parallel to edges\n\n    clip_on : bool (default=True)\n        Turn on clipping of edge labels at axis boundaries\n\n    Returns\n    -------\n    dict\n        `dict` of labels keyed by edge\n\n    Examples\n    --------\n    >>> G = nx.dodecahedral_graph()\n    >>> edge_labels = nx.draw_networkx_edge_labels(G, pos=nx.spring_layout(G))\n\n    Also see the NetworkX drawing examples at\n    https://networkx.org/documentation/latest/auto_examples/index.html\n\n    See Also\n    --------\n    draw\n    draw_networkx\n    draw_networkx_nodes\n    draw_networkx_edges\n    draw_networkx_labels\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    if ax is None:\n        ax = plt.gca()\n    if edge_labels is None:\n        labels = {(u, v): d for u, v, d in G.edges(data=True)}\n    else:\n        labels = edge_labels\n    text_items = {}\n    for (n1, n2), label in labels.items():\n        (x1, y1) = pos[n1]\n        (x2, y2) = pos[n2]\n        (x, y) = (\n            x1 * label_pos + x2 * (1.0 - label_pos),\n            y1 * label_pos + y2 * (1.0 - label_pos),\n        )\n\n        if rotate:\n            # in degrees\n            angle = np.arctan2(y2 - y1, x2 - x1) / (2.0 * np.pi) * 360\n            # make label orientation \"right-side-up\"\n            if angle > 90:\n                angle -= 180\n            if angle < -90:\n                angle += 180\n            # transform data coordinate angle to screen coordinate angle\n            xy = np.array((x, y))\n            trans_angle = ax.transData.transform_angles(\n                np.array((angle,)), xy.reshape((1, 2))\n            )[0]\n        else:\n            trans_angle = 0.0\n        # use default box of white with white border\n        if bbox is None:\n            bbox = dict(boxstyle=\"round\", ec=(1.0, 1.0, 1.0), fc=(1.0, 1.0, 1.0))\n        if not isinstance(label, str):\n            label = str(label)  # this makes \"1\" and 1 labeled the same\n\n        t = ax.text(\n            x,\n            y,\n            label,\n            size=font_size,\n            color=font_color,\n            family=font_family,\n            weight=font_weight,\n            alpha=alpha,\n            horizontalalignment=horizontalalignment,\n            verticalalignment=verticalalignment,\n            rotation=trans_angle,\n            transform=ax.transData,\n            bbox=bbox,\n            zorder=1,\n            clip_on=clip_on,\n        )\n        text_items[(n1, n2)] = t\n\n    ax.tick_params(\n        axis=\"both\",\n        which=\"both\",\n        bottom=False,\n        left=False,\n        labelbottom=False,\n        labelleft=False,\n    )\n\n    return text_items\n\n\ndef draw_circular(G, **kwargs):\n    \"\"\"Draw the graph G with a circular layout.\n\n    Parameters\n    ----------\n    G : graph\n        A networkx graph\n\n    kwargs : optional keywords\n        See networkx.draw_networkx() for a description of optional keywords,\n        with the exception of the pos parameter which is not used by this\n        function.\n    \"\"\"\n    draw(G, circular_layout(G), **kwargs)\n\n\ndef draw_kamada_kawai(G, **kwargs):\n    \"\"\"Draw the graph G with a Kamada-Kawai force-directed layout.\n\n    Parameters\n    ----------\n    G : graph\n        A networkx graph\n\n    kwargs : optional keywords\n        See networkx.draw_networkx() for a description of optional keywords,\n        with the exception of the pos parameter which is not used by this\n        function.\n    \"\"\"\n    draw(G, kamada_kawai_layout(G), **kwargs)\n\n\ndef draw_random(G, **kwargs):\n    \"\"\"Draw the graph G with a random layout.\n\n    Parameters\n    ----------\n    G : graph\n        A networkx graph\n\n    kwargs : optional keywords\n        See networkx.draw_networkx() for a description of optional keywords,\n        with the exception of the pos parameter which is not used by this\n        function.\n    \"\"\"\n    draw(G, random_layout(G), **kwargs)\n\n\ndef draw_spectral(G, **kwargs):\n    \"\"\"Draw the graph G with a spectral 2D layout.\n\n    Using the unnormalized Laplacian, the layout shows possible clusters of\n    nodes which are an approximation of the ratio cut. The positions are the\n    entries of the second and third eigenvectors corresponding to the\n    ascending eigenvalues starting from the second one.\n\n    Parameters\n    ----------\n    G : graph\n        A networkx graph\n\n    kwargs : optional keywords\n        See networkx.draw_networkx() for a description of optional keywords,\n        with the exception of the pos parameter which is not used by this\n        function.\n    \"\"\"\n    draw(G, spectral_layout(G), **kwargs)\n\n\ndef draw_spring(G, **kwargs):\n    \"\"\"Draw the graph G with a spring layout.\n\n    Parameters\n    ----------\n    G : graph\n        A networkx graph\n\n    kwargs : optional keywords\n        See networkx.draw_networkx() for a description of optional keywords,\n        with the exception of the pos parameter which is not used by this\n        function.\n    \"\"\"\n    draw(G, spring_layout(G), **kwargs)\n\n\ndef draw_shell(G, **kwargs):\n    \"\"\"Draw networkx graph with shell layout.\n\n    Parameters\n    ----------\n    G : graph\n        A networkx graph\n\n    kwargs : optional keywords\n        See networkx.draw_networkx() for a description of optional keywords,\n        with the exception of the pos parameter which is not used by this\n        function.\n    \"\"\"\n    nlist = kwargs.get(\"nlist\", None)\n    if nlist is not None:\n        del kwargs[\"nlist\"]\n    draw(G, shell_layout(G, nlist=nlist), **kwargs)\n\n\ndef draw_planar(G, **kwargs):\n    \"\"\"Draw a planar networkx graph with planar layout.\n\n    Parameters\n    ----------\n    G : graph\n        A planar networkx graph\n\n    kwargs : optional keywords\n        See networkx.draw_networkx() for a description of optional keywords,\n        with the exception of the pos parameter which is not used by this\n        function.\n    \"\"\"\n    draw(G, planar_layout(G), **kwargs)\n\n\ndef apply_alpha(colors, alpha, elem_list, cmap=None, vmin=None, vmax=None):\n    \"\"\"Apply an alpha (or list of alphas) to the colors provided.\n\n    Parameters\n    ----------\n\n    colors : color string or array of floats (default='r')\n        Color of element. Can be a single color format string,\n        or a sequence of colors with the same length as nodelist.\n        If numeric values are specified they will be mapped to\n        colors using the cmap and vmin,vmax parameters.  See\n        matplotlib.scatter for more details.\n\n    alpha : float or array of floats\n        Alpha values for elements. This can be a single alpha value, in\n        which case it will be applied to all the elements of color. Otherwise,\n        if it is an array, the elements of alpha will be applied to the colors\n        in order (cycling through alpha multiple times if necessary).\n\n    elem_list : array of networkx objects\n        The list of elements which are being colored. These could be nodes,\n        edges or labels.\n\n    cmap : matplotlib colormap\n        Color map for use if colors is a list of floats corresponding to points\n        on a color mapping.\n\n    vmin, vmax : float\n        Minimum and maximum values for normalizing colors if a colormap is used\n\n    Returns\n    -------\n\n    rgba_colors : numpy ndarray\n        Array containing RGBA format values for each of the node colours.\n\n    \"\"\"\n    from itertools import islice, cycle\n    import numpy as np\n    import matplotlib as mpl\n    import matplotlib.colors  # call as mpl.colors\n    import matplotlib.cm  # call as mpl.cm\n\n    # If we have been provided with a list of numbers as long as elem_list,\n    # apply the color mapping.\n    if len(colors) == len(elem_list) and isinstance(colors[0], Number):\n        mapper = mpl.cm.ScalarMappable(cmap=cmap)\n        mapper.set_clim(vmin, vmax)\n        rgba_colors = mapper.to_rgba(colors)\n    # Otherwise, convert colors to matplotlib's RGB using the colorConverter\n    # object.  These are converted to numpy ndarrays to be consistent with the\n    # to_rgba method of ScalarMappable.\n    else:\n        try:\n            rgba_colors = np.array([mpl.colors.colorConverter.to_rgba(colors)])\n        except ValueError:\n            rgba_colors = np.array(\n                [mpl.colors.colorConverter.to_rgba(color) for color in colors]\n            )\n    # Set the final column of the rgba_colors to have the relevant alpha values\n    try:\n        # If alpha is longer than the number of colors, resize to the number of\n        # elements.  Also, if rgba_colors.size (the number of elements of\n        # rgba_colors) is the same as the number of elements, resize the array,\n        # to avoid it being interpreted as a colormap by scatter()\n        if len(alpha) > len(rgba_colors) or rgba_colors.size == len(elem_list):\n            rgba_colors = np.resize(rgba_colors, (len(elem_list), 4))\n            rgba_colors[1:, 0] = rgba_colors[0, 0]\n            rgba_colors[1:, 1] = rgba_colors[0, 1]\n            rgba_colors[1:, 2] = rgba_colors[0, 2]\n        rgba_colors[:, 3] = list(islice(cycle(alpha), len(rgba_colors)))\n    except TypeError:\n        rgba_colors[:, -1] = alpha\n    return rgba_colors\n", "hunk": "@@ -474,7 +474,8 @@ def draw_networkx_nodes(\n         labelleft=False,\n     )\n \n-    ax.margins(margins[0], margins[1])\n+    if margins is not None:\n+        ax.margins(margins[0], margins[1])\n \n     node_collection.set_zorder(2)\n     return node_collection\n", "comment": "```suggestion if margins is not None: ax.margins(margins[0], margins[1]) ``` I think this is the cause of (at least) most errors. We have to treat the default case explicitly.", "ids": ["22379", "58ce7b013c670751792190e08b406398a1a3ac6a", "8f08c40b2d7d3291971171d6984bd1462d109fe1"], "repo": "networkx/networkx", "ghid": 4769, "old": "         labelleft=False,\n     )\n-    ax.margins(margins[0], margins[1])\n     node_collection.set_zorder(2)\n     return node_collection", "new": "         labelleft=False,\n     )\n+    if margins is not None:\n+        ax.margins(margins[0], margins[1])\n     node_collection.set_zorder(2)\n     return node_collection", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -30,6 +30,37 @@ def test_ce_loss():\n     loss_cls = build_loss(loss_cls_cfg)\n     assert torch.allclose(loss_cls(fake_pred, fake_label), torch.tensor(200.))\n \n+    # test bce_loss\n+    cls_score = torch.Tensor([[-200, 100], [500, -1000], [300, -300]])", "oldf": "import pytest\nimport torch\n\nfrom mmdet.models import Accuracy, build_loss\n\n\ndef test_ce_loss():\n    # use_mask and use_sigmoid cannot be true at the same time\n    with pytest.raises(AssertionError):\n        loss_cfg = dict(\n            type='CrossEntropyLoss',\n            use_mask=True,\n            use_sigmoid=True,\n            loss_weight=1.0)\n        build_loss(loss_cfg)\n\n    # test loss with class weights\n    loss_cls_cfg = dict(\n        type='CrossEntropyLoss',\n        use_sigmoid=False,\n        class_weight=[0.8, 0.2],\n        loss_weight=1.0)\n    loss_cls = build_loss(loss_cls_cfg)\n    fake_pred = torch.Tensor([[100, -100]])\n    fake_label = torch.Tensor([1]).long()\n    assert torch.allclose(loss_cls(fake_pred, fake_label), torch.tensor(40.))\n\n    loss_cls_cfg = dict(\n        type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)\n    loss_cls = build_loss(loss_cls_cfg)\n    assert torch.allclose(loss_cls(fake_pred, fake_label), torch.tensor(200.))\n\n    # test bce_loss\n    cls_score = torch.Tensor([[-200, 100], [500, -1000], [300, -300]])\n    label = torch.Tensor([0, 1, 0]).long()\n    weight = torch.Tensor([0.6, 0.4, 0.5])\n    class_weight = torch.tensor([0.1, 0.9])  # class 0: 0.1, class 1: 0.9\n\n    # test bce_loss without class weight\n    loss_cfg = dict(\n        type='CrossEntropyLoss',\n        use_sigmoid=True,\n        reduction='mean',\n        loss_weight=1.0)\n    loss = build_loss(loss_cfg)\n    assert torch.allclose(loss(cls_score, label), torch.tensor(300.))\n    # test ce_loss with weight\n    assert torch.allclose(\n        loss(cls_score, label, weight=weight), torch.tensor(130.))\n\n    # test bce_loss with class weight\n    loss_cfg = dict(\n        type='CrossEntropyLoss',\n        use_sigmoid=True,\n        reduction='mean',\n        loss_weight=1.0,\n        class_weight=class_weight)\n    loss = build_loss(loss_cfg)\n    assert torch.allclose(loss(cls_score, label), torch.tensor(176.667))\n    # test bce_loss with weight\n    assert torch.allclose(\n        loss(cls_score, label, weight=weight), torch.tensor(74.333))\n\n\ndef test_varifocal_loss():\n    # only sigmoid version of VarifocalLoss is implemented\n    with pytest.raises(AssertionError):\n        loss_cfg = dict(\n            type='VarifocalLoss', use_sigmoid=False, loss_weight=1.0)\n        build_loss(loss_cfg)\n\n    # test that alpha should be greater than 0\n    with pytest.raises(AssertionError):\n        loss_cfg = dict(\n            type='VarifocalLoss',\n            alpha=-0.75,\n            gamma=2.0,\n            use_sigmoid=True,\n            loss_weight=1.0)\n        build_loss(loss_cfg)\n\n    # test that pred and target should be of the same size\n    loss_cls_cfg = dict(\n        type='VarifocalLoss',\n        use_sigmoid=True,\n        alpha=0.75,\n        gamma=2.0,\n        iou_weighted=True,\n        reduction='mean',\n        loss_weight=1.0)\n    loss_cls = build_loss(loss_cls_cfg)\n    with pytest.raises(AssertionError):\n        fake_pred = torch.Tensor([[100.0, -100.0]])\n        fake_target = torch.Tensor([[1.0]])\n        loss_cls(fake_pred, fake_target)\n\n    # test the calculation\n    loss_cls = build_loss(loss_cls_cfg)\n    fake_pred = torch.Tensor([[100.0, -100.0]])\n    fake_target = torch.Tensor([[1.0, 0.0]])\n    assert torch.allclose(loss_cls(fake_pred, fake_target), torch.tensor(0.0))\n\n    # test the loss with weights\n    loss_cls = build_loss(loss_cls_cfg)\n    fake_pred = torch.Tensor([[0.0, 100.0]])\n    fake_target = torch.Tensor([[1.0, 1.0]])\n    fake_weight = torch.Tensor([0.0, 1.0])\n    assert torch.allclose(\n        loss_cls(fake_pred, fake_target, fake_weight), torch.tensor(0.0))\n\n\ndef test_kd_loss():\n    # test that temeprature should be greater than 1\n    with pytest.raises(AssertionError):\n        loss_cfg = dict(\n            type='KnowledgeDistillationKLDivLoss', loss_weight=1.0, T=0.5)\n        build_loss(loss_cfg)\n\n    # test that pred and target should be of the same size\n    loss_cls_cfg = dict(\n        type='KnowledgeDistillationKLDivLoss', loss_weight=1.0, T=1)\n    loss_cls = build_loss(loss_cls_cfg)\n    with pytest.raises(AssertionError):\n        fake_pred = torch.Tensor([[100, -100]])\n        fake_label = torch.Tensor([1]).long()\n        loss_cls(fake_pred, fake_label)\n\n    # test the calculation\n    loss_cls = build_loss(loss_cls_cfg)\n    fake_pred = torch.Tensor([[100.0, 100.0]])\n    fake_target = torch.Tensor([[1.0, 1.0]])\n    assert torch.allclose(loss_cls(fake_pred, fake_target), torch.tensor(0.0))\n\n    # test the loss with weights\n    loss_cls = build_loss(loss_cls_cfg)\n    fake_pred = torch.Tensor([[100.0, -100.0], [100.0, 100.0]])\n    fake_target = torch.Tensor([[1.0, 0.0], [1.0, 1.0]])\n    fake_weight = torch.Tensor([0.0, 1.0])\n    assert torch.allclose(\n        loss_cls(fake_pred, fake_target, fake_weight), torch.tensor(0.0))\n\n\ndef test_seesaw_loss():\n    # only softmax version of Seesaw Loss is implemented\n    with pytest.raises(AssertionError):\n        loss_cfg = dict(type='SeesawLoss', use_sigmoid=True, loss_weight=1.0)\n        build_loss(loss_cfg)\n\n    # test that cls_score.size(-1) == num_classes + 2\n    loss_cls_cfg = dict(\n        type='SeesawLoss', p=0.0, q=0.0, loss_weight=1.0, num_classes=2)\n    loss_cls = build_loss(loss_cls_cfg)\n    # the length of fake_pred should be num_classes + 2 = 4\n    with pytest.raises(AssertionError):\n        fake_pred = torch.Tensor([[-100, 100]])\n        fake_label = torch.Tensor([1]).long()\n        loss_cls(fake_pred, fake_label)\n    # the length of fake_pred should be num_classes + 2 = 4\n    with pytest.raises(AssertionError):\n        fake_pred = torch.Tensor([[-100, 100, -100]])\n        fake_label = torch.Tensor([1]).long()\n        loss_cls(fake_pred, fake_label)\n\n    # test the calculation without p and q\n    loss_cls_cfg = dict(\n        type='SeesawLoss', p=0.0, q=0.0, loss_weight=1.0, num_classes=2)\n    loss_cls = build_loss(loss_cls_cfg)\n    fake_pred = torch.Tensor([[-100, 100, -100, 100]])\n    fake_label = torch.Tensor([1]).long()\n    loss = loss_cls(fake_pred, fake_label)\n    assert torch.allclose(loss['loss_cls_objectness'], torch.tensor(200.))\n    assert torch.allclose(loss['loss_cls_classes'], torch.tensor(0.))\n\n    # test the calculation with p and without q\n    loss_cls_cfg = dict(\n        type='SeesawLoss', p=1.0, q=0.0, loss_weight=1.0, num_classes=2)\n    loss_cls = build_loss(loss_cls_cfg)\n    fake_pred = torch.Tensor([[-100, 100, -100, 100]])\n    fake_label = torch.Tensor([0]).long()\n    loss_cls.cum_samples[0] = torch.exp(torch.Tensor([20]))\n    loss = loss_cls(fake_pred, fake_label)\n    assert torch.allclose(loss['loss_cls_objectness'], torch.tensor(200.))\n    assert torch.allclose(loss['loss_cls_classes'], torch.tensor(180.))\n\n    # test the calculation with q and without p\n    loss_cls_cfg = dict(\n        type='SeesawLoss', p=0.0, q=1.0, loss_weight=1.0, num_classes=2)\n    loss_cls = build_loss(loss_cls_cfg)\n    fake_pred = torch.Tensor([[-100, 100, -100, 100]])\n    fake_label = torch.Tensor([0]).long()\n    loss = loss_cls(fake_pred, fake_label)\n    assert torch.allclose(loss['loss_cls_objectness'], torch.tensor(200.))\n    assert torch.allclose(loss['loss_cls_classes'],\n                          torch.tensor(200.) + torch.tensor(100.).log())\n\n    # test the others\n    loss_cls_cfg = dict(\n        type='SeesawLoss',\n        p=0.0,\n        q=1.0,\n        loss_weight=1.0,\n        num_classes=2,\n        return_dict=False)\n    loss_cls = build_loss(loss_cls_cfg)\n    fake_pred = torch.Tensor([[100, -100, 100, -100]])\n    fake_label = torch.Tensor([0]).long()\n    loss = loss_cls(fake_pred, fake_label)\n    acc = loss_cls.get_accuracy(fake_pred, fake_label)\n    act = loss_cls.get_activation(fake_pred)\n    assert torch.allclose(loss, torch.tensor(0.))\n    assert torch.allclose(acc['acc_objectness'], torch.tensor(100.))\n    assert torch.allclose(acc['acc_classes'], torch.tensor(100.))\n    assert torch.allclose(act, torch.tensor([1., 0., 0.]))\n\n\ndef test_accuracy():\n    # test for empty pred\n    pred = torch.empty(0, 4)\n    label = torch.empty(0)\n    accuracy = Accuracy(topk=1)\n    acc = accuracy(pred, label)\n    assert acc.item() == 0\n\n    pred = torch.Tensor([[0.2, 0.3, 0.6, 0.5], [0.1, 0.1, 0.2, 0.6],\n                         [0.9, 0.0, 0.0, 0.1], [0.4, 0.7, 0.1, 0.1],\n                         [0.0, 0.0, 0.99, 0]])\n    # test for top1\n    true_label = torch.Tensor([2, 3, 0, 1, 2]).long()\n    accuracy = Accuracy(topk=1)\n    acc = accuracy(pred, true_label)\n    assert acc.item() == 100\n\n    # test for top1 with score thresh=0.8\n    true_label = torch.Tensor([2, 3, 0, 1, 2]).long()\n    accuracy = Accuracy(topk=1, thresh=0.8)\n    acc = accuracy(pred, true_label)\n    assert acc.item() == 40\n\n    # test for top2\n    accuracy = Accuracy(topk=2)\n    label = torch.Tensor([3, 2, 0, 0, 2]).long()\n    acc = accuracy(pred, label)\n    assert acc.item() == 100\n\n    # test for both top1 and top2\n    accuracy = Accuracy(topk=(1, 2))\n    true_label = torch.Tensor([2, 3, 0, 1, 2]).long()\n    acc = accuracy(pred, true_label)\n    for a in acc:\n        assert a.item() == 100\n\n    # topk is larger than pred class number\n    with pytest.raises(AssertionError):\n        accuracy = Accuracy(topk=5)\n        accuracy(pred, true_label)\n\n    # wrong topk type\n    with pytest.raises(AssertionError):\n        accuracy = Accuracy(topk='wrong type')\n        accuracy(pred, true_label)\n\n    # label size is larger than required\n    with pytest.raises(AssertionError):\n        label = torch.Tensor([2, 3, 0, 1, 2, 0]).long()  # size mismatch\n        accuracy = Accuracy()\n        accuracy(pred, label)\n\n    # wrong pred dimension\n    with pytest.raises(AssertionError):\n        accuracy = Accuracy()\n        accuracy(pred[:, :, None], true_label)\n", "hunk": "@@ -30,11 +30,11 @@ def test_ce_loss():\n     loss_cls = build_loss(loss_cls_cfg)\n     assert torch.allclose(loss_cls(fake_pred, fake_label), torch.tensor(200.))\n \n-    # test bce_loss\n-    cls_score = torch.Tensor([[-200, 100], [500, -1000], [300, -300]])\n-    label = torch.Tensor([0, 1, 0]).long()\n-    weight = torch.Tensor([0.6, 0.4, 0.5])\n-    class_weight = torch.tensor([0.1, 0.9])  # class 0: 0.1, class 1: 0.9\n+    # test bce_loss matrix(M, C)\n+    cls_score_M_C = torch.Tensor([[-200, 100], [500, -1000], [300, -300]])\n+    label_M_C = torch.Tensor([0, 1, 0]).long()\n+    weight_M = torch.Tensor([0.6, 0.4, 0.5])  # elemrntwise weight\n+    class_weight_C = torch.tensor([0.1, 0.9])  # class 0: 0.1, class 1: 0.9\n \n     # test bce_loss without class weight\n     loss_cfg = dict(\n", "comment": "`bce_loss` now only supports the input tensor with shape (n, 1).", "ids": ["24947", "6e59bdc4c99dab57627e5dcbdd40cee473896fe4", "4d326d075874e7b7780d01ffe394d3f491d5dcd6"], "repo": "open-mmlab/mmdetection", "ghid": 5776, "old": "     loss_cls = build_loss(loss_cls_cfg)\n     assert torch.allclose(loss_cls(fake_pred, fake_label), torch.tensor(200.))\n-    # test bce_loss\n-    cls_score = torch.Tensor([[-200, 100], [500, -1000], [300, -300]])\n-    label = torch.Tensor([0, 1, 0]).long()\n-    weight = torch.Tensor([0.6, 0.4, 0.5])\n-    class_weight = torch.tensor([0.1, 0.9])  # class 0: 0.1, class 1: 0.9\n     # test bce_loss without class weight\n     loss_cfg = dict(", "new": "     loss_cls = build_loss(loss_cls_cfg)\n     assert torch.allclose(loss_cls(fake_pred, fake_label), torch.tensor(200.))\n+    # test bce_loss matrix(M, C)\n+    cls_score_M_C = torch.Tensor([[-200, 100], [500, -1000], [300, -300]])\n+    label_M_C = torch.Tensor([0, 1, 0]).long()\n+    weight_M = torch.Tensor([0.6, 0.4, 0.5])  # elemrntwise weight\n+    class_weight_C = torch.tensor([0.1, 0.9])  # class 0: 0.1, class 1: 0.9\n     # test bce_loss without class weight\n     loss_cfg = dict(", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -114,9 +115,9 @@ def centered_integer_range(data, lower, upper, center):\n     ndarray = ()\n \n \n-def check_sample(values):\n+def check_sample(values, require_1d_array=True, require_sequence=True):", "oldf": "# coding=utf-8\n#\n# This file is part of Hypothesis, which may be found at\n# https://github.com/HypothesisWorks/hypothesis-python\n#\n# Most of this work is copyright (C) 2013-2018 David R. MacIver\n# (david@drmaciver.com), but it contains contributions by others. See\n# CONTRIBUTING.rst for a full list of people who may hold copyright, and\n# consult the git log if you need to determine who owns an individual\n# contribution.\n#\n# This Source Code Form is subject to the terms of the Mozilla Public License,\n# v. 2.0. If a copy of the MPL was not distributed with this file, You can\n# obtain one at http://mozilla.org/MPL/2.0/.\n#\n# END HEADER\n\nfrom __future__ import division, print_function, absolute_import\n\nimport enum\nimport math\nimport heapq\nimport hashlib\nfrom fractions import Fraction\nfrom collections import Sequence, OrderedDict\n\nfrom hypothesis._settings import note_deprecation\nfrom hypothesis.internal.compat import floor, hbytes, hrange, qualname, \\\n    bit_length, str_to_bytes, int_from_bytes\nfrom hypothesis.internal.floats import int_to_float\n\nLABEL_MASK = 2 ** 64 - 1\n_SEQUENCE_TYPES = (OrderedDict, Sequence, enum.EnumMeta)\n\n\ndef calc_label_from_name(name):\n    hashed = hashlib.md5(str_to_bytes(name)).digest()\n    return int_from_bytes(hashed[:8])\n\n\ndef calc_label_from_cls(cls):\n    return calc_label_from_name(qualname(cls))\n\n\ndef combine_labels(*labels):\n    label = 0\n    for l in labels:\n        label = (label << 1) & LABEL_MASK\n        label ^= l\n    return label\n\n\nINTEGER_RANGE_DRAW_LABEL = calc_label_from_name(\n    'another draw in integer_range()')\nGEOMETRIC_LABEL = calc_label_from_name('geometric()')\nBIASED_COIN_LABEL = calc_label_from_name('biased_coin()')\nSAMPLE_IN_SAMPLER_LABLE = calc_label_from_name('a sample() in Sampler')\nONE_FROM_MANY_LABEL = calc_label_from_name('one more from many()')\n\n\ndef integer_range(data, lower, upper, center=None):\n    assert lower <= upper\n    if lower == upper:\n        # Write a value even when this is trival so that when a bound depends\n        # on other values we don't suddenly disappear when the gap shrinks to\n        # zero - if that happens then often the data stream becomes misaligned\n        # and we fail to shrink in cases where we really should be able to.\n        data.write(hbytes([0]))\n        return int(lower)\n\n    if center is None:\n        center = lower\n    center = min(max(center, lower), upper)\n\n    if center == upper:\n        above = False\n    elif center == lower:\n        above = True\n    else:\n        above = boolean(data)\n\n    if above:\n        gap = upper - center\n    else:\n        gap = center - lower\n\n    assert gap > 0\n\n    bits = bit_length(gap)\n    probe = gap + 1\n\n    while probe > gap:\n        data.start_example(INTEGER_RANGE_DRAW_LABEL)\n        probe = data.draw_bits(bits)\n        data.stop_example(discard=probe > gap)\n\n    if above:\n        result = center + probe\n    else:\n        result = center - probe\n\n    assert lower <= result <= upper\n    return int(result)\n\n\ndef centered_integer_range(data, lower, upper, center):\n    return integer_range(\n        data, lower, upper, center=center\n    )\n\n\ntry:\n    from numpy import ndarray\nexcept ImportError:  # pragma: no cover\n    ndarray = ()\n\n\ndef check_sample(values, require_1d_array=True, require_sequence=True):\n    if isinstance(values, ndarray):\n        if require_1d_array and values.ndim != 1:\n            note_deprecation((\n                'Only one-dimensional arrays are supported for sampling, '\n                'and the given value has {ndim} dimensions (shape '\n                '{shape}).  This array would give samples of array slices '\n                'instead of elements!  Use np.ravel(values) to convert '\n                'to a one-dimensional array, or tuple(values) if you '\n                'want to sample slices.  Sampling a multi-dimensional '\n                'array will be an error in a future version of Hypothesis.'\n            ).format(ndim=values.ndim, shape=values.shape))\n    elif require_sequence and not isinstance(values, _SEQUENCE_TYPES):\n        note_deprecation(\n            ('Cannot sample from %r, not a sequence.  ' % (values,)) +\n            'Hypothesis goes to some length to ensure that sampling an '\n            'element from a collection (with `sampled_from`, `choices`, or '\n            '`permutations`) is replayable and can be minimised.  To  '\n            'replay a saved example, the sampled values must have the same '\n            'iteration order on every run - ruling out sets, dicts, etc due '\n            'to hash randomisation. Most cases can simply use '\n            '`sorted(values)`, but mixed types or special values such as '\n            'math.nan require careful handling - and  note that when '\n            'simplifying an example, Hypothesis treats earlier values as '\n            'simpler.')\n    return tuple(values)\n\n\ndef choice(data, values):\n    return values[integer_range(data, 0, len(values) - 1)]\n\n\ndef getrandbits(data, n):\n    n_bytes = n // 8\n    if n % 8 != 0:\n        n_bytes += 1\n    return int_from_bytes(data.draw_bytes(n_bytes)) & ((1 << n) - 1)\n\n\nFLOAT_PREFIX = 0b1111111111 << 52\nFULL_FLOAT = int_to_float(FLOAT_PREFIX | ((2 << 53) - 1)) - 1\n\n\ndef fractional_float(data):\n    return (\n        int_to_float(FLOAT_PREFIX | getrandbits(data, 52)) - 1\n    ) / FULL_FLOAT\n\n\ndef geometric(data, p):\n    denom = math.log1p(-p)\n    data.start_example(GEOMETRIC_LABEL)\n    while True:\n        probe = fractional_float(data)\n        if probe < 1.0:\n            result = int(math.log1p(-probe) / denom)\n            assert result >= 0, (probe, p, result)\n            data.stop_example()\n            return result\n\n\ndef boolean(data):\n    return bool(data.draw_bits(1))\n\n\ndef biased_coin(data, p):\n    \"\"\"Return False with probability p (assuming a uniform generator),\n    shrinking towards False.\"\"\"\n    data.start_example(BIASED_COIN_LABEL)\n    while True:\n        # The logic here is a bit complicated and special cased to make it\n        # play better with the shrinker.\n\n        # We imagine partitioning the real interval [0, 1] into 256 equal parts\n        # and looking at each part and whether its interior is wholly <= p\n        # or wholly >= p. At most one part can be neither.\n\n        # We then pick a random part. If it's wholly on one side or the other\n        # of p then we use that as the answer. If p is contained in the\n        # interval then we start again with a new probability that is given\n        # by the fraction of that interval that was <= our previous p.\n\n        # We then take advantage of the fact that we have control of the\n        # labelling to make this shrink better, using the following tricks:\n\n        # If p is <= 0 or >= 1 the result of this coin is certain. We make sure\n        # to write a byte to the data stream anyway so that these don't cause\n        # difficulties when shrinking.\n        if p <= 0:\n            data.write(hbytes([0]))\n            result = False\n        elif p >= 1:\n            data.write(hbytes([1]))\n            result = True\n        else:\n            falsey = floor(256 * (1 - p))\n            truthy = floor(256 * p)\n            remainder = 256 * p - truthy\n\n            if falsey + truthy == 256:\n                if isinstance(p, Fraction):\n                    m = p.numerator\n                    n = p.denominator\n                else:\n                    m, n = p.as_integer_ratio()\n                assert n & (n - 1) == 0, n  # n is a power of 2\n                assert n > m > 0\n                truthy = m\n                falsey = n - m\n                bits = bit_length(n) - 1\n                partial = False\n            else:\n                bits = 8\n                partial = True\n\n            i = data.draw_bits(bits)\n\n            # We always label the region that causes us to repeat the loop as\n            # 255 so that shrinking this byte never causes us to need to draw\n            # more data.\n            if partial and i == 255:\n                p = remainder\n                continue\n            if falsey == 0:\n                # Every other partition is truthy, so the result is true\n                result = True\n            elif truthy == 0:\n                # Every other partition is falsey, so the result is false\n                result = False\n            elif i <= 1:\n                # We special case so that zero is always false and 1 is always\n                # true which makes shrinking easier because we can always\n                # replace a truthy block with 1. This has the slightly weird\n                # property that shrinking from 2 to 1 can cause the result to\n                # grow, but the shrinker always tries 0 and 1 first anyway, so\n                # this will usually be fine.\n                result = bool(i)\n            else:\n                # Originally everything in the region 0 <= i < falsey was false\n                # and everything above was true. We swapped one truthy element\n                # into this region, so the region becomes 0 <= i <= falsey\n                # except for i = 1. We know i > 1 here, so the test for truth\n                # becomes i > falsey.\n                result = i > falsey\n        break\n    data.stop_example()\n    return result\n\n\nclass Sampler(object):\n    \"\"\"Sampler based on Vose's algorithm for the alias method. See\n    http://www.keithschwarz.com/darts-dice-coins/ for a good explanation.\n\n    The general idea is that we store a table of triples (base, alternate, p).\n    base. We then pick a triple uniformly at random, and choose its alternate\n    value with probability p and else choose its base value. The triples are\n    chosen so that the resulting mixture has the right distribution.\n\n    We maintain the following invariants to try to produce good shrinks:\n\n    1. The table is in lexicographic (base, alternate) order, so that choosing\n       an earlier value in the list always lowers (or at least leaves\n       unchanged) the value.\n    2. base[i] < alternate[i], so that shrinking the draw always results in\n       shrinking the chosen element.\n    \"\"\"\n\n    def __init__(self, weights):\n\n        n = len(weights)\n\n        self.table = [[i, None, None] for i in hrange(n)]\n\n        total = sum(weights)\n\n        num_type = type(total)\n\n        zero = num_type(0)\n        one = num_type(1)\n\n        small = []\n        large = []\n\n        probabilities = [w / total for w in weights]\n        scaled_probabilities = []\n\n        for i, p in enumerate(probabilities):\n            scaled = p * n\n            scaled_probabilities.append(scaled)\n            if scaled == 1:\n                self.table[i][2] = zero\n            elif scaled < 1:\n                small.append(i)\n            else:\n                large.append(i)\n        heapq.heapify(small)\n        heapq.heapify(large)\n\n        while small and large:\n            lo = heapq.heappop(small)\n            hi = heapq.heappop(large)\n\n            assert lo != hi\n            assert scaled_probabilities[hi] > one\n            assert self.table[lo][1] is None\n            self.table[lo][1] = hi\n            self.table[lo][2] = one - scaled_probabilities[lo]\n            scaled_probabilities[hi] = (\n                scaled_probabilities[hi] + scaled_probabilities[lo]) - one\n\n            if scaled_probabilities[hi] < 1:\n                heapq.heappush(small, hi)\n            elif scaled_probabilities[hi] == 1:\n                self.table[hi][2] = zero\n            else:\n                heapq.heappush(large, hi)\n        while large:\n            self.table[large.pop()][2] = zero\n        while small:\n            self.table[small.pop()][2] = zero\n\n        for entry in self.table:\n            assert entry[2] is not None\n            if entry[1] is None:\n                entry[1] = entry[0]\n            elif entry[1] < entry[0]:\n                entry[0], entry[1] = entry[1], entry[0]\n                entry[2] = one - entry[2]\n        self.table.sort()\n\n    def sample(self, data):\n        data.start_example(SAMPLE_IN_SAMPLER_LABLE)\n        i = integer_range(data, 0, len(self.table) - 1)\n        base, alternate, alternate_chance = self.table[i]\n        use_alternate = biased_coin(data, alternate_chance)\n        data.stop_example()\n        if use_alternate:\n            return alternate\n        else:\n            return base\n\n\nclass many(object):\n    \"\"\"Utility class for collections. Bundles up the logic we use for \"should I\n    keep drawing more values?\" and handles starting and stopping examples in\n    the right place.\n\n    Intended usage is something like:\n\n    elements = many(data, ...)\n    while elements.more():\n        add_stuff_to_result()\n    \"\"\"\n\n    def __init__(self, data, min_size, max_size, average_size):\n        assert 0 <= min_size <= average_size <= max_size\n        self.min_size = min_size\n        self.max_size = max_size\n        self.data = data\n        self.stopping_value = 1 - 1.0 / (1 + average_size)\n        self.count = 0\n        self.rejections = 0\n        self.drawn = False\n        self.force_stop = False\n        self.rejected = False\n\n    def more(self):\n        \"\"\"Should I draw another element to add to the collection?\"\"\"\n        if self.drawn:\n            self.data.stop_example(discard=self.rejected)\n\n        self.drawn = True\n        self.rejected = False\n\n        if self.min_size == self.max_size:\n            should_continue = self.count < self.min_size\n        elif self.force_stop:\n            should_continue = False\n        else:\n            if self.count < self.min_size:\n                p_continue = 1.0\n            elif self.count >= self.max_size:\n                p_continue = 0.0\n            else:\n                p_continue = self.stopping_value\n            should_continue = biased_coin(self.data, p_continue)\n\n        if should_continue:\n            self.data.start_example(ONE_FROM_MANY_LABEL)\n            self.count += 1\n            return True\n        else:\n            return False\n\n    def reject(self):\n        \"\"\"Reject the last example (i.e. don't count it towards our budget of\n        elements because it's not going to go in the final collection).\"\"\"\n        assert self.count > 0\n        self.count -= 1\n        self.rejections += 1\n        self.rejected = True\n        if self.rejections > 2 * self.count:\n            if self.count < self.min_size:\n                self.data.mark_invalid()\n            else:\n                self.force_stop = True\n", "hunk": "@@ -115,9 +115,9 @@ except ImportError:  # pragma: no cover\n     ndarray = ()\n \n \n-def check_sample(values, require_1d_array=True, require_sequence=True):\n+def check_sample(values, strategy_name):\n     if isinstance(values, ndarray):\n-        if require_1d_array and values.ndim != 1:\n+        if values.ndim != 1:\n             note_deprecation((\n                 'Only one-dimensional arrays are supported for sampling, '\n                 'and the given value has {ndim} dimensions (shape '\n", "comment": "These new arguments are not needed - the function should always have the same (existing) requirements for the input.", "ids": ["14937", "5572ec503cf772bcef54ace40b6dab3b552f8c9d", "8ea1fd76c6518a2cb4dbbcadfe6ec797b13544e0"], "repo": "HypothesisWorks/hypothesis", "ghid": 1287, "old": "     ndarray = ()\n-def check_sample(values, require_1d_array=True, require_sequence=True):\n     if isinstance(values, ndarray):\n-        if require_1d_array and values.ndim != 1:\n             note_deprecation((\n                 'Only one-dimensional arrays are supported for sampling, '\n                 'and the given value has {ndim} dimensions (shape '", "new": "     ndarray = ()\n+def check_sample(values, strategy_name):\n     if isinstance(values, ndarray):\n+        if values.ndim != 1:\n             note_deprecation((\n                 'Only one-dimensional arrays are supported for sampling, '\n                 'and the given value has {ndim} dimensions (shape '", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -498,6 +498,21 @@ def run_testcase_and_return_result_in_queue(crash_queue,\n                    'run_testcase_and_return_result_in_queue.')\n \n \n+def engine_reproduce(engine_impl, target_name, testcase_path, arguments,\n+                     timeout):\n+  \"\"\"Do engine reproduction.\"\"\"\n+  if environment.is_trusted_host():\n+    from bot.untrusted_runner import tasks_host\n+    return tasks_host.engine_reproduce(engine_impl, target_name, testcase_path,\n+                                       arguments, timeout)\n+  build_dir = environment.get_value('BUILD_DIR')\n+  target_path = engine_common.find_fuzzer_path(build_dir, target_name)\n+  if not target_path:\n+    raise TargetNotFoundError('Failed to find target ' + target_name)", "oldf": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Functions for testcase management.\"\"\"\n\nfrom builtins import object\nfrom builtins import range\nimport base64\nimport collections\nimport datetime\nimport os\nimport re\nimport zlib\n\nfrom base import utils\nfrom bot.fuzzers import engine\nfrom bot.fuzzers import engine_common\nfrom build_management import revisions\nfrom crash_analysis.crash_comparer import CrashComparer\nfrom crash_analysis.crash_result import CrashResult\nfrom datastore import data_handler\nfrom datastore import data_types\nfrom metrics import fuzzer_logs\nfrom metrics import fuzzer_stats\nfrom metrics import logs\nfrom platforms import android\nfrom system import archive\nfrom system import environment\nfrom system import process_handler\nfrom system import shell\n\n# Testcase filename prefixes and suffixes.\nCRASH_PREFIX = 'crash-'\nFUZZ_PREFIX = 'fuzz-'\nFLAGS_PREFIX = 'flags-'\nHTTP_PREFIX = 'http-'\nRESOURCES_PREFIX = 'resources-'\n\n# TODO(mbarbella): Once all fuzzers are converted to \"resources-\", remove this.\nDEPENDENCY_PREFIX = 'cfdependency-'\nAPPS_PREFIX = 'fuzz-apps-'\nEXTENSIONS_PREFIX = 'fuzz-extension-'\nCOVERAGE_SUFFIX = '.cov'\n\nINFO_FILE_EXTENSION = '.info'\nIPCDUMP_EXTENSION = '.ipcdump'\nREPRODUCIBILITY_FACTOR = 0.5\nSEARCH_INDEX_TESTCASES_DIRNAME = 'common'\nSEARCH_INDEX_BUNDLE_PREFIX = '__%s_' % SEARCH_INDEX_TESTCASES_DIRNAME\nTESTCASE_LIST_FILENAME = 'files.info'\n\nNETWORK_DELETEGATE_URL_REGEX = re.compile(\n    r'.*NetworkDelegate::NotifyBeforeURLRequest:\\s+([^\\s]+)')\nFILE_URL_REGEX = re.compile(r'file:///([^\"#?]+)')\nHTTP_URL_REGEX = re.compile(\n    r'.*(localhost|\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})[^/]*[/]([^\"#?]+)')\n\nBAD_STATE_HINTS = [\n    # X server issues.\n    'cannot open display',\n    'Maximum number of clients reached',\n    'Missing X server',\n\n    # Android logging issues.\n    'logging service has stopped',\n]\n\n\nclass TestcaseManagerError(Exception):\n  \"\"\"Base exception.\"\"\"\n\n\nclass TargetNotFoundError(TestcaseManagerError):\n  \"\"\"Error when a fuzz target is not found.\"\"\"\n\n\ndef create_testcase_list_file(output_directory):\n  \"\"\"Create a testcase list file for tests in a directory.\"\"\"\n  files_list = []\n  files_list_file_path = os.path.join(output_directory, TESTCASE_LIST_FILENAME)\n  for root, _, files in os.walk(output_directory):\n    for filename in files:\n      if filename.endswith(INFO_FILE_EXTENSION):\n        # Skip an info file.\n        continue\n\n      file_path = os.path.join(root, filename)\n      if not utils.is_valid_testcase_file(file_path, check_if_exists=False):\n        continue\n\n      normalized_relative_file_path = utils.get_normalized_relative_path(\n          file_path, output_directory)\n      files_list.append(normalized_relative_file_path)\n\n  utils.write_data_to_file('\\n'.join(sorted(files_list)), files_list_file_path)\n\n\ndef get_testcases_from_directories(directories):\n  \"\"\"Returns all testcases from testcase directories.\"\"\"\n  testcase_paths = []\n  max_testcases = environment.get_value('MAX_TESTCASES')\n\n  generators = []\n  for directory in directories:\n    if not directory.strip():\n      continue\n\n    generators.append(os.walk(directory))\n\n  for generator in generators:\n    for structure in generator:\n      base_directory = structure[0]\n      for filename in structure[2]:\n        if not filename.startswith(FUZZ_PREFIX):\n          continue\n\n        if filename.endswith(COVERAGE_SUFFIX):\n          continue\n\n        file_path = os.path.join(base_directory, filename)\n        if not os.path.getsize(file_path):\n          continue\n\n        testcase_paths.append(utils.normalize_path(file_path))\n        if len(testcase_paths) == max_testcases:\n          return testcase_paths\n\n  return testcase_paths\n\n\ndef is_testcase_resource(filename):\n  \"\"\"Returns true if this is a testcase or its resource dependency.\"\"\"\n  if filename.startswith(FUZZ_PREFIX):\n    return True\n\n  if filename.startswith(FLAGS_PREFIX):\n    return True\n\n  if filename.startswith(DEPENDENCY_PREFIX):\n    return True\n\n  if filename.startswith(RESOURCES_PREFIX):\n    return True\n\n  if filename.endswith(COVERAGE_SUFFIX):\n    return True\n\n  return False\n\n\ndef remove_testcases_from_directories(directories):\n  \"\"\"Removes all testcases and their dependencies from testcase directories.\"\"\"\n  generators = []\n  for directory in directories:\n    if not directory.strip():\n      continue\n\n    # If there is a bot-specific files list, delete it now.\n    bot_testcases_file_path = utils.get_bot_testcases_file_path(directory)\n    shell.remove_file(bot_testcases_file_path)\n\n    generators.append(os.walk(directory))\n\n  for generator in generators:\n    for structure in generator:\n      base_directory = structure[0]\n      for filename in structure[2]:\n        if not is_testcase_resource(filename):\n          continue\n\n        if filename.startswith(RESOURCES_PREFIX):\n          # In addition to removing this file, remove all resources.\n          resources_file_path = os.path.join(base_directory, filename)\n          resources = read_resource_list(resources_file_path)\n          for resource in resources:\n            shell.remove_file(resource)\n\n        file_path = os.path.join(base_directory, filename)\n        shell.remove_file(file_path)\n\n\ndef read_resource_list(resource_file_path):\n  \"\"\"Generate a resource list.\"\"\"\n  if not os.path.exists(resource_file_path):\n    return []\n\n  resources = []\n  base_directory = os.path.dirname(resource_file_path)\n  with open(resource_file_path) as file_handle:\n    resource_file_contents = file_handle.read()\n    for line in resource_file_contents.splitlines():\n      resource = os.path.join(base_directory, line.strip())\n      if not os.path.exists(resource):\n        break\n\n      resources.append(resource)\n\n  return resources\n\n\ndef get_resource_dependencies(testcase_absolute_path, test_prefix=FUZZ_PREFIX):\n  \"\"\"Returns the list of testcase resource dependencies.\"\"\"\n  resources = []\n  if not os.path.exists(testcase_absolute_path):\n    return resources\n\n  base_directory = os.path.dirname(testcase_absolute_path)\n  testcase_filename = os.path.basename(testcase_absolute_path)\n\n  # FIXME(mbarbella): Remove this when all fuzzers are using \"resources-\".\n  # This code includes the dependencies that begin with\n  # dependency prefix and are referenced in the testcase.\n  testcase_contents = None\n  for filename in os.listdir(base_directory):\n    if filename.startswith(DEPENDENCY_PREFIX):\n      # Only load the testcase contents if necessary.\n      if not testcase_contents:\n        file_handle = open(testcase_absolute_path, 'rb')\n        testcase_contents = file_handle.read()\n        file_handle.close()\n\n      if filename in testcase_contents:\n        file_path = os.path.join(base_directory, filename)\n        resources.append(file_path)\n\n  # This code includes the dependencies in cases when the testcase itself is a\n  # just a wrapper file around the actual testcase.\n  if DEPENDENCY_PREFIX in testcase_absolute_path:\n    dependency_filename = os.path.splitext(testcase_filename)[0]\n    dependency_filename = re.compile(DEPENDENCY_PREFIX).sub(\n        '', dependency_filename, 1)\n    dependency_filename = re.compile(FUZZ_PREFIX).sub('', dependency_filename,\n                                                      1)\n    dependency_filename = re.compile(HTTP_PREFIX).sub('', dependency_filename,\n                                                      1)\n    dependency_file_path = os.path.join(base_directory, dependency_filename)\n    resources.append(dependency_file_path)\n\n  # Check to see if this test case lists all resources in a resources file.\n  if testcase_filename.startswith(test_prefix):\n    stripped_testcase_name = testcase_filename[len(test_prefix):]\n    resources_filename = '%s%s' % (RESOURCES_PREFIX, stripped_testcase_name)\n    resources_file_path = os.path.join(base_directory, resources_filename)\n    resources += read_resource_list(resources_file_path)\n\n  # For extensions, archive everything in the extension directory.\n  if APPS_PREFIX in testcase_filename or EXTENSIONS_PREFIX in testcase_filename:\n    for root, _, files in os.walk(base_directory):\n      for filename in files:\n        file_path = os.path.join(root, filename)\n        if file_path == testcase_absolute_path:\n          continue\n\n        resources.append(file_path)\n\n  return resources\n\n\ndef get_additional_command_line_flags(testcase_path):\n  \"\"\"Returns additional command line flags to use for a testcase.\"\"\"\n  # Get the initial flags list from the environment value.\n  additional_command_line_flags = (\n      environment.get_value('ADDITIONAL_COMMAND_LINE_FLAGS', ''))\n\n  # If we don't have a fuzz prefix, no need to look further for flags file.\n  testcase_filename = os.path.basename(testcase_path)\n  if not testcase_filename.startswith(FUZZ_PREFIX):\n    return additional_command_line_flags\n\n  # Gets the flags list from the flags file.\n  stripped_testcase_name = testcase_filename[len(FUZZ_PREFIX):]\n  flags_filename = '%s%s' % (FLAGS_PREFIX, stripped_testcase_name)\n  flags_file_path = os.path.join(os.path.dirname(testcase_path), flags_filename)\n  flags_file_content = utils.read_data_from_file(\n      flags_file_path, eval_data=False)\n  if flags_file_content:\n    additional_command_line_flags += ' %s' % flags_file_content\n  return additional_command_line_flags.strip()\n\n\ndef run_testcase(thread_index, file_path, gestures, env_copy):\n  \"\"\"Run a single testcase and return crash results in the crash queue.\"\"\"\n  try:\n    # Update environment with environment copy from parent.\n    if env_copy:\n      os.environ.update(env_copy)\n\n    # Initialize variables.\n    needs_http = '-http-' in file_path\n    test_timeout = environment.get_value('TEST_TIMEOUT', 10)\n    app_directory = environment.get_value('APP_DIR')\n    environment.set_value('PIDS', '[]')\n\n    # Get command line options.\n    command = get_command_line_for_application(\n        file_path, user_profile_index=thread_index, needs_http=needs_http)\n\n    # Run testcase.\n    return process_handler.run_process(\n        command,\n        timeout=test_timeout,\n        gestures=gestures,\n        env_copy=env_copy,\n        current_working_directory=app_directory)\n  except Exception:\n    logs.log_error('Exception occurred while running run_testcase.')\n\n    return None, None, None\n\n\nclass Crash(\n    collections.namedtuple(\n        'Crash', 'file_path crash_time return_code resource_list gestures '\n        'stack_file_path')):\n  \"\"\"Represents a crash in a queue. This class is transformed into\n    fuzz_task.Crash. Therefore, please be careful when adding/removing\n    fields.\"\"\"\n\n\ndef get_resource_paths(output):\n  \"\"\"Read the urls from the output.\"\"\"\n  resource_paths = set()\n  for line in output.splitlines():\n    match = NETWORK_DELETEGATE_URL_REGEX.match(line)\n    if not match:\n      continue\n\n    local_path = convert_dependency_url_to_local_path(match.group(1))\n    if local_path:\n      logs.log('Detected resource: %s.' % local_path)\n      resource_paths.add(local_path)\n\n  return list(resource_paths)\n\n\ndef convert_dependency_url_to_local_path(url):\n  \"\"\"Convert a dependency URL to a corresponding local path.\"\"\"\n  # Bot-specific import.\n  from bot.webserver import http_server\n\n  logs.log('Process dependency: %s.' % url)\n  file_match = FILE_URL_REGEX.search(url)\n  http_match = HTTP_URL_REGEX.search(url)\n  platform = environment.platform()\n\n  local_path = None\n  if file_match:\n    file_path = file_match.group(1)\n    logs.log('Detected file dependency: %s.' % file_path)\n    if platform == 'WINDOWS':\n      local_path = file_path\n    else:\n      local_path = '/' + file_path\n\n      # Convert remote to local path for android.\n      if platform == 'ANDROID':\n        remote_testcases_directory = android.constants.DEVICE_TESTCASES_DIR\n        local_testcases_directory = environment.get_value('FUZZ_INPUTS')\n        local_path = local_path.replace(remote_testcases_directory,\n                                        local_testcases_directory)\n\n  elif http_match:\n    relative_http_path = os.path.sep + http_match.group(2)\n    logs.log('Detected http dependency: %s.' % relative_http_path)\n    local_path = http_server.get_absolute_testcase_file(relative_http_path)\n    if not local_path:\n      # This needs to be a warning since in many cases, it is actually a\n      # non-existent path. For others, we need to add the directory aliases in\n      # file http_server.py.\n      logs.log_warn(\n          'Unable to find server resource %s, skipping.' % relative_http_path)\n\n  if local_path:\n    local_path = utils.normalize_path(local_path)\n\n  return local_path\n\n\ndef _get_testcase_time(testcase_path):\n  \"\"\"Returns the timestamp of a testcase.\"\"\"\n  stats = fuzzer_stats.TestcaseRun.read_from_disk(testcase_path)\n  if stats:\n    return datetime.datetime.utcfromtimestamp(float(stats.timestamp))\n\n  return None\n\n\ndef upload_testcase(testcase_path, log_time):\n  \"\"\"Uploads testcase so that a log file can be matched with it folder.\"\"\"\n  fuzz_logs_bucket = environment.get_value('FUZZ_LOGS_BUCKET')\n  if not fuzz_logs_bucket:\n    return\n\n  with open(testcase_path, 'rb') as file_handle:\n    testcase_contents = file_handle.read()\n\n  fuzzer_logs.upload_to_logs(\n      fuzz_logs_bucket,\n      testcase_contents,\n      time=log_time,\n      file_extension='.testcase')\n\n\ndef _get_crash_output(output):\n  \"\"\"Returns crash part of the output, excluding unrelated content (e.g. output\n  from corpus merge, etc).\"\"\"\n  if output is None:\n    return None\n\n  crash_stacktrace_end_marker_index = output.find(\n      data_types.CRASH_STACKTRACE_END_MARKER)\n  if crash_stacktrace_end_marker_index == -1:\n    return output\n\n  return output[:crash_stacktrace_end_marker_index]\n\n\ndef run_testcase_and_return_result_in_queue(crash_queue,\n                                            thread_index,\n                                            file_path,\n                                            gestures,\n                                            env_copy,\n                                            upload_output=False):\n  \"\"\"Run a single testcase and return crash results in the crash queue.\"\"\"\n\n  # Since this is running in its own process, initialize the log handler again.\n  # This is needed for Windows where instances are not shared across child\n  # processes. See:\n  # https://stackoverflow.com/questions/34724643/python-logging-with-multiprocessing-root-logger-different-in-windows\n  logs.configure('run_testcase', {\n      'testcase_path': file_path,\n  })\n\n  try:\n    # Run testcase and check whether a crash occurred or not.\n    return_code, crash_time, output = run_testcase(thread_index, file_path,\n                                                   gestures, env_copy)\n\n    # Pull testcase directory to host to get any stats files.\n    if environment.is_trusted_host():\n      from bot.untrusted_runner import file_host\n      file_host.pull_testcases_from_worker()\n\n    # Analyze the crash.\n    crash_output = _get_crash_output(output)\n    crash_result = CrashResult(return_code, crash_time, crash_output)\n\n    # To provide consistency between stats and logs, we use timestamp taken\n    # from stats when uploading logs and testcase.\n    if upload_output:\n      log_time = _get_testcase_time(file_path)\n\n    if crash_result.is_crash():\n      # Initialize resource list with the testcase path.\n      resource_list = [file_path]\n      resource_list += get_resource_paths(crash_output)\n\n      # Store the crash stack file in the crash stacktrace directory\n      # with filename as the hash of the testcase path.\n      crash_stacks_directory = environment.get_value('CRASH_STACKTRACES_DIR')\n      stack_file_path = os.path.join(crash_stacks_directory,\n                                     utils.string_hash(file_path))\n      utils.write_data_to_file(crash_output, stack_file_path)\n\n      # Put crash/no-crash results in the crash queue.\n      crash_queue.put(\n          Crash(\n              file_path=file_path,\n              crash_time=crash_time,\n              return_code=return_code,\n              resource_list=resource_list,\n              gestures=gestures,\n              stack_file_path=stack_file_path))\n\n      # Don't upload uninteresting testcases (no crash) or if there is no log to\n      # correlate it with (not upload_output).\n      if upload_output:\n        upload_testcase(file_path, log_time)\n\n    if upload_output:\n      # Include full output for uploaded logs (crash output, merge output, etc).\n      crash_result_full = CrashResult(return_code, crash_time, output)\n      log = prepare_log_for_upload(crash_result_full.get_stacktrace(),\n                                   return_code)\n      upload_log(log, log_time)\n  except Exception:\n    logs.log_error('Exception occurred while running '\n                   'run_testcase_and_return_result_in_queue.')\n\n\ndef engine_reproduce(engine_impl, target_name, testcase_path, arguments,\n                     timeout):\n  \"\"\"Do engine reproduction.\"\"\"\n  if environment.is_trusted_host():\n    from bot.untrusted_runner import tasks_host\n    return tasks_host.engine_reproduce(engine_impl, target_name, testcase_path,\n                                       arguments, timeout)\n  build_dir = environment.get_value('BUILD_DIR')\n  target_path = engine_common.find_fuzzer_path(build_dir, target_name)\n  if not target_path:\n    raise TargetNotFoundError('Failed to find target ' + target_name)\n\n  return engine_impl.reproduce(target_path, testcase_path, arguments, timeout)\n\n\nclass TestcaseRunner(object):\n  \"\"\"Testcase runner.\"\"\"\n\n  def __init__(self,\n               fuzzer_name,\n               testcase_path,\n               test_timeout,\n               gestures,\n               needs_http=False):\n    self._testcase_path = testcase_path\n    self._test_timeout = test_timeout\n    self._gestures = gestures\n    self._needs_http = needs_http\n\n    fuzz_target = data_handler.get_fuzz_target(fuzzer_name)\n    if fuzz_target:\n      engine_impl = engine.get(fuzz_target.engine)\n    else:\n      engine_impl = None\n\n    # TODO(ochang): Make this hard fail once migration to new fuzzing pipeline\n    # is complete.\n    if fuzz_target and engine_impl:\n      self._is_black_box = False\n      self._engine_impl = engine_impl\n\n      # Read target_name + args from flags file.\n      arguments = get_additional_command_line_flags(testcase_path)\n      arguments = data_handler.filter_arguments(arguments, fuzz_target.binary)\n      self._arguments = arguments.split()\n\n      self._fuzz_target = fuzz_target\n    else:\n      self._is_black_box = True\n      self._command = get_command_line_for_application(\n          testcase_path, needs_http=needs_http)\n\n  def run(self, round_number):\n    \"\"\"Run the testcase once.\"\"\"\n    app_directory = environment.get_value('APP_DIR')\n    warmup_timeout = environment.get_value('WARMUP_TIMEOUT')\n    run_timeout = warmup_timeout if round_number == 1 else self._test_timeout\n\n    if self._is_black_box:\n      return_code, crash_time, output = process_handler.run_process(\n          self._command,\n          timeout=run_timeout,\n          gestures=self._gestures,\n          current_working_directory=app_directory)\n    else:\n      result = engine_reproduce(self._engine_impl, self._fuzz_target.binary,\n                                self._testcase_path, self._arguments,\n                                run_timeout)\n      return_code = result.return_code\n      crash_time = result.time_executed\n\n      log_header = engine_common.get_log_header(\n          result.command, environment.get_value('BOT_NAME'),\n          result.time_executed)\n      output = log_header + '\\n' + result.output\n\n    process_handler.terminate_stale_application_instances()\n\n    crash_result = CrashResult(return_code, crash_time, output)\n    if not crash_result.is_crash():\n      logs.log(\n          'No crash occurred (round {round_number}).'.format(\n              round_number=round_number),\n          output=output)\n\n    return crash_result\n\n  def _pre_run_cleanup(self):\n    \"\"\"Common cleanup before running a testcase.\"\"\"\n    # Cleanup any existing application instances and user profile directories.\n    # Cleaning up temp user profile directories. Should be done before calling\n    # |get_command_line_for_application| call since that creates dependencies in\n    # the profile folder.\n    process_handler.terminate_stale_application_instances()\n    shell.clear_temp_directory()\n\n  def _get_crash_state(self, round_number, crash_result):\n    \"\"\"Get crash state from a CrashResult.\"\"\"\n    state = crash_result.get_symbolized_data()\n    logs.log(\n        ('Crash occurred in {crash_time} seconds (round {round_number}). '\n         'State:\\n{crash_state}').format(\n             crash_time=crash_result.crash_time,\n             round_number=round_number,\n             crash_state=state.crash_state),\n        output=state.crash_stacktrace)\n\n    return state\n\n  def reproduce_with_retries(self,\n                             retries,\n                             expected_state=None,\n                             expected_security_flag=None,\n                             flaky_stacktrace=False):\n    \"\"\"Try reproducing a crash with retries.\"\"\"\n    self._pre_run_cleanup()\n    crash_result = None\n\n    for round_number in range(1, retries + 1):\n      crash_result = self.run(round_number)\n      state = self._get_crash_state(round_number, crash_result)\n\n      if not expected_state:\n        logs.log('Crash stacktrace comparison skipped.')\n        return crash_result\n\n      if crash_result.should_ignore():\n        logs.log('Crash stacktrace matched ignore signatures, ignored.')\n        continue\n\n      if crash_result.is_security_issue() != expected_security_flag:\n        logs.log('Crash security flag does not match, ignored.')\n        continue\n\n      if flaky_stacktrace:\n        logs.log('Crash stacktrace is marked flaky, skipping comparison.')\n        return crash_result\n\n      crash_comparer = CrashComparer(state.crash_state, expected_state)\n      if crash_comparer.is_similar():\n        logs.log('Crash stacktrace is similar to original stacktrace.')\n        return crash_result\n      else:\n        logs.log('Crash stacktrace does not match original stacktrace.')\n\n    logs.log('Didn\\'t crash at all.')\n    return CrashResult(return_code=0, crash_time=0, output=crash_result.output)\n\n  def test_reproduce_reliability(self, retries, expected_state,\n                                 expected_security_flag):\n    \"\"\"Test to see if a crash is fully reproducible or is a one-time crasher.\"\"\"\n    self._pre_run_cleanup()\n\n    reproducible_crash_target_count = retries * REPRODUCIBILITY_FACTOR\n    round_number = 0\n    crash_count = 0\n    for round_number in range(1, retries + 1):\n      # Bail out early if there is no hope of finding a reproducible crash.\n      if (retries - round_number + crash_count + 1 <\n          reproducible_crash_target_count):\n        break\n\n      crash_result = self.run(round_number)\n      state = self._get_crash_state(round_number, crash_result)\n\n      # If we don't have an expected crash state, set it to the one from initial\n      # crash.\n      if not expected_state:\n        expected_state = state.crash_state\n\n      if crash_result.is_security_issue() != expected_security_flag:\n        logs.log('Detected a crash without the correct security flag.')\n        continue\n\n      crash_comparer = CrashComparer(state.crash_state, expected_state)\n      if not crash_comparer.is_similar():\n        logs.log(\n            'Detected a crash with an unrelated state: '\n            'Expected(%s), Found(%s).' % (expected_state, state.crash_state))\n        continue\n\n      crash_count += 1\n      if crash_count >= reproducible_crash_target_count:\n        logs.log('Crash is reproducible.')\n        return True\n\n    logs.log('Crash is not reproducible. Crash count: %d/%d.' % (crash_count,\n                                                                 round_number))\n    return False\n\n\ndef test_for_crash_with_retries(testcase,\n                                testcase_path,\n                                test_timeout,\n                                http_flag=False,\n                                use_gestures=True,\n                                compare_crash=True):\n  \"\"\"Test for a crash and return crash parameters like crash type, crash state,\n  crash stacktrace, etc.\"\"\"\n  gestures = testcase.gestures if use_gestures else None\n  try:\n    runner = TestcaseRunner(testcase.actual_fuzzer_name(), testcase_path,\n                            test_timeout, gestures, http_flag)\n  except TargetNotFoundError:\n    # If a target isn't found, treat it as not crashing.\n    return CrashResult(return_code=0, crash_time=0, output='')\n\n  crash_retries = environment.get_value('CRASH_RETRIES')\n  if compare_crash:\n    expected_state = testcase.crash_state\n    expected_security_flag = testcase.security_flag\n  else:\n    expected_state = None\n    expected_security_flag = None\n\n  return runner.reproduce_with_retries(crash_retries, expected_state,\n                                       expected_security_flag,\n                                       testcase.flaky_stack)\n\n\ndef test_for_reproducibility(fuzzer_name, testcase_path, expected_state,\n                             expected_security_flag, test_timeout, http_flag,\n                             gestures):\n  \"\"\"Test to see if a crash is fully reproducible or is a one-time crasher.\"\"\"\n  try:\n    runner = TestcaseRunner(fuzzer_name, testcase_path, test_timeout, gestures,\n                            http_flag)\n  except TargetNotFoundError:\n    # If a target isn't found, treat it as not crashing.\n    return False\n\n  crash_retries = environment.get_value('CRASH_RETRIES')\n  return runner.test_reproduce_reliability(crash_retries, expected_state,\n                                           expected_security_flag)\n\n\ndef prepare_log_for_upload(symbolized_output, return_code):\n  \"\"\"Prepare log for upload.\"\"\"\n  # Add revision information to the logs.\n  app_revision = environment.get_value('APP_REVISION')\n  job_name = environment.get_value('JOB_NAME')\n  components = revisions.get_component_list(app_revision, job_name)\n  component_revisions = (\n      revisions.format_revision_list(components, use_html=False) or\n      'Not available.\\n')\n\n  revisions_header = (\n      'Component revisions (build r{app_revision}):\\n{component_revisions}\\n'.\n      format(\n          app_revision=app_revision, component_revisions=component_revisions))\n  return_code_header = 'Return code: %s\\n\\n' % return_code\n\n  return revisions_header + return_code_header + symbolized_output\n\n\ndef upload_log(log, log_time):\n  \"\"\"Upload the output into corresponding GCS logs bucket.\"\"\"\n  fuzz_logs_bucket = environment.get_value('FUZZ_LOGS_BUCKET')\n  if not fuzz_logs_bucket:\n    return\n\n  fuzzer_logs.upload_to_logs(fuzz_logs_bucket, log, time=log_time)\n\n\ndef get_user_profile_directory(user_profile_index):\n  \"\"\"Returns a user profile directory from a directory index.\"\"\"\n  temp_directory = environment.get_value('BOT_TMPDIR')\n  user_profile_in_memory = environment.get_value('USER_PROFILE_IN_MEMORY')\n  user_profile_root_directory = (\n      temp_directory if user_profile_in_memory else\n      environment.get_value('USER_PROFILE_ROOT_DIR'))\n\n  # Create path to user profile directory.\n  user_profile_directory_name = 'user_profile_%d' % user_profile_index\n  user_profile_directory = os.path.join(user_profile_root_directory,\n                                        user_profile_directory_name)\n\n  return user_profile_directory\n\n\ndef get_command_line_for_application(file_to_run='',\n                                     user_profile_index=0,\n                                     app_path=None,\n                                     app_args=None,\n                                     needs_http=False,\n                                     write_command_line_file=False):\n  \"\"\"Returns the complete command line required to execute application.\"\"\"\n  if app_args is None:\n    app_args = environment.get_value('APP_ARGS')\n  if app_path is None:\n    app_path = environment.get_value('APP_PATH')\n\n  additional_command_line_flags = get_additional_command_line_flags(file_to_run)\n  app_args_append_testcase = environment.get_value('APP_ARGS_APPEND_TESTCASE')\n  app_directory = environment.get_value('APP_DIR')\n  app_name = environment.get_value('APP_NAME')\n  apps_argument = environment.get_value('APPS_ARG')\n  crash_stacks_directory = environment.get_value('CRASH_STACKTRACES_DIR')\n  debugger = environment.get_value('DEBUGGER_PATH')\n  device_testcases_directory = android.constants.DEVICE_TESTCASES_DIR\n  fuzzer_directory = environment.get_value('FUZZER_DIR')\n  extension_argument = environment.get_value('EXTENSION_ARG')\n  input_directory = environment.get_value('INPUT_DIR')\n  plt = environment.platform()\n  root_directory = environment.get_value('ROOT_DIR')\n  temp_directory = environment.get_value('BOT_TMPDIR')\n  user_profile_argument = environment.get_value('USER_PROFILE_ARG')\n  window_argument = environment.get_value('WINDOW_ARG')\n  user_profile_directory = get_user_profile_directory(user_profile_index)\n\n  # Create user profile directory and setup contents if needed.\n  setup_user_profile_directory_if_needed(user_profile_directory)\n\n  # Handle spaces in APP_PATH.\n  # If application path has spaces, then we need to quote it.\n  if ' ' in app_path:\n    app_path = '\"%s\"' % app_path\n\n  # Prepend command with interpreter if it is a script.\n  interpreter = shell.get_interpreter(app_name)\n  if interpreter:\n    app_path = '%s %s' % (interpreter, app_path)\n\n  # Start creating the command line.\n  command = ''\n\n  launcher = environment.get_value('LAUNCHER_PATH')\n  if environment.is_trusted_host() and not launcher:\n    # Rebase the file_to_run path to the worker's root (unless we're running\n    # under a launcher, which runs on the host).\n    from bot.untrusted_runner import file_host\n    file_to_run = file_host.rebase_to_worker_root(file_to_run)\n\n  # Default case.\n  testcase_path = file_to_run\n  testcase_filename = os.path.basename(testcase_path)\n  testcase_directory = os.path.dirname(testcase_path)\n  testcase_file_url = utils.file_path_to_file_url(testcase_path)\n  testcase_http_url = ''\n\n  # Determine where |testcase_file_url| should point depending on platform and\n  # whether or not a launcher script is used.\n  if file_to_run:\n    if launcher:\n      # In the case of launcher scripts, the testcase file to be run resides on\n      # the host running the launcher script. Thus |testcase_file_url|, which\n      # may point to a location on the device for Android job types, does not\n      # apply. Instead, the launcher script should be passed the original file\n      # to run. By setting |testcase_file_url| to |file_to_run|, we avoid\n      # duplicating job definitions solely for supporting launcher scripts.\n      testcase_file_url = file_to_run\n      # Jobs that have a launcher script which needs to be run on the host will\n      # have app_name == launcher. In this case don't prepend launcher to\n      # command - just use app_name.\n      if os.path.basename(launcher) != app_name:\n        command += launcher + ' '\n    elif plt in ['ANDROID']:\n      # Android-specific testcase path fixup for fuzzers that don't rely on\n      # launcher scripts.\n      local_testcases_directory = environment.get_value('FUZZ_INPUTS')\n\n      # Check if the file to run is in fuzzed testcases folder. If yes, then we\n      # can substitute with a local device path. Otherwise, it is part of some\n      # data bundle with resource dependencies and we just need to use http\n      # host forwarder for that.\n      if file_to_run.startswith(local_testcases_directory):\n        testcase_relative_path = (\n            file_to_run[len(local_testcases_directory) + 1:])\n        testcase_path = os.path.join(device_testcases_directory,\n                                     testcase_relative_path)\n        testcase_file_url = utils.file_path_to_file_url(testcase_path)\n      else:\n        # Force use of host_forwarder based on comment above.\n        needs_http = True\n\n    # Check if the testcase needs to be loaded over http.\n    # TODO(ochang): Make this work for trusted/untrusted.\n    http_ip = '127.0.0.1'\n    http_port_1 = environment.get_value('HTTP_PORT_1', 8000)\n    relative_testcase_path = file_to_run[len(input_directory + os.path.sep):]\n    relative_testcase_path = relative_testcase_path.replace('\\\\', '/')\n    testcase_http_url = 'http://%s:%d/%s' % (http_ip, http_port_1,\n                                             relative_testcase_path)\n\n    if needs_http:\n      # TODO(unassigned): Support https.\n      testcase_file_url = testcase_http_url\n      testcase_path = testcase_http_url\n\n  # Compose app arguments.\n  all_app_args = ''\n\n  if user_profile_argument:\n    all_app_args += ' %s=%s' % (user_profile_argument, user_profile_directory)\n  if extension_argument and EXTENSIONS_PREFIX in testcase_filename:\n    all_app_args += ' %s=%s' % (extension_argument, testcase_directory)\n  if apps_argument and APPS_PREFIX in testcase_filename:\n    all_app_args += ' %s=%s' % (apps_argument, testcase_directory)\n  if window_argument:\n    all_app_args += ' %s' % window_argument\n  if additional_command_line_flags:\n    all_app_args += ' %s' % additional_command_line_flags.strip()\n  if app_args:\n    all_app_args += ' %s' % app_args.strip()\n  # Append %TESTCASE% at end if no testcase pattern is found in app arguments.\n  if not utils.sub_string_exists_in(\n      ['%TESTCASE%', '%TESTCASE_FILE_URL%', '%TESTCASE_HTTP_URL%'],\n      all_app_args) and app_args_append_testcase:\n    all_app_args += ' %TESTCASE%'\n  all_app_args = all_app_args.strip()\n\n  # Build the actual command to run now.\n  if debugger:\n    command += '%s ' % debugger\n  if app_path:\n    command += app_path\n  if all_app_args:\n    command += ' %s' % all_app_args\n  command = command.replace('%APP_DIR%', app_directory)\n  command = command.replace('%CRASH_STACKTRACES_DIR%', crash_stacks_directory)\n  command = command.replace('%DEVICE_TESTCASES_DIR%',\n                            device_testcases_directory)\n  command = command.replace('%FUZZER_DIR%', fuzzer_directory)\n  command = command.replace('%INPUT_DIR%', input_directory)\n  command = command.replace('%ROOT_DIR%', root_directory)\n  command = command.replace('%TESTCASE%', testcase_path)\n  command = command.replace('%TESTCASE_FILE_URL%', testcase_file_url)\n  command = command.replace('%TESTCASE_HTTP_URL%', testcase_http_url)\n  command = command.replace('%TMP_DIR%', temp_directory)\n  command = command.replace('%USER_PROFILE_DIR%', user_profile_directory)\n\n  # Though we attempt to pass all flags that have been used to run html as\n  # a test in our content shell job types for backwards compatibility, a\n  # deprecation warning in recent revisions now causes it to fail. Remove\n  # the --run-layout-test flag to avoid this.\n  content_shell_app_names = [\n      'content_shell', 'content_shell.exe', 'Content Shell'\n  ]\n  if (environment.get_value('APP_NAME') in content_shell_app_names and\n      environment.get_value('APP_REVISION', 0) >= 558998):\n    command = command.replace(' --run-layout-test', '')\n\n  if plt == 'ANDROID' and not launcher:\n    # Initial setup phase for command line.\n    if write_command_line_file:\n      android.adb.write_command_line_file(command, app_path)\n\n    return android.app.get_launch_command(all_app_args, testcase_path,\n                                          testcase_file_url)\n\n  # Decide which directory we will run the application from.\n  # We are using |app_directory| since it helps to locate pdbs\n  # in same directory, other dependencies, etc.\n  if os.path.exists(app_directory):\n    os.chdir(app_directory)\n\n  return str(command)\n\n\ndef setup_user_profile_directory_if_needed(user_profile_directory):\n  \"\"\"Set user profile directory if it does not exist.\"\"\"\n  if os.path.exists(user_profile_directory):\n    # User profile directory already exists. Bail out.\n    return\n\n  shell.create_directory(user_profile_directory)\n\n  # Create a file in user profile directory based on format:\n  # filename;base64 encoded zlib compressed file contents.\n  user_profile_file = environment.get_value('USER_PROFILE_FILE')\n  if user_profile_file and ';' in user_profile_file:\n    user_profile_filename, encoded_file_contents = (\n        user_profile_file.split(';', 1))\n    user_profile_file_contents = zlib.decompress(\n        base64.b64decode(encoded_file_contents))\n    user_profile_file_path = os.path.join(user_profile_directory,\n                                          user_profile_filename)\n    utils.write_data_to_file(user_profile_file_contents, user_profile_file_path)\n\n  # For Firefox, we need to install a special fuzzPriv extension that exposes\n  # special functions to javascript, e.g. gc(), etc.\n  app_name = environment.get_value('APP_NAME')\n  if app_name.startswith('firefox'):\n    # Create extensions directory.\n    extensions_directory = os.path.join(user_profile_directory, 'extensions')\n    shell.create_directory(extensions_directory)\n\n    # Unpack the fuzzPriv extension.\n    extension_archive = os.path.join(environment.get_resources_directory(),\n                                     'firefox', 'fuzzPriv-extension.zip')\n    archive.unpack(extension_archive, extensions_directory)\n\n    # Add this extension in the extensions configuration file.\n    extension_config_file_path = os.path.join(user_profile_directory,\n                                              'extensions.ini')\n    fuzz_extension_directory = os.path.join(extensions_directory,\n                                            'domfuzz@squarefree.com')\n    extension_config_file_contents = (\n        '[ExtensionDirs]\\r\\n'\n        'Extension0=%s\\r\\n'\n        '\\r\\n'\n        '[ThemeDirs]\\r\\n' % fuzz_extension_directory)\n    utils.write_data_to_file(extension_config_file_contents,\n                             extension_config_file_path)\n\n\ndef check_for_bad_build(job_type, crash_revision):\n  \"\"\"Return true if the build is bad, i.e. crashes on startup.\"\"\"\n  # Check the bad build check flag to see if we want do this.\n  if not environment.get_value('BAD_BUILD_CHECK'):\n    return False\n\n  # Create a blank command line with no file to run and no http.\n  command = get_command_line_for_application(file_to_run='', needs_http=False)\n\n  # When checking for bad builds, we use the default window size.\n  # We don't want to pick a custom size since it can potentially cause a\n  # startup crash and cause a build to be detected incorrectly as bad.\n  default_window_argument = environment.get_value('WINDOW_ARG', '')\n  if default_window_argument:\n    command = command.replace(' %s' % default_window_argument, '')\n\n  # TSAN is slow, and boots slow on first startup. Increase the warmup\n  # timeout for this case.\n  if environment.tool_matches('TSAN', job_type):\n    fast_warmup_timeout = environment.get_value('WARMUP_TIMEOUT')\n  else:\n    fast_warmup_timeout = environment.get_value('FAST_WARMUP_TIMEOUT')\n\n  # Initialize helper variables.\n  is_bad_build = False\n  build_run_console_output = ''\n  app_directory = environment.get_value('APP_DIR')\n\n  # Exit all running instances.\n  process_handler.terminate_stale_application_instances()\n\n  # Check if the build is bad.\n  return_code, crash_time, output = process_handler.run_process(\n      command,\n      timeout=fast_warmup_timeout,\n      current_working_directory=app_directory)\n  crash_result = CrashResult(return_code, crash_time, output)\n\n  # 1. Need to account for startup crashes with no crash state. E.g. failed to\n  #    load shared library. So, ignore state for comparison.\n  # 2. Ignore leaks as they don't block a build from reporting regular crashes\n  #    and also don't impact regression range calculations.\n  if (crash_result.is_crash(ignore_state=True) and\n      not crash_result.should_ignore() and\n      not crash_result.get_type() in ['Direct-leak', 'Indirect-leak']):\n    is_bad_build = True\n    build_run_console_output = utils.get_crash_stacktrace_output(\n        command,\n        crash_result.get_stacktrace(symbolized=True),\n        crash_result.get_stacktrace(symbolized=False))\n    logs.log(\n        'Bad build for %s detected at r%d.' % (job_type, crash_revision),\n        output=build_run_console_output)\n\n  # Exit all running instances.\n  process_handler.terminate_stale_application_instances()\n\n  # Any of the conditions below indicate that bot is in a bad state and it is\n  # not caused by the build itself. In that case, just exit.\n  build_state = data_handler.get_build_state(job_type, crash_revision)\n  if is_bad_build and utils.sub_string_exists_in(BAD_STATE_HINTS, output):\n    logs.log_fatal_and_exit(\n        'Bad bot environment detected, exiting.',\n        output=build_run_console_output,\n        snapshot=process_handler.get_runtime_snapshot())\n\n  # If none of the other bots have added information about this build,\n  # then add it now.\n  if (build_state == data_types.BuildState.UNMARKED and\n      not crash_result.should_ignore()):\n    data_handler.add_build_metadata(job_type, crash_revision, is_bad_build,\n                                    build_run_console_output)\n\n  return is_bad_build\n", "hunk": "@@ -510,7 +510,8 @@ def engine_reproduce(engine_impl, target_name, testcase_path, arguments,\n   if not target_path:\n     raise TargetNotFoundError('Failed to find target ' + target_name)\n \n-  return engine_impl.reproduce(target_path, testcase_path, arguments, timeout)\n+  return engine_impl.reproduce(target_path, testcase_path, list(arguments),\n+                               timeout)\n \n \n class TestcaseRunner(object):\n", "comment": "Why not leave this logic in the constructor? Isn't it better to fail earlier?", "ids": ["10819", "37f9229a0ac7eff5f42fa7cedd7cf0307ecd4926", "e4d5499e51a1097eb75e1fc7989aeecf2aa986e1"], "repo": "google/clusterfuzz", "ghid": 888, "old": "   if not target_path:\n     raise TargetNotFoundError('Failed to find target ' + target_name)\n-  return engine_impl.reproduce(target_path, testcase_path, arguments, timeout)\n class TestcaseRunner(object):", "new": "   if not target_path:\n     raise TargetNotFoundError('Failed to find target ' + target_name)\n+  return engine_impl.reproduce(target_path, testcase_path, list(arguments),\n+                               timeout)\n class TestcaseRunner(object):", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -171,17 +171,21 @@ void DoResurrect(int pnum, int rid)\n \t\tClrPlrPath(rid);\r\n \t\tplr[rid].destAction = ACTION_NONE;\r\n \t\tplr[rid]._pInvincible = FALSE;\r\n+#ifndef HELLFIRE\r\n \t\tPlacePlayer(rid);\r\n \r\n \t\thp = 640;\r\n \t\tif (plr[rid]._pMaxHPBase < 640) {\r\n \t\t\thp = plr[rid]._pMaxHPBase;\r\n \t\t}\r\n+#else\r", "oldf": "#include \"diablo.h\"\r\n\r\nint GetManaAmount(int id, int sn)\r\n{\r\n\tint i;  // \"raw\" mana cost\r\n\tint ma; // mana amount\r\n\r\n\t// mana adjust\r\n\tint adj = 0;\r\n\r\n\t// spell level\r\n\tint sl = plr[id]._pSplLvl[sn] + plr[id]._pISplLvlAdd - 1;\r\n\r\n\tif (sl < 0) {\r\n\t\tsl = 0;\r\n\t}\r\n\r\n\tif (sl > 0) {\r\n\t\tadj = sl * spelldata[sn].sManaAdj;\r\n\t}\r\n\tif (sn == SPL_FIREBOLT) {\r\n\t\tadj >>= 1;\r\n\t}\r\n\tif (sn == SPL_RESURRECT && sl > 0) {\r\n\t\tadj = sl * (spelldata[SPL_RESURRECT].sManaCost / 8);\r\n\t}\r\n\r\n\tif (spelldata[sn].sManaCost == 255) {\r\n\t\ti = (BYTE)plr[id]._pMaxManaBase;\r\n\t} else {\r\n\t\ti = spelldata[sn].sManaCost;\r\n\t}\r\n\r\n\tma = (i - adj) << 6;\r\n\r\n\tif (sn == SPL_HEAL) {\r\n\t\tma = (spelldata[SPL_HEAL].sManaCost + 2 * plr[id]._pLevel - adj) << 6;\r\n\t}\r\n\tif (sn == SPL_HEALOTHER) {\r\n\t\tma = (spelldata[SPL_HEAL].sManaCost + 2 * plr[id]._pLevel - adj) << 6;\r\n\t}\r\n\r\n\tif (plr[id]._pClass == PC_ROGUE) {\r\n\t\tma -= ma >> 2;\r\n\t}\r\n\r\n\tif (spelldata[sn].sMinMana > ma >> 6) {\r\n\t\tma = spelldata[sn].sMinMana << 6;\r\n\t}\r\n\r\n\treturn ma * (100 - plr[id]._pISplCost) / 100;\r\n}\r\n\r\nvoid UseMana(int id, int sn)\r\n{\r\n\tint ma; // mana cost\r\n\r\n\tif (id == myplr) {\r\n\t\tswitch (plr[id]._pSplType) {\r\n\t\tcase RSPLTYPE_SKILL:\r\n\t\tcase RSPLTYPE_INVALID:\r\n\t\t\tbreak;\r\n\t\tcase RSPLTYPE_SCROLL:\r\n\t\t\tRemoveScroll(id);\r\n\t\t\tbreak;\r\n\t\tcase RSPLTYPE_CHARGES:\r\n\t\t\tUseStaffCharge(id);\r\n\t\t\tbreak;\r\n\t\tcase RSPLTYPE_SPELL:\r\n#ifdef _DEBUG\r\n\t\t\tif (!debug_mode_key_inverted_v) {\r\n#endif\r\n\t\t\t\tma = GetManaAmount(id, sn);\r\n\t\t\t\tplr[id]._pMana -= ma;\r\n\t\t\t\tplr[id]._pManaBase -= ma;\r\n\t\t\t\tdrawmanaflag = TRUE;\r\n#ifdef _DEBUG\r\n\t\t\t}\r\n#endif\r\n\t\t\tbreak;\r\n\t\t}\r\n\t}\r\n}\r\n\r\nBOOL CheckSpell(int id, int sn, char st, BOOL manaonly)\r\n{\r\n\tBOOL result;\r\n\r\n#ifdef _DEBUG\r\n\tif (debug_mode_key_inverted_v)\r\n\t\treturn TRUE;\r\n#endif\r\n\r\n\tresult = TRUE;\r\n\tif (!manaonly && pcurs != 1) {\r\n\t\tresult = FALSE;\r\n\t} else {\r\n\t\tif (st != RSPLTYPE_SKILL) {\r\n\t\t\tif (GetSpellLevel(id, sn) <= 0) {\r\n\t\t\t\tresult = FALSE;\r\n\t\t\t} else {\r\n\t\t\t\tresult = plr[id]._pMana >= GetManaAmount(id, sn);\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\treturn result;\r\n}\r\n\r\nvoid CastSpell(int id, int spl, int sx, int sy, int dx, int dy, int caster, int spllvl)\r\n{\r\n\tint i;\r\n\tint dir; // missile direction\r\n\r\n\tswitch (caster) {\r\n\tcase 1:\r\n\t\tdir = monster[id]._mdir;\r\n\t\tbreak;\r\n\tcase 0:\r\n\t\t// caster must be 0 already in this case, but oh well,\r\n\t\t// it's needed to generate the right code\r\n\t\tcaster = 0;\r\n\t\tdir = plr[id]._pdir;\r\n\r\n\t\tif (spl == SPL_FIREWALL) {\r\n\t\t\tdir = plr[id]._pVar3;\r\n\t\t}\r\n\t\tbreak;\r\n\t}\r\n\r\n\tfor (i = 0; spelldata[spl].sMissiles[i] != MIS_ARROW && i < 3; i++) {\r\n\t\tAddMissile(sx, sy, dx, dy, dir, spelldata[spl].sMissiles[i], caster, id, 0, spllvl);\r\n\t}\r\n\r\n\tif (spelldata[spl].sMissiles[0] == MIS_TOWN) {\r\n\t\tUseMana(id, SPL_TOWN);\r\n\t}\r\n\tif (spelldata[spl].sMissiles[0] == MIS_CBOLT) {\r\n\t\tUseMana(id, SPL_CBOLT);\r\n\r\n\t\tfor (i = 0; i < (spllvl >> 1) + 3; i++) {\r\n\t\t\tAddMissile(sx, sy, dx, dy, dir, MIS_CBOLT, caster, id, 0, spllvl);\r\n\t\t}\r\n\t}\r\n}\r\n\r\n/**\r\n * @param pnum player index\r\n * @param rid target player index\r\n */\r\nvoid DoResurrect(int pnum, int rid)\r\n{\r\n\tint hp;\r\n\r\n\tif ((char)rid != -1) {\r\n\t\tAddMissile(plr[rid].WorldX, plr[rid].WorldY, plr[rid].WorldX, plr[rid].WorldY, 0, MIS_RESURRECTBEAM, 0, pnum, 0, 0);\r\n\t}\r\n\r\n\tif (pnum == myplr) {\r\n\t\tNewCursor(CURSOR_HAND);\r\n\t}\r\n\r\n\tif ((char)rid != -1 && plr[rid]._pHitPoints == 0) {\r\n\t\tif (rid == myplr) {\r\n\t\t\tdeathflag = FALSE;\r\n\t\t\tgamemenu_off();\r\n\t\t\tdrawhpflag = TRUE;\r\n\t\t\tdrawmanaflag = TRUE;\r\n\t\t}\r\n\r\n\t\tClrPlrPath(rid);\r\n\t\tplr[rid].destAction = ACTION_NONE;\r\n\t\tplr[rid]._pInvincible = FALSE;\r\n#ifndef HELLFIRE\r\n\t\tPlacePlayer(rid);\r\n\r\n\t\thp = 640;\r\n\t\tif (plr[rid]._pMaxHPBase < 640) {\r\n\t\t\thp = plr[rid]._pMaxHPBase;\r\n\t\t}\r\n#else\r\n\t\thp = 640;\r\n#endif\r\n\t\tSetPlayerHitPoints(rid, hp);\r\n\r\n\t\tplr[rid]._pHPBase = plr[rid]._pHitPoints + (plr[rid]._pMaxHPBase - plr[rid]._pMaxHP);\r\n\t\tplr[rid]._pMana = 0;\r\n\t\tplr[rid]._pManaBase = plr[rid]._pMana + (plr[rid]._pMaxManaBase - plr[rid]._pMaxMana);\r\n\r\n\t\tCalcPlrInv(rid, TRUE);\r\n\r\n\t\tif (plr[rid].plrlevel == currlevel) {\r\n\t\t\tStartStand(rid, plr[rid]._pdir);\r\n\t\t} else {\r\n\t\t\tplr[rid]._pmode = PM_STAND;\r\n\t\t}\r\n\t}\r\n}\r\n\r\nvoid PlacePlayer(int pnum)\r\n{\r\n\tint nx, ny, max, min, x, y;\r\n\tDWORD i;\r\n\tBOOL done;\r\n\r\n\tif (plr[pnum].plrlevel == currlevel) {\r\n\t\tfor (i = 0; i < 8; i++) {\r\n\t\t\tnx = plr[pnum].WorldX + plrxoff2[i];\r\n\t\t\tny = plr[pnum].WorldY + plryoff2[i];\r\n\r\n\t\t\tif (PosOkPlayer(pnum, nx, ny)) {\r\n\t\t\t\tbreak;\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tif (!PosOkPlayer(pnum, nx, ny)) {\r\n\t\t\tdone = FALSE;\r\n\r\n\t\t\tfor (max = 1, min = -1; min > -50 && !done; max++, min--) {\r\n\t\t\t\tfor (y = min; y <= max && !done; y++) {\r\n\t\t\t\t\tny = plr[pnum].WorldY + y;\r\n\r\n\t\t\t\t\tfor (x = min; x <= max && !done; x++) {\r\n\t\t\t\t\t\tnx = plr[pnum].WorldX + x;\r\n\r\n\t\t\t\t\t\tif (PosOkPlayer(pnum, nx, ny)) {\r\n\t\t\t\t\t\t\tdone = TRUE;\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tplr[pnum].WorldX = nx;\r\n\t\tplr[pnum].WorldY = ny;\r\n\r\n\t\tdPlayer[nx][ny] = pnum + 1;\r\n\r\n\t\tif (pnum == myplr) {\r\n\t\t\tViewX = nx;\r\n\t\t\tViewY = ny;\r\n\t\t}\r\n\t}\r\n}\r\n\r\nvoid DoHealOther(int pnum, int rid)\r\n{\r\n\tint i, j, hp;\r\n\r\n\tif (pnum == myplr) {\r\n\t\tNewCursor(CURSOR_HAND);\r\n\t}\r\n\r\n\tif ((char)rid != -1 && (plr[rid]._pHitPoints >> 6) > 0) {\r\n\t\thp = (random(57, 10) + 1) << 6;\r\n\r\n\t\tfor (i = 0; i < plr[pnum]._pLevel; i++) {\r\n\t\t\thp += (random(57, 4) + 1) << 6;\r\n\t\t}\r\n\r\n\t\tfor (j = 0; j < GetSpellLevel(pnum, SPL_HEALOTHER); ++j) {\r\n\t\t\thp += (random(57, 6) + 1) << 6;\r\n\t\t}\r\n\r\n\t\tif (plr[pnum]._pClass == PC_WARRIOR) {\r\n\t\t\thp <<= 1;\r\n\t\t}\r\n\r\n\t\tif (plr[pnum]._pClass == PC_ROGUE) {\r\n\t\t\thp += hp >> 1;\r\n\t\t}\r\n\r\n\t\tplr[rid]._pHitPoints += hp;\r\n\r\n\t\tif (plr[rid]._pHitPoints > plr[rid]._pMaxHP) {\r\n\t\t\tplr[rid]._pHitPoints = plr[rid]._pMaxHP;\r\n\t\t}\r\n\r\n\t\tplr[rid]._pHPBase += hp;\r\n\r\n\t\tif (plr[rid]._pHPBase > plr[rid]._pMaxHPBase) {\r\n\t\t\tplr[rid]._pHPBase = plr[rid]._pMaxHPBase;\r\n\t\t}\r\n\r\n\t\tdrawhpflag = TRUE;\r\n\t}\r\n}\r\n", "hunk": "@@ -173,13 +173,13 @@ void DoResurrect(int pnum, int rid)\n \t\tplr[rid]._pInvincible = FALSE;\n #ifndef HELLFIRE\n \t\tPlacePlayer(rid);\n+#endif\n \n \t\thp = 640;\n+#ifndef HELLFIRE\n \t\tif (plr[rid]._pMaxHPBase < 640) {\n \t\t\thp = plr[rid]._pMaxHPBase;\n \t\t}\n-#else\n-\t\thp = 640;\n #endif\n \t\tSetPlayerHitPoints(rid, hp);\n \n", "comment": "I would prefer two hellfire `ifdef`s to an `else`.", "ids": ["9736", "5cf7327e8af69552b79eba270ea53f7903abcce3", "326f353a0da526bc59f69bcf2b6d4fbef2195e0e"], "repo": "diasurgical/devilution", "ghid": 1629, "old": " \t\tplr[rid]._pInvincible = FALSE;\n #ifndef HELLFIRE\n \t\tPlacePlayer(rid);\n \t\thp = 640;\n \t\tif (plr[rid]._pMaxHPBase < 640) {\n \t\t\thp = plr[rid]._pMaxHPBase;\n \t\t}\n-#else\n-\t\thp = 640;\n #endif\n \t\tSetPlayerHitPoints(rid, hp);", "new": " \t\tplr[rid]._pInvincible = FALSE;\n #ifndef HELLFIRE\n \t\tPlacePlayer(rid);\n+#endif\n \t\thp = 640;\n+#ifndef HELLFIRE\n \t\tif (plr[rid]._pMaxHPBase < 640) {\n \t\t\thp = plr[rid]._pMaxHPBase;\n \t\t}\n #endif\n \t\tSetPlayerHitPoints(rid, hp);", "lang": "cpp", "norm_lang": "cpp"}
{"old_hunk": "@@ -42,8 +42,6 @@ def read(handle, pfm_format):\n def _read_pfm_four_columns(handle):\n     \"\"\"Read motifs in Cluster Buster position frequency matrix format from a file handle.\n \n-    Cluster Buster motif format: http://zlab.bu.edu/cluster-buster/help/cis-format.html", "oldf": "# Copyright 2015 by Gert Hulselmans.  All rights reserved.\n# This file is part of the Biopython distribution and governed by your\n# choice of the \"Biopython License Agreement\" or the \"BSD 3-Clause License\".\n# Please see the LICENSE file that should have been included as part of this\n# package.\n\n\"\"\"Parse various position frequency matrix format files.\"\"\"\n\nimport re\n\nfrom Bio import motifs\n\n\nclass Record(list):\n    \"\"\"Class to store the information in a position frequency matrix table.\n\n    The record inherits from a list containing the individual motifs.\n    \"\"\"\n\n    def __str__(self):\n        return \"\\n\".join(str(motif) for motif in self)\n\n\ndef read(handle, pfm_format):\n    \"\"\"Read motif(s) from a file in various position frequency matrix formats.\n\n    Return the record of PFM(s).\n    Call the appropriate routine based on the format passed.\n    \"\"\"\n    # Supporting underscores here for backward compatibility\n    pfm_format = pfm_format.lower().replace(\"_\", \"-\")\n    if pfm_format == \"pfm-four-columns\":\n        record = _read_pfm_four_columns(handle)\n        return record\n    elif pfm_format == \"pfm-four-rows\":\n        record = _read_pfm_four_rows(handle)\n        return record\n    else:\n        raise ValueError(\"Unknown Position Frequency matrix format '%s'\" % pfm_format)\n\n\ndef _read_pfm_four_columns(handle):\n    \"\"\"Read motifs in Cluster Buster position frequency matrix format from a file handle.\n\n    #cisbp\n    Pos A   C   G   T\n    1   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846\n    2   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846\n    3   0.971153846153846   0.00961538461538462 0.00961538461538462 0.00961538461538462\n    4   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846\n    5   0.00961538461538462 0.971153846153846   0.00961538461538462 0.00961538461538462\n    6   0.971153846153846   0.00961538461538462 0.00961538461538462 0.00961538461538462\n    7   0.00961538461538462 0.971153846153846   0.00961538461538462 0.00961538461538462\n    8   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846\n\n    #c2h2 zfs\n    Gene    ENSG00000197372\n    Pos A   C   G   T\n    1   0.341303    0.132427    0.117054    0.409215\n    2   0.283785    0.077066    0.364552    0.274597\n    3   0.491055    0.078208    0.310520    0.120217\n    4   0.492621    0.076117    0.131007    0.300256\n    5   0.250645    0.361464    0.176504    0.211387\n    6   0.276694    0.498070    0.197793    0.027444\n    7   0.056317    0.014631    0.926202    0.002850\n    8   0.004470    0.007769    0.983797    0.003964\n    9   0.936213    0.058787    0.002387    0.002613\n    10  0.004352    0.004030    0.002418    0.989200\n    11  0.013277    0.008165    0.001991    0.976567\n    12  0.968132    0.002263    0.002868    0.026737\n    13  0.397623    0.052017    0.350783    0.199577\n    14  0.000000    0.000000    1.000000    0.000000\n    15  1.000000    0.000000    0.000000    0.000000\n    16  0.000000    0.000000    1.000000    0.000000\n    17  0.000000    0.000000    1.000000    0.000000\n    18  1.000000    0.000000    0.000000    0.000000\n    19  0.000000    1.000000    0.000000    0.000000\n    20  1.000000    0.000000    0.000000    0.000000\n\n    #c2h2 zfs\n    Gene    FBgn0000210\n    Motif   M1734_0.90\n    Pos A   C   G   T\n    1   0.25    0.0833333   0.0833333   0.583333\n    2   0.75    0.166667    0.0833333   0\n    3   0.833333    0   0   0.166667\n    4   1   0   0   0\n    5   0   0.833333    0.0833333   0.0833333\n    6   0.333333    0   0   0.666667\n    7   0.833333    0   0   0.166667\n    8   0.5 0   0.333333    0.166667\n    9   0.5 0.0833333   0.166667    0.25\n    10  0.333333    0.25    0.166667    0.25\n    11  0.166667    0.25    0.416667    0.166667\n\n    # flyfactorsurvey (cluster buster)\n    >AbdA_Cell_FBgn0000014\n    1   3   0   14\n    0   0   0   18\n    16  0   0   2\n    18  0   0   0\n    1   0   0   17\n    0   0   6   12\n    15  1   2   0\n\n    # homer\n    >ATGACTCATC AP-1(bZIP)/ThioMac-PU.1-ChIP-Seq(GSE21512)/Homer    6.049537    -1.782996e+03   0   9805.3,5781.0,3085.1,2715.0,0.00e+00\n    0.419   0.275   0.277   0.028\n    0.001   0.001   0.001   0.997\n    0.010   0.002   0.965   0.023\n    0.984   0.003   0.001   0.012\n    0.062   0.579   0.305   0.054\n    0.026   0.001   0.001   0.972\n    0.043   0.943   0.001   0.012\n    0.980   0.005   0.001   0.014\n    0.050   0.172   0.307   0.471\n    0.149   0.444   0.211   0.195\n\n    # hocomoco\n    > AHR_si\n    40.51343240527031  18.259112547756697  56.41253757072521  38.77363485291994\n    10.877470982533044  11.870876719950774  34.66312982331297  96.54723985087516\n    21.7165707818416  43.883079837598544  20.706746561638717  67.6523201955933\n    2.5465132509466635  1.3171620263517245  145.8637051322628  4.231336967110781\n    0.0  150.35847450464382  1.4927836298652875  2.1074592421627525\n    3.441039751299748  0.7902972158110341  149.37613720253387  0.3512432070271259\n    0.0  3.441039751299748  0.7024864140542533  149.81519121131782\n    0.0  0.0  153.95871737667187  0.0\n    43.07922333291745  66.87558226865211  16.159862546986584  27.844049228115868\n\n    # neph\n    UW.Motif.0001   atgactca\n    0.772949    0.089579    0.098612    0.038860\n    0.026652    0.004653    0.025056    0.943639\n    0.017663    0.023344    0.918728    0.040264\n    0.919596    0.025414    0.029759    0.025231\n    0.060312    0.772259    0.104968    0.062462\n    0.037406    0.020643    0.006667    0.935284\n    0.047316    0.899024    0.026928    0.026732\n    0.948639    0.019497    0.005737    0.026128\n\n    # tiffin\n    T   A   G   C\n    30  0   28  40\n    0   0   0   99\n    0   55  14  29\n    0   99  0   0\n    20  78  0   0\n    0   52  7   39\n    19  46  11  22\n    0   60  38  0\n    0   33  0   66\n    73  0   25  0\n    99  0   0   0\n    \"\"\"\n    record = Record()\n\n    motif_name = None\n    motif_nbr = 0\n    motif_nbr_added = 0\n\n    default_nucleotide_order = [\"A\", \"C\", \"G\", \"T\"]\n    nucleotide_order = default_nucleotide_order\n    nucleotide_counts = {\"A\": [], \"C\": [], \"G\": [], \"T\": []}\n\n    for line in handle:\n        line = line.strip()\n\n        if line:\n            columns = line.split()\n            nbr_columns = len(columns)\n\n            if line.startswith(\"#\"):\n                # Skip comment lines.\n                continue\n            elif line.startswith(\">\"):\n                # Parse \">AbdA_Cell_FBgn0000014\" and \"> AHR_si\" like lines and put the part after \">\" as motif name.\n                if motif_nbr != 0 and motif_nbr_added != motif_nbr:\n                    # Add the previous motif to the record.\n                    motif = motifs.Motif(alphabet=\"GATC\", counts=nucleotide_counts)\n                    motif.name = motif_name\n                    record.append(motif)\n                    motif_nbr_added = motif_nbr\n\n                # Reinitialize variables for the new motif.\n                motif_name = line[1:].strip()\n                nucleotide_order = default_nucleotide_order\n            elif columns[0] == \"Gene\":\n                # Parse \"Gene   ENSG00000197372\" like lines and put the gene name as motif name.\n                if motif_nbr != 0 and motif_nbr_added != motif_nbr:\n                    # Add the previous motif to the record.\n                    motif = motifs.Motif(alphabet=\"GATC\", counts=nucleotide_counts)\n                    motif.name = motif_name\n                    record.append(motif)\n                    motif_nbr_added = motif_nbr\n\n                # Reinitialize variables for the new motif.\n                motif_name = columns[1]\n                nucleotide_order = default_nucleotide_order\n            elif columns[0] == \"Motif\":\n                # Parse \"Motif  M1734_0.90\" like lines.\n                if motif_nbr != 0 and motif_nbr_added != motif_nbr:\n                    # Add the previous motif to the record.\n                    motif = motifs.Motif(alphabet=\"GATC\", counts=nucleotide_counts)\n                    motif.name = motif_name\n                    record.append(motif)\n                    motif_nbr_added = motif_nbr\n\n                # Reinitialize variables for the new motif.\n                motif_name = columns[1]\n                nucleotide_order = default_nucleotide_order\n            elif columns[0] == \"Pos\":\n                # Parse \"Pos    A   C   G   T\" like lines and change nucleotide order if necessary.\n                if nbr_columns == 5:\n                    # If the previous line was not a \"Gene  ENSG00000197372\" like line, a new motif starts here.\n                    if motif_nbr != 0 and motif_nbr_added != motif_nbr:\n                        # Add the previous motif to the record.\n                        motif = motifs.Motif(alphabet=\"GATC\", counts=nucleotide_counts)\n                        motif.name = motif_name\n                        record.append(motif)\n                        motif_nbr_added = motif_nbr\n\n                    nucleotide_order = default_nucleotide_order\n\n                    if set(columns[1:]) == set(default_nucleotide_order):\n                        nucleotide_order = columns[1:]\n            elif columns[0] in default_nucleotide_order:\n                # Parse \"A  C   G   T\" like lines and change nucleotide order if necessary.\n                if nbr_columns == 4:\n                    nucleotide_order = default_nucleotide_order\n                    if set(columns) == set(default_nucleotide_order):\n                        nucleotide_order = columns\n            else:\n                # Parse matrix columns lines and use the correct nucleotide order.\n                if nbr_columns == 4:\n                    matrix_columns = columns\n                elif nbr_columns == 5:\n                    matrix_columns = columns[1:]\n                else:\n                    continue\n\n                if motif_nbr == motif_nbr_added:\n                    # A new motif matrix starts here, so reinitialize variables for the new motif.\n                    nucleotide_counts = {\"A\": [], \"C\": [], \"G\": [], \"T\": []}\n                    motif_nbr += 1\n\n                [\n                    nucleotide_counts[nucleotide].append(float(nucleotide_count))\n                    for nucleotide, nucleotide_count in zip(\n                        nucleotide_order, matrix_columns\n                    )\n                ]\n        else:\n            # Empty lines can be separators between motifs.\n            if motif_nbr != 0 and motif_nbr_added != motif_nbr:\n                # Add the previous motif to the record.\n                motif = motifs.Motif(alphabet=\"GATC\", counts=nucleotide_counts)\n                motif.name = motif_name\n                record.append(motif)\n                motif_nbr_added = motif_nbr\n\n            # Reinitialize variables for the new motif.\n            motif_name = None\n            nucleotide_order = default_nucleotide_order\n            # nucleotide_counts = {'A': [], 'C': [], 'G': [], 'T': []}\n\n    if motif_nbr != 0 and motif_nbr_added != motif_nbr:\n        motif = motifs.Motif(alphabet=\"GATC\", counts=nucleotide_counts)\n        motif.name = motif_name\n        record.append(motif)\n\n    return record\n\n\ndef _read_pfm_four_rows(handle):\n    \"\"\"Read motifs in position frequency matrix format from a file handle.\n\n    #hdpi\n    A   0   5   6   5   1   0\n    C   1   1   0   0   0   4\n    G   5   0   0   0   3   0\n    T   0   0   0   1   2   2\n\n    # yetfasco\n    A   0.5 0.0 0.0 0.25    0.25    0.25    0.25    0.25    0.25    0.25    0.25    0.25    0.5 0.0 0.0833333334583333\n    T   0.0 0.0 0.0 0.25    0.25    0.25    0.25    0.25    0.25    0.25    0.25    0.25    0.0 0.0 0.0833333334583333\n    G   0.0 1.0 0.0 0.25    0.25    0.25    0.25    0.25    0.25    0.25    0.25    0.25    0.0 1.0 0.249999999875\n    C   0.5 0.0 1.0 0.25    0.25    0.25    0.25    0.25    0.25    0.25    0.25    0.25    0.5 0.0 0.583333333208333\n\n    #flyfactorsurvey ZFP finger\n    A |     92    106    231    135      0      1    780     28      0    700    739     94     60    127    130\n    C |    138     82    129     81    774      1      3      1      0      6     17     49    193    122    148\n    G |    270    398     54    164      7    659      1    750    755     65      1     41    202    234    205\n    T |    290    204    375    411      9    127      6     11     36     20     31    605    335    307    308\n\n    # scertf pcm\n    A | 9 1 1 97 1 94\n    T | 80 1 97 1 1 2\n    C | 9 97 1 1 1 2\n    G | 2 1 1 1 97 2\n\n    # scertf pfm\n    A | 0.090 0.010 0.010 0.970 0.010 0.940\n    C | 0.090 0.970 0.010 0.010 0.010 0.020\n    G | 0.020 0.010 0.010 0.010 0.970 0.020\n    T | 0.800 0.010 0.970 0.010 0.010 0.020\n\n    #idmmpmm\n    > abd-A\n    0.218451749734889 0.0230646871686108 0.656680805938494 0.898197242841994 0.040694591728526 0.132953340402969 0.74907211028632 0.628313891834571\n    0.0896076352067868 0.317338282078473 0.321580063626723 0.0461293743372216 0.0502386002120891 0.040694591728526 0.0284994697773065 0.0339342523860021\n    0.455991516436904 0.0691940615058324 0.0108695652173913 0.0217391304347826 0.0284994697773065 0.0284994697773065 0.016304347826087 0.160127253446448\n    0.235949098621421 0.590402969247084 0.0108695652173913 0.0339342523860021 0.880567338282079 0.797852598091198 0.206124072110286 0.17762460233298\n\n    # JASPAR\n        >MA0001.1 AGL3\n        A  [ 0  3 79 40 66 48 65 11 65  0 ]\n        C  [94 75  4  3  1  2  5  2  3  3 ]\n        G  [ 1  0  3  4  1  0  5  3 28 88 ]\n        T  [ 2 19 11 50 29 47 22 81  1  6 ]\n\n    or::\n\n        >MA0001.1 AGL3\n        0  3 79 40 66 48 65 11 65  0\n        94 75  4  3  1  2  5  2  3  3\n        1  0  3  4  1  0  5  3 28 88\n        2 19 11 50 29 47 22 81  1  6\n    \"\"\"\n    record = Record()\n\n    name_pattern = re.compile(r\"^>\\s*(.+)\\s*\")\n    row_pattern_with_nucleotide_letter = re.compile(\n        r\"\\s*([ACGT])\\s*[[]*[|]*\\s*([0-9.\\-eE\\s]+)\\s*[]]*\\s*\"\n    )\n    row_pattern_without_nucleotide_letter = re.compile(r\"\\s*([0-9.\\-eE\\s]+)\\s*\")\n\n    motif_name = None\n    nucleotide_counts = {}\n    row_count = 0\n    nucleotides = [\"A\", \"C\", \"G\", \"T\"]\n\n    for line in handle:\n        line = line.strip()\n\n        name_match = name_pattern.match(line)\n        row_match_with_nucleotide_letter = row_pattern_with_nucleotide_letter.match(\n            line\n        )\n        row_match_without_nucleotide_letter = row_pattern_without_nucleotide_letter.match(\n            line\n        )\n\n        if name_match:\n            motif_name = name_match.group(1)\n        elif row_match_with_nucleotide_letter:\n            (nucleotide, counts_str) = row_match_with_nucleotide_letter.group(1, 2)\n            current_nucleotide_counts = counts_str.split()\n            nucleotide_counts[nucleotide] = [\n                float(current_nucleotide_count)\n                for current_nucleotide_count in current_nucleotide_counts\n            ]\n            row_count += 1\n            if row_count == 4:\n                motif = motifs.Motif(alphabet=\"GATC\", counts=nucleotide_counts)\n\n                if motif_name:\n                    motif.name = motif_name\n\n                record.append(motif)\n\n                motif_name = None\n                nucleotide_counts = {}\n                row_count = 0\n        elif row_match_without_nucleotide_letter:\n            current_nucleotide_counts = row_match_without_nucleotide_letter.group(\n                1\n            ).split()\n            nucleotide_counts[nucleotides[row_count]] = [\n                float(current_nucleotide_count)\n                for current_nucleotide_count in current_nucleotide_counts\n            ]\n            row_count += 1\n            if row_count == 4:\n                motif = motifs.Motif(alphabet=\"GATC\", counts=nucleotide_counts)\n\n                if motif_name:\n                    motif.name = motif_name\n\n                record.append(motif)\n\n                motif_name = None\n                nucleotide_counts = {}\n                row_count = 0\n\n    return record\n\n\ndef write(motifs):\n    \"\"\"Return the representation of motifs in Cluster Buster position frequency matrix format.\"\"\"\n    lines = []\n    for m in motifs:\n        line = f\">{m.name}\\n\"\n        lines.append(line)\n        for ACGT_counts in zip(\n            m.counts[\"A\"], m.counts[\"C\"], m.counts[\"G\"], m.counts[\"T\"]\n        ):\n            lines.append(\"{:0.0f}\\t{:0.0f}\\t{:0.0f}\\t{:0.0f}\\n\".format(*ACGT_counts))\n\n    # Finished; glue the lines together.\n    text = \"\".join(lines)\n\n    return text\n", "hunk": "@@ -40,9 +40,9 @@ def read(handle, pfm_format):\n \n \n def _read_pfm_four_columns(handle):\n-    \"\"\"Read motifs in Cluster Buster position frequency matrix format from a file handle.\n+    \"\"\"Read motifs in position frequency matrix format (4 columns) from a file handle.\n \n-    #cisbp\n+    # cisbp\n     Pos A   C   G   T\n     1   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846\n     2   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846\n", "comment": "Why remove this URL?", "ids": ["27951", "da0943fe745961c62e5a39905221936e38a46f58", "a3a57c54dc4e751d67557f5dfe77865b874848be"], "repo": "biopython/biopython", "ghid": 3501, "old": " def _read_pfm_four_columns(handle):\n-    \"\"\"Read motifs in Cluster Buster position frequency matrix format from a file handle.\n-    #cisbp\n     Pos A   C   G   T\n     1   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846\n     2   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846", "new": " def _read_pfm_four_columns(handle):\n+    \"\"\"Read motifs in position frequency matrix format (4 columns) from a file handle.\n+    # cisbp\n     Pos A   C   G   T\n     1   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846\n     2   0.00961538461538462 0.00961538461538462 0.00961538461538462 0.971153846153846", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -3,34 +3,96 @@\n \"\"\"Setup lightgbm package.\"\"\"\n from __future__ import absolute_import\n \n+import struct\n import os\n import sys\n-\n+import getopt\n+import distutils\n+from distutils import dir_util\n+from distutils import file_util\n from setuptools import find_packages, setup\n \n-sys.path.insert(0, '.')\n+if __name__ == \"__main__\":\n+    if (8 * struct.calcsize(\"P\")) != 64:\n+        raise Exception('Cannot install LightGBM in 32-bit python, please use 64-bit python instead.')\n+    use_gpu = False\n+    use_mingw = False\n+    use_precompile = False\n+    try:\n+        opts, args = getopt.getopt(sys.argv[2:], 'mgp', ['mingw', 'gpu', 'precompile'])\n+        for opt, arg in opts:\n+            if opt in ('-m', '--mingw'):\n+                use_mingw = True\n+            elif opt in ('-g', '--gpu'):\n+                use_gpu = True\n+            elif opt in ('-p', '--precompile'):\n+                use_precompile = True\n+    except getopt.GetoptError as err:\n+        pass\n+    sys.argv = sys.argv[0:2]", "oldf": "# coding: utf-8\n# pylint: disable=invalid-name, exec-used\n\"\"\"Setup lightgbm package.\"\"\"\nfrom __future__ import absolute_import\n\nimport struct\nimport os\nimport sys\nimport getopt\nimport distutils\nfrom distutils import dir_util\nfrom distutils import file_util\nfrom setuptools import find_packages, setup\n\nif __name__ == \"__main__\":\n    if (8 * struct.calcsize(\"P\")) != 64:\n        raise Exception('Cannot install LightGBM in 32-bit python, please use 64-bit python instead.')\n    use_gpu = False\n    use_mingw = False\n    use_precompile = False\n    try:\n        opts, args = getopt.getopt(sys.argv[2:], 'mgp', ['mingw', 'gpu', 'precompile'])\n        for opt, arg in opts:\n            if opt in ('-m', '--mingw'):\n                use_mingw = True\n            elif opt in ('-g', '--gpu'):\n                use_gpu = True\n            elif opt in ('-p', '--precompile'):\n                use_precompile = True\n    except getopt.GetoptError as err:\n        pass\n    sys.argv = sys.argv[0:2]\n    if not use_precompile:\n        if not os.path.isfile(\"_IS_FULL_PACKAGE.txt\"):\n            if os.path.exists(\"../include\"):\n                distutils.dir_util.copy_tree(\"../include\", \"./lightgbm/include\")\n            else:\n                raise Exception('Cannot copy ../include folder')\n            if os.path.exists(\"../src\"):\n                distutils.dir_util.copy_tree(\"../src\", \"./lightgbm/src\")\n            else:\n                raise Exception('Cannot copy ../src folder')\n            if use_gpu:\n                if os.path.exists(\"../compute\"):\n                    distutils.dir_util.copy_tree(\"../compute\", \"./lightgbm/compute\")\n                else:\n                    raise Exception('Cannot copy ../compute folder')\n            distutils.file_util.copy_file(\"../CMakeLists.txt\", \"./lightgbm/\")\n            file_flag = open(\"_IS_FULL_PACKAGE.txt\", 'w')\n            file_flag.close()\n\n        if not os.path.exists(\"build\"):\n            os.makedirs(\"build\")\n        os.chdir(\"build\")\n\n        cmake_cmd = \"cmake -DBUILD_EXE=OFF -DBUILD_LIB=ON \"\n        build_cmd = \"make\"\n\n        if os.name == \"nt\":\n            if use_mingw:\n                cmake_cmd = cmake_cmd + \" -G \\\"MinGW Makefiles\\\" \"\n                build_cmd = \"mingw32-make.exe\"\n            else:\n                cmake_cmd = cmake_cmd + \" -DCMAKE_GENERATOR_PLATFORM=x64 \"\n                build_cmd = \"cmake --build . --target _lightgbm  --config Release\"\n        if use_gpu:\n            cmake_cmd = cmake_cmd + \" -DUSE_GPU=ON \"\n        print(\"Start to build libarary.\")\n        os.system(cmake_cmd + \" ../lightgbm/\")\n        os.system(build_cmd)\n        os.chdir(\"..\")\n\n    sys.path.insert(0, '.')\n\n    CURRENT_DIR = os.path.dirname(__file__)\n\n    libpath_py = os.path.join(CURRENT_DIR, 'lightgbm/libpath.py')\n    libpath = {'__file__': libpath_py}\n    exec(compile(open(libpath_py, \"rb\").read(), libpath_py, 'exec'), libpath, libpath)\n\n    LIB_PATH = [os.path.relpath(path, CURRENT_DIR) for path in libpath['find_lib_path']()]\n    print(\"Install lib_lightgbm from: %s\" % LIB_PATH)\n    setup(name='lightgbm',\n          version='0.2a0',\n          description='LightGBM Python Package',\n          install_requires=[\n              'numpy',\n              'scipy',\n              'scikit-learn'\n          ],\n          maintainer='Guolin Ke',\n          maintainer_email='guolin.ke@microsoft.com',\n          zip_safe=False,\n          packages=find_packages(),\n          include_package_data=True,\n          data_files=[('lightgbm', LIB_PATH)],\n          license='The MIT License(https://github.com/Microsoft/LightGBM/blob/master/LICENSE)',\n          url='https://github.com/Microsoft/LightGBM')\n", "hunk": "@@ -8,11 +8,13 @@ import os\n import sys\n import getopt\n import distutils\n+import shutil\n from distutils import dir_util\n from distutils import file_util\n from setuptools import find_packages, setup\n \n if __name__ == \"__main__\":\n+    build_sdist = sys.argv[1] == 'sdist'\n     if (8 * struct.calcsize(\"P\")) != 64:\n         raise Exception('Cannot install LightGBM in 32-bit python, please use 64-bit python instead.')\n     use_gpu = False\n", "comment": "I got the same error on my mac, fix by removing this line", "ids": ["16935", "4718773fd30cfb6c23b38eaeab1b766dc56617d9", "5963d70ed8a3110e26f710524f85e8f081e785e4"], "repo": "microsoft/LightGBM", "ghid": 635, "old": " import sys\n import getopt\n import distutils\n from distutils import dir_util\n from distutils import file_util\n from setuptools import find_packages, setup\n if __name__ == \"__main__\":\n     if (8 * struct.calcsize(\"P\")) != 64:\n         raise Exception('Cannot install LightGBM in 32-bit python, please use 64-bit python instead.')\n     use_gpu = False", "new": " import sys\n import getopt\n import distutils\n+import shutil\n from distutils import dir_util\n from distutils import file_util\n from setuptools import find_packages, setup\n if __name__ == \"__main__\":\n+    build_sdist = sys.argv[1] == 'sdist'\n     if (8 * struct.calcsize(\"P\")) != 64:\n         raise Exception('Cannot install LightGBM in 32-bit python, please use 64-bit python instead.')\n     use_gpu = False", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -594,6 +594,10 @@ void Settings::parseCommonArguments(QStringList& args)\n         foreach (QString v, values)\n         {\n           QStringList newList = conf().getList(kvl[0]);\n+          if( !newList.contains(v))", "oldf": "/*\n * This file is part of Hootenanny.\n *\n * Hootenanny is free software: you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n *\n * --------------------------------------------------------------------\n *\n * The following copyright notices are generated automatically. If you\n * have a new notice to add, please use the format:\n * \" * @copyright Copyright ...\"\n * This will properly maintain the copyright information. DigitalGlobe\n * copyrights will be updated automatically.\n *\n * @copyright Copyright (C) 2015, 2016, 2017, 2018, 2019 DigitalGlobe (http://www.digitalglobe.com/)\n */\n\n#include \"Settings.h\"\n\n// hoot\n#include <hoot/core/Hoot.h>\n#include <hoot/core/util/ConfPath.h>\n#include <hoot/core/util/ConfigDefaults.h>\n#include <hoot/core/util/HootException.h>\n#include <hoot/core/util/Log.h>\n\n// Boost\n#include <boost/property_tree/ptree.hpp>\n#include <boost/property_tree/json_parser.hpp>\n#include <boost/foreach.hpp>\nnamespace pt = boost::property_tree;\n\n// Qt\n#include <QStringList>\n\n// Standard\n#include <fstream>\n#include <iostream>\n#include <sstream>\n#include <unistd.h>\nusing namespace std;\n\nnamespace hoot\n{\n\nclass JsonLoader\n{\npublic:\n  JsonLoader(Settings* s) : _s(s)\n  {\n  }\n\n  void load(QString path)\n  {\n    fstream is(path.toUtf8().constData(), ios::in);\n\n    if (is.good() == false)\n    {\n      throw HootException(QString(\"Error reading %1\").arg(path));\n    }\n\n    try\n    {\n      load(is);\n    }\n    catch (const HootException& e)\n    {\n      throw HootException(\"Error parsing file: \" + path + \". \" + e.what());\n    }\n  }\n\n  void load(istream& is)\n  {\n    try\n    {\n      pt::ptree pt;\n      pt::read_json(is, pt);\n      _loadTags(pt);\n    }\n    catch (const std::exception& e)\n    {\n      QString reason = e.what();\n      throw HootException(\"Error parsing JSON \" + reason);\n    }\n  }\n\n  void loadFromString(QString json)\n  {\n    stringstream ss(json.toUtf8().constData(), ios::in);\n\n    if (ss.good() == false)\n    {\n      throw HootException(QString(\"Error reading from JSON string:\\n%1\").arg(json));\n    }\n\n    load(ss);\n  }\n\nprivate:\n\n  Settings* _s;\n\n  void _loadTags(pt::ptree& tree)\n  {\n    BOOST_FOREACH(pt::ptree::value_type& element, tree.get_child(\"\"))\n    {\n      QString name = QString::fromUtf8(element.first.c_str());\n      //  Skip comments\n      if (name.startsWith(\"#\"))\n        continue;\n      if (!_s->hasKey(name))\n      {\n        throw HootException(\"Unknown JSON setting: (\" + name + \")\");\n      }\n      //  Set key/value pair as name and data, data() turns everything to a string\n      _s->set(name, QString::fromUtf8(element.second.data().c_str()));\n    }\n  }\n};\n\nboost::shared_ptr<Settings> Settings::_theInstance = NULL;\n\nSettings::Settings() :\n  _dynamicRegex(\"\\\\$\\\\{([\\\\w\\\\.]+)\\\\}\"),\n  _staticRegex(\"\\\\$\\\\(([\\\\w\\\\.]+)\\\\)\")\n{\n\n}\n\nvoid Settings::prepend(const QString& key, const QStringList& values)\n{\n  QStringList l = getList(key, QStringList());\n  for (int i = values.size() - 1; i == 0; i--)\n  {\n    l.prepend(values.at(i));\n  }\n  set(key, l);\n}\n\nvoid Settings::append(const QString& key, const QStringList& values)\n{\n  QStringList l = getList(key, QStringList());\n  l.append(values);\n  set(key, l);\n}\n\nvoid Settings::_checkConvert(const QString& key, const QVariant& value, QVariant::Type type) const\n{\n  if (value.isNull() || value.canConvert(QVariant::Bool) == false)\n  {\n    throw HootException(QString(\"Unable to convert key: '%1', value: '%2' to %3.\")\n      .arg(key).arg(value.toString()).arg(QVariant::typeToName(type)));\n  }\n}\n\nvoid Settings::clear()\n{\n  // this can be very handy when determining why/when settings got cleared.\n  if (this == _theInstance.get())\n  {\n    LOG_DEBUG(\"Clearing global settings.\");\n  }\n  _settings.clear();\n}\n\nQVariant Settings::get(const QString& key) const\n{\n  if (hasKey(key) == false)\n  {\n    throw HootException(\"Error finding key: \" + key);\n  }\n  QVariant result = _settings.value(key);\n\n  if (result.type() == QVariant::String)\n  {\n    std::set<QString> used;\n    QString r = _replaceVariables(key, used);\n    result = r;\n  }\n  return result;\n}\n\nbool Settings::getBool(const QString& key) const\n{\n  if (hasKey(key) == false)\n  {\n    throw HootException(\"Error finding key: \" + key);\n  }\n  const QVariant v = get(key);\n  _checkConvert(key, v, QVariant::Bool);\n  return v.toBool();\n}\n\nbool Settings::getBool(const QString& key, bool defaultValue) const\n{\n  if (hasKey(key) == false)\n  {\n    return defaultValue;\n  }\n  const QVariant v = get(key);\n  _checkConvert(key, v, QVariant::Bool);\n  return v.toBool();\n}\n\ndouble Settings::getDouble(const QString& key) const\n{\n  if (hasKey(key) == false)\n  {\n    throw HootException(\"Error finding key: \" + key);\n  }\n  const QVariant v = get(key);\n  _checkConvert(key, v, QVariant::Double);\n  return v.toDouble();\n}\n\ndouble Settings::getDouble(const QString& key, double defaultValue) const\n{\n  if (hasKey(key) == false)\n  {\n    return defaultValue;\n  }\n  const QVariant v = get(key);\n  _checkConvert(key, v, QVariant::Double);\n  return v.toDouble();\n}\n\ndouble Settings::getDouble(const QString& key, double defaultValue, double min, double max ) const\n{\n  double retVal = getDouble(key, defaultValue);\n  if ( retVal < min )\n  {\n    retVal = min;\n  }\n  else if ( retVal > max )\n  {\n    retVal = max;\n  }\n\n  return retVal;\n}\n\ndouble Settings::getDoubleValue(const QString& value) const\n{\n  QVariant v = getValue(value);\n\n  if (v.isNull() || v.canConvert(QVariant::Double) == false)\n  {\n    throw HootException(\"Unable to convert \" + v.toString() + \" to a double.\");\n  }\n  return v.toDouble();\n}\n\nSettings& Settings::getInstance()\n{\n  if (_theInstance == NULL)\n  {\n    _theInstance.reset(new Settings());\n    _theInstance->loadDefaults();\n  }\n  return *_theInstance;\n}\n\nint Settings::getInt(const QString& key) const\n{\n  if (hasKey(key) == false)\n  {\n    throw HootException(\"Error finding key: \" + key);\n  }\n  const QVariant v = get(key);\n  _checkConvert(key, v, QVariant::Int);\n  return v.toInt();\n}\n\nint Settings::getInt(const QString& key, int defaultValue) const\n{\n  if (hasKey(key) == false)\n  {\n    return defaultValue;\n  }\n  const QVariant v = get(key);\n  _checkConvert(key, v, QVariant::Int);\n  return v.toInt();\n}\n\nint Settings::getInt(const QString& key, int defaultValue, int min, int max) const\n{\n  int retVal = getInt(key, defaultValue);\n\n  if ( retVal < min )\n  {\n    retVal = min;\n  }\n  else if ( retVal > max )\n  {\n    retVal = max;\n  }\n\n  return retVal;\n}\n\nlong Settings::getLong(const QString& key) const\n{\n  if (hasKey(key) == false)\n  {\n    throw HootException(\"Error finding key: \" + key);\n  }\n  const QVariant v = get(key);\n  _checkConvert(key, v, QVariant::LongLong);\n  return v.toLongLong();\n}\n\nlong Settings::getLong(const QString& key, long defaultValue) const\n{\n  if (hasKey(key) == false)\n  {\n    return defaultValue;\n  }\n  const QVariant v = get(key);\n  _checkConvert(key, v, QVariant::LongLong);\n  return v.toLongLong();\n}\n\nlong Settings::getLong(const QString& key, long defaultValue, long min, long max) const\n{\n  long retVal = getLong(key, defaultValue);\n\n  if ( retVal < min )\n  {\n    retVal = min;\n  }\n  else if ( retVal > max )\n  {\n    retVal = max;\n  }\n\n  return retVal;\n}\n\n\nQStringList Settings::getList(const QString& key) const\n{\n  QString str = getString(key);\n\n  return str.split(\";\");\n}\n\nQStringList Settings::getList(const QString& key, const QString& defaultValue) const\n{\n  QString str = getString(key, defaultValue);\n\n  return str.split(\";\", QString::SkipEmptyParts);\n}\n\nQStringList Settings::getList(const QString& key, const QStringList& defaultValue) const\n{\n  QStringList result;\n\n  if (hasKey(key))\n  {\n    QString str = getString(key);\n    result = str.split(\";\", QString::SkipEmptyParts);\n  }\n  else\n  {\n    for (int i = 0; i < defaultValue.size(); i++)\n    {\n      result.append(_replaceVariablesValue(defaultValue[i]));\n    }\n  }\n  return result;\n}\n\nQString Settings::getString(const QString& key) const\n{\n  if (hasKey(key) == false)\n  {\n    throw HootException(\"Error finding key: \" + key);\n  }\n  return get(key).toString();\n}\n\nQString Settings::getString(const QString& key, const QString& defaultValue) const\n{\n  if (hasKey(key) == false)\n  {\n    std::set<QString> used;\n    used.insert(key);\n    return _replaceVariablesValue(defaultValue, used);\n  }\n  return getString(key);\n}\n\nQVariant Settings::getValue(const QString& value) const\n{\n  return QVariant(_replaceVariablesValue(value));\n}\n\nvoid Settings::loadEnvironment()\n{\n  LOG_DEBUG(\"Loading environment...\");\n  for (int n = 0; environ[n]; n++)\n  {\n    QString e = environ[n];\n    int i = e.indexOf(\"=\");\n    QString k = e.mid(0, i);\n    QString v = e.mid(i + 1);\n    set(k, v);\n  }\n}\n\nvoid Settings::loadDefaults()\n{\n  ConfigDefaults::populateDefaults(*this);\n  QString path = ConfPath::search(\"hoot.json\");\n  loadEnvironment();\n  loadJson(path);\n  try\n  {\n    QString localPath = ConfPath::search(\"LocalHoot.json\");\n    loadJson(localPath);\n  }\n  catch (const FileNotFoundException&)\n  {\n    // pass\n  }\n}\n\nvoid Settings::loadFromString(QString json)\n{\n  JsonLoader l(this);\n  l.loadFromString(json);\n}\n\nvoid Settings::loadJson(QString path)\n{\n  JsonLoader l(this);\n  l.load(ConfPath::search(path));\n}\n\nvoid Settings::parseCommonArguments(QStringList& args)\n{\n  LOG_DEBUG(\"Parsing command arguments...\");\n\n  bool foundOne = true;\n\n  QStringList hootTestCmdsIgnore;\n  hootTestCmdsIgnore.append(\"--quick\");\n  hootTestCmdsIgnore.append(\"--slow\");\n  hootTestCmdsIgnore.append(\"--glacial\");\n  hootTestCmdsIgnore.append(\"--all\");\n  hootTestCmdsIgnore.append(\"--quick-only\");\n  hootTestCmdsIgnore.append(\"--slow-only\");\n  hootTestCmdsIgnore.append(\"--glacial-only\");\n  hootTestCmdsIgnore.append(\"--single\");\n  hootTestCmdsIgnore.append(\"--names\");\n  hootTestCmdsIgnore.append(\"--all-names\");\n  hootTestCmdsIgnore.append(\"--diff\");\n\n  const QString optionInputFormatErrorMsg =\n    \"define must takes the form key=value (or key+=value, key++=value, or key-=value).\";\n\n  while (args.size() > 0 && foundOne)\n  {\n    if (args[0] == \"--conf\" || args[0] == \"-C\")\n    {\n      if (args.size() < 2)\n      {\n        throw HootException(\"--conf must be followed by a file name.\");\n      }\n      conf().loadJson(args[1]);\n      // move on to the next argument.\n      args = args.mid(2);\n    }\n    else if (args[0] == \"--trace\")\n    {\n      Log::getInstance().setLevel(Log::Trace);\n      args = args.mid(1);\n    }\n    else if (args[0] == \"--debug\")\n    {\n      Log::getInstance().setLevel(Log::Debug);\n      args = args.mid(1);\n    }\n    else if (args[0] == \"--verbose\")\n    {\n      Log::getInstance().setLevel(Log::Verbose);\n      args = args.mid(1);\n    }\n    else if (args[0] == \"--info\")\n    {\n      Log::getInstance().setLevel(Log::Info);\n      args = args.mid(1);\n    }\n    else if (args[0] == \"--warn\")\n    {\n      Log::getInstance().setLevel(Log::Warn);\n      args = args.mid(1);\n    }\n    else if (args[0] == \"--error\")\n    {\n      Log::getInstance().setLevel(Log::Error);\n      args = args.mid(1);\n    }\n    else if (args[0] == \"--fatal\")\n    {\n      Log::getInstance().setLevel(Log::Fatal);\n      args = args.mid(1);\n    }\n    //HootTest settings have already been parsed by this point\n    else if (hootTestCmdsIgnore.contains(args[0]) || args[0].contains(\"--include\") ||\n             args[0].contains(\"--exclude\"))\n    {\n      args = args.mid(1);\n    }\n    else if (args[0] == \"--define\" || args[0] == \"-D\")\n    {\n      if (args.size() < 2)\n      {\n        throw HootException(optionInputFormatErrorMsg);\n      }\n      QString kv = args[1];\n      QStringList kvl = kv.split(\"++=\");\n      bool append = false;\n      bool remove = false;\n      bool prepend = true;\n      if (kvl.size() != 2)\n      {\n        // split on the first '+='\n        int sep = kv.indexOf(\"+=\");\n        kvl.clear();\n        if (sep != -1)\n        {\n          kvl << kv.mid(0, sep);\n          kvl << kv.mid(sep + 2);\n        }\n        append = true;\n        remove = false;\n        prepend = false;\n      }\n      if (kvl.size() != 2)\n      {\n        // split on the first '-='\n        int sep = kv.indexOf(\"-=\");\n        kvl.clear();\n        if (sep != -1)\n        {\n          kvl << kv.mid(0, sep);\n          kvl << kv.mid(sep + 2);\n        }\n        append = false;\n        remove = true;\n        prepend = false;\n      }\n      if (kvl.size() != 2)\n      {\n        // split on the first '='\n        int sep = kv.indexOf('=');\n        kvl.clear();\n        if (sep != -1)\n        {\n          kvl << kv.mid(0, sep);\n          kvl << kv.mid(sep + 1);\n        }\n        append = false;\n        remove = false;\n        prepend = false;\n      }\n      if (kvl.size() != 2)\n      {\n        throw HootException(optionInputFormatErrorMsg);\n      }\n      if (!conf().hasKey(kvl[0]))\n      {\n        throw HootException(\"Unknown settings option: (\" + kvl[0] + \")\");\n      }\n      if (append)\n      {\n        QStringList values = kvl[1].split(\";\", QString::SkipEmptyParts);\n        conf().append(kvl[0], values);\n      }\n      else if (remove)\n      {\n        QStringList values = kvl[1].split(\";\", QString::SkipEmptyParts);\n        foreach (QString v, values)\n        {\n          QStringList newList = conf().getList(kvl[0]);\n          if( !newList.contains(v))\n          {\n            throw HootException(\"Unknown default value: (\" + v + \")\");\n          }\n          newList.removeAll(v);\n          conf().set(kvl[0], newList);\n        }\n      }\n      else if (prepend)\n      {\n        QStringList values = kvl[1].split(\";\", QString::SkipEmptyParts);\n        conf().prepend(kvl[0], values);\n      }\n      else\n      {\n        conf().set(kvl[0], kvl[1]);\n      }\n      // move on to the next argument.\n      args = args.mid(2);\n    }\n    else\n    {\n      foundOne = false;\n    }\n  }\n  // re-initialize the logger and other resources after the settings have been parsed.\n  Hoot::getInstance().reinit();\n}\n\nQString Settings::_replaceVariables(const QString& key, std::set<QString> used) const\n{\n  if (used.find(key) != used.end())\n  {\n    throw HootException(\"Recursive key in configuration file. (\" + key + \")\");\n  }\n  // if the variable doesn't exist then it defaults to an empty string.\n  if (_settings.contains(key) == false)\n  {\n    return \"\";\n  }\n  QString value = _settings.value(key).toString();\n\n  used.insert(key);\n  return _replaceVariablesValue(value, used);\n}\n\nQString Settings::_replaceStaticVariables(QString value) const\n{\n  bool done = false;\n\n  while (!done)\n  {\n    done = true;\n    int offset = 0;\n    if (_staticRegex.indexIn(value, offset) >= 0)\n    {\n      offset += _staticRegex.matchedLength();\n      QString varStr = _staticRegex.capturedTexts()[0];\n      QString subKey = _staticRegex.capturedTexts()[1];\n      QString expanded;\n      if (hasKey(subKey))\n      {\n        expanded = getString(subKey);\n      }\n      value.replace(varStr, expanded);\n      done = false;\n    }\n  }\n\n  return value;\n}\n\nQString Settings::_replaceVariablesValue(QString value) const\n{\n  std::set<QString> used;\n  return _replaceVariablesValue(value, used);\n}\n\nQString Settings::_replaceVariablesValue(QString value, std::set<QString> used) const\n{\n  bool done = false;\n\n  while (!done)\n  {\n    done = true;\n    int offset = 0;\n    if (_dynamicRegex.indexIn(value, offset) >= 0)\n    {\n      offset += _dynamicRegex.matchedLength();\n      QString varStr = _dynamicRegex.capturedTexts()[0];\n      QString subKey = _dynamicRegex.capturedTexts()[1];\n      QString expanded = _replaceVariables(subKey, used);\n      value.replace(varStr, expanded);\n      done = false;\n    }\n  }\n\n  return value;\n}\n\nvoid Settings::set(const QString& key, const QStringList &value)\n{\n  set(key, value.join(\";\"));\n}\n\nvoid Settings::set(const QString& key, const QString& value)\n{\n  _settings[key] = _replaceStaticVariables(value);\n}\n\nvoid Settings::storeJson(const QString& path) const\n{\n  fstream os(path.toUtf8().constData(), ios::out);\n\n  if (os.good() == false)\n  {\n    throw HootException(QString(\"Error opening %1 for writing.\").arg(path));\n  }\n\n  os << toString().toUtf8().constData();\n}\n\nQString Settings::toString() const\n{\n  QString result = \"{\\n\";\n\n  for (SettingsMap::const_iterator it = _settings.constBegin(); it != _settings.constEnd(); ++it)\n  {\n    result += QString(\"  \\\"%1\\\":\\\"%2\\\",\\n\").arg(_markup(it.key()))\n        .arg(_markup(it.value().toString()));\n  }\n  result += \"  \\\"#end\\\": \\\"\\\"\\n\";\n\n  result += \"}\\n\";\n\n  return result;\n}\n\n}\n", "hunk": "@@ -594,7 +594,7 @@ void Settings::parseCommonArguments(QStringList& args)\n         foreach (QString v, values)\n         {\n           QStringList newList = conf().getList(kvl[0]);\n-          if( !newList.contains(v))\n+          if (!newList.contains(v))\n           {\n             throw HootException(\"Unknown default value: (\" + v + \")\");\n           }\n", "comment": "just formatting: `if (!newList.contains(v))`", "ids": ["42003", "0f793ecb5c7f06d34ece379ce1ab7ce182f3987a", "22dacd573fce6ab6224f60b324a463958950e459"], "repo": "ngageoint/hootenanny", "ghid": 2909, "old": "         foreach (QString v, values)\n         {\n           QStringList newList = conf().getList(kvl[0]);\n-          if( !newList.contains(v))\n           {\n             throw HootException(\"Unknown default value: (\" + v + \")\");\n           }", "new": "         foreach (QString v, values)\n         {\n           QStringList newList = conf().getList(kvl[0]);\n+          if (!newList.contains(v))\n           {\n             throw HootException(\"Unknown default value: (\" + v + \")\");\n           }", "lang": "cpp", "norm_lang": "cpp"}
{"old_hunk": "@@ -135,4 +146,19 @@ module.exports = class Generator {\n       return id === _id || id.replace(prefixRE, '') === _id\n     })\n   }\n+\n+  printExitLogs () {\n+    if (this.exitLogs.length) {\n+      this.exitLogs.forEach(({ id, msg, type }) => {\n+        const shortId = id.replace('@vue/cli-plugin-', '').replace('vue-cli-plugin-', '')", "oldf": "const ejs = require('ejs')\nconst slash = require('slash')\nconst debug = require('debug')\nconst GeneratorAPI = require('./GeneratorAPI')\nconst sortObject = require('./util/sortObject')\nconst writeFileTree = require('./util/writeFileTree')\nconst configTransforms = require('./util/configTransforms')\nconst logger = require('@vue/cli-shared-utils/lib/logger')\n\nconst logTypes = {\n  log: logger.log,\n  info: logger.info,\n  done: logger.done,\n  warn: logger.warn,\n  error: logger.error\n}\n\nmodule.exports = class Generator {\n  constructor (context, pkg, plugins, completeCbs = []) {\n    this.context = context\n    this.plugins = plugins\n    this.originalPkg = pkg\n    this.pkg = Object.assign({}, pkg)\n    this.completeCbs = completeCbs\n\n    // for conflict resolution\n    this.depSources = {}\n    // virtual file tree\n    this.files = {}\n    this.fileMiddlewares = []\n    this.postProcessFilesCbs = []\n    // exit messages\n    this.exitLogs = []\n\n    const cliService = plugins.find(p => p.id === '@vue/cli-service')\n    const rootOptions = cliService && cliService.options\n    // apply generators from plugins\n    plugins.forEach(({ id, apply, options }) => {\n      const api = new GeneratorAPI(id, this, options, rootOptions || {})\n      apply(api, options, rootOptions)\n    })\n  }\n\n  async generate ({\n    extractConfigFiles = false,\n    checkExisting = false\n  } = {}) {\n    // extract configs from package.json into dedicated files.\n    this.extractConfigFiles(extractConfigFiles, checkExisting)\n    // wait for file resolve\n    await this.resolveFiles()\n    // set package.json\n    this.sortPkg()\n    this.files['package.json'] = JSON.stringify(this.pkg, null, 2)\n    // write file tree to disk\n    await writeFileTree(this.context, this.files)\n  }\n\n  extractConfigFiles (extractAll, checkExisting) {\n    const extract = key => {\n      if (\n        configTransforms[key] &&\n        this.pkg[key] &&\n        // do not extract if the field exists in original package.json\n        !this.originalPkg[key]\n      ) {\n        const value = this.pkg[key]\n        const transform = configTransforms[key]\n        const res = transform(\n          value,\n          checkExisting,\n          this.context\n        )\n        const { content, filename } = res\n        this.files[filename] = content\n        delete this.pkg[key]\n      }\n    }\n    if (extractAll) {\n      for (const key in this.pkg) {\n        extract(key)\n      }\n    } else if (!process.env.VUE_CLI_TEST) {\n      // by default, always extract vue.config.js\n      extract('vue')\n    }\n  }\n\n  sortPkg () {\n    // ensure package.json keys has readable order\n    this.pkg.dependencies = sortObject(this.pkg.dependencies)\n    this.pkg.devDependencies = sortObject(this.pkg.devDependencies)\n    this.pkg.scripts = sortObject(this.pkg.scripts, [\n      'serve',\n      'build',\n      'test',\n      'e2e',\n      'lint',\n      'deploy'\n    ])\n    this.pkg = sortObject(this.pkg, [\n      'name',\n      'version',\n      'private',\n      'scripts',\n      'dependencies',\n      'devDependencies',\n      'vue',\n      'babel',\n      'eslintConfig',\n      'prettier',\n      'postcss',\n      'browserslist',\n      'jest'\n    ])\n\n    debug('vue:cli-pkg')(this.pkg)\n  }\n\n  async resolveFiles () {\n    const files = this.files\n    for (const middleware of this.fileMiddlewares) {\n      await middleware(files, ejs.render)\n    }\n    // normalize paths\n    Object.keys(files).forEach(file => {\n      const normalized = slash(file)\n      if (file !== normalized) {\n        files[normalized] = files[file]\n        delete files[file]\n      }\n    })\n    for (const postProcess of this.postProcessFilesCbs) {\n      await postProcess(files)\n    }\n    debug('vue:cli-files')(this.files)\n  }\n\n  hasPlugin (_id) {\n    const prefixRE = /^(@vue\\/|vue-)cli-plugin-/\n    return [\n      ...this.plugins.map(p => p.id),\n      ...Object.keys(this.pkg.devDependencies || {}),\n      ...Object.keys(this.pkg.dependencies || {})\n    ].some(id => {\n      return id === _id || id.replace(prefixRE, '') === _id\n    })\n  }\n\n  printExitLogs () {\n    if (this.exitLogs.length) {\n      this.exitLogs.forEach(({ id, msg, type }) => {\n        const shortId = id.replace('@vue/cli-plugin-', '').replace('vue-cli-plugin-', '')\n        const logFn = logTypes[type]\n        if (!logFn) {\n          logger.error(`Invalid api.exitLog type '${type}'.`, shortId)\n        } else {\n          logFn(msg, msg && shortId)\n        }\n      })\n      logger.log()\n    }\n  }\n}\n", "hunk": "@@ -150,7 +150,7 @@ module.exports = class Generator {\n   printExitLogs () {\n     if (this.exitLogs.length) {\n       this.exitLogs.forEach(({ id, msg, type }) => {\n-        const shortId = id.replace('@vue/cli-plugin-', '').replace('vue-cli-plugin-', '')\n+        const shortId = toShortId(id)\n         const logFn = logTypes[type]\n         if (!logFn) {\n           logger.error(`Invalid api.exitLog type '${type}'.`, shortId)\n", "comment": "We probably should extract the shortId logic into a util function since it's used in a few other places as well. There's a request regarding custom scoped cli plugins (#908) so the regex may need to be adjusted, it's better to keep it in a single place.", "ids": ["8366", "1ec3ead73ba659df1fcec73c03b84e15d292540d", "54d7209369540e241aa5e28e58b997efa525d168"], "repo": "vuejs/vue-cli", "ghid": 935, "old": "   printExitLogs () {\n     if (this.exitLogs.length) {\n       this.exitLogs.forEach(({ id, msg, type }) => {\n-        const shortId = id.replace('@vue/cli-plugin-', '').replace('vue-cli-plugin-', '')\n         const logFn = logTypes[type]\n         if (!logFn) {\n           logger.error(`Invalid api.exitLog type '${type}'.`, shortId)", "new": "   printExitLogs () {\n     if (this.exitLogs.length) {\n       this.exitLogs.forEach(({ id, msg, type }) => {\n+        const shortId = toShortId(id)\n         const logFn = logTypes[type]\n         if (!logFn) {\n           logger.error(`Invalid api.exitLog type '${type}'.`, shortId)", "lang": "js", "norm_lang": "javascript"}
{"old_hunk": "@@ -141,6 +137,13 @@ def download_system_symbols_if_needed(symbols_directory):\n \n   build_params_check_path = os.path.join(symbols_directory,", "oldf": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Used to download Android symbols.\"\"\"\n\nimport os\n\nfrom clusterfuzz._internal.base import utils\nfrom clusterfuzz._internal.google_cloud_utils import storage\nfrom clusterfuzz._internal.metrics import logs\nfrom clusterfuzz._internal.platforms.android import adb\nfrom clusterfuzz._internal.platforms.android import fetch_artifact\nfrom clusterfuzz._internal.platforms.android import kernel_utils\nfrom clusterfuzz._internal.platforms.android import settings\nfrom clusterfuzz._internal.system import archive\nfrom clusterfuzz._internal.system import environment\nfrom clusterfuzz._internal.system import shell\n\n\ndef get_repo_prop_archive_filename(build_id, target):\n  return f'{target}-{build_id}-repo.prop'\n\n\ndef should_download_symbols():\n  \"\"\"Return True if we should continue to download symbols.\"\"\"\n  # For local testing or when running the reproduce tool locally, we do not\n  # have access to the cloud storage bucket with the symbols. In this case,\n  # just bail out.\n  # We have archived symbols for google builds only.\n  return (not environment.get_value('LOCAL_DEVELOPMENT') and\n          not environment.get_value('REPRODUCE_TOOL') and\n          settings.is_google_device())\n\n\ndef download_artifact_if_needed(\n    build_id, artifact_directory, artifact_archive_path,\n    targets_with_type_and_san, artifact_file_name, output_filename_override):\n  \"\"\"Downloads artifact to actifacts_archive_path if needed\"\"\"\n  # Delete existing symbols directory first.\n  shell.remove_directory(artifact_directory, recreate=True)\n\n  # Fetch symbol file from cloud storage cache (if available).\n  found_in_cache = storage.get_file_from_cache_if_exists(\n      artifact_archive_path, update_modification_time_on_access=False)\n  if not found_in_cache:\n    for target_with_type_and_san in targets_with_type_and_san:\n      # Fetch the artifact now.\n      fetch_artifact.get(build_id, target_with_type_and_san, artifact_file_name,\n                         artifact_directory, output_filename_override)\n      if os.path.exists(artifact_archive_path):\n        break\n\n\ndef download_repo_prop_if_needed(symbols_directory, build_id, cache_target,\n                                 targets_with_type_and_san, cache_type):\n  \"\"\"Downloads the repo.prop for a branch\"\"\"\n  artifact_file_name = 'repo.prop'\n  symbols_archive_filename = get_repo_prop_archive_filename(\n      build_id, cache_target)\n  output_filename_override = symbols_archive_filename\n  # We create our own build_params for cache\n  build_params = {\n      'build_id': build_id,\n      'target': cache_target,\n      'type': cache_type\n  }\n\n  build_params_check_path = os.path.join(symbols_directory,\n                                         '.cached_build_params')\n  # Check if we already have the symbols locally.\n  cached_build_params = utils.read_data_from_file(\n      build_params_check_path, eval_data=True)\n  if cached_build_params and cached_build_params == build_params:\n    return\n\n  symbols_archive_path = os.path.join(symbols_directory,\n                                      symbols_archive_filename)\n  download_artifact_if_needed(build_id, symbols_directory, symbols_archive_path,\n                              targets_with_type_and_san, artifact_file_name,\n                              output_filename_override)\n  if not os.path.exists(symbols_archive_path):\n    logs.log_error('Unable to locate repo.prop %s.' % symbols_archive_path)\n    return\n\n  # Store the artifact for later use or for use by other bots.\n  storage.store_file_in_cache(symbols_archive_path)\n  utils.write_data_to_file(build_params, build_params_check_path)\n\n\ndef download_kernel_repo_prop_if_needed(symbols_directory):\n  \"\"\"Downloads the repo.prop for the kernel of a device\"\"\"\n  if not should_download_symbols():\n    return\n\n  # For Android kernel we want to get the repro.prop\n  # Note: kasan and non-kasan kernel should have the same repo.prop for a given\n  # build_id.\n  _, build_id = kernel_utils.get_kernel_hash_and_build_id()\n  target = kernel_utils.get_kernel_name()\n  if not build_id or not target:\n    logs.log_error('Could not get kernel parameters, exiting.')\n    return\n\n  tool_suffix = environment.get_value('SANITIZER_TOOL_NAME')\n  # Some kernels are just 'kernel', some are kernel_target\n  if tool_suffix:\n    targets_with_type_and_san = [\n        f'kernel_{tool_suffix}', f'kernel_{tool_suffix}_{target}'\n    ]\n  else:\n    targets_with_type_and_san = ['kernel', f'kernel_{target}']\n\n  download_repo_prop_if_needed(symbols_directory, build_id, target,\n                               targets_with_type_and_san, 'kernel')\n\n\ndef download_system_symbols_if_needed(symbols_directory):\n  \"\"\"Download system libraries from |SYMBOLS_URL| and cache locally.\"\"\"\n  if not should_download_symbols():\n    return\n\n  # Get the build fingerprint parameters.\n  build_params = settings.get_build_parameters()\n  if not build_params:\n    logs.log_error('Unable to determine build parameters.')\n    return\n\n  build_params_check_path = os.path.join(symbols_directory,\n                                         '.cached_build_params')\n  # Check if we already have the symbols locally.\n  cached_build_params = utils.read_data_from_file(\n      build_params_check_path, eval_data=True)\n  if cached_build_params and cached_build_params == build_params:\n    # No work to do, same system symbols already in local.\n    return\n\n  build_id = build_params.get('build_id')\n  target = build_params.get('target')\n  build_type = build_params.get('type')\n  if not build_id or not target or not build_type:\n    logs.log_error('Null build parameters found, exiting.')\n    return\n\n  symbols_archive_filename = f'{target}-symbols-{build_id}.zip'\n  artifact_file_name = symbols_archive_filename\n  output_filename_override = None\n\n  # Include type and sanitizer information in the target.\n  tool_suffix = environment.get_value('SANITIZER_TOOL_NAME')\n  target_with_type_and_san = f'{target}-{build_type}'\n  if tool_suffix and not tool_suffix in target_with_type_and_san:\n    target_with_type_and_san += f'_{tool_suffix}'\n\n  targets_with_type_and_san = [target_with_type_and_san]\n\n  symbols_archive_path = os.path.join(symbols_directory,\n                                      symbols_archive_filename)\n  download_artifact_if_needed(build_id, symbols_directory, symbols_archive_path,\n                              targets_with_type_and_san, artifact_file_name,\n                              output_filename_override)\n  if not os.path.exists(symbols_archive_path):\n    logs.log_error(\n        'Unable to locate symbols archive %s.' % symbols_archive_path)\n    return\n\n  # Store the artifact for later use or for use by other bots.\n  storage.store_file_in_cache(symbols_archive_path)\n\n  archive.unpack(symbols_archive_path, symbols_directory, trusted=True)\n  shell.remove_file(symbols_archive_path)\n\n  utils.write_data_to_file(build_params, build_params_check_path)\n\n\ndef _get_binary_from_build_or_device(binary_path):\n  \"\"\"Look for binary on build server or on device.\"\"\"\n  # Initialize some helper variables.\n  symbols_directory = environment.get_value('SYMBOLS_DIR')\n  binary_filename = os.path.basename(binary_path)\n\n  # We didn't find the library locally in the build directory.\n  # Try finding the library in the local system library cache.\n  download_system_symbols_if_needed(symbols_directory)\n  local_binary_path = utils.find_binary_path(symbols_directory, binary_path)\n  if local_binary_path:\n    return local_binary_path\n\n  # Try pulling in the binary directly from the device into the\n  # system library cache directory.\n  local_binary_path = os.path.join(symbols_directory, binary_filename)\n  adb.run_command('pull %s %s' % (binary_path, local_binary_path))\n  if os.path.exists(local_binary_path):\n    return local_binary_path\n\n  return None\n\n\ndef filter_binary_path(binary_path):\n  \"\"\"Filter binary path to provide local copy.\"\"\"\n  # LKL fuzzer name is not full path.\n  if environment.is_android():\n    # Skip symbolization when running it on bad entries like [stack:XYZ].\n    if not binary_path.startswith('/') or '(deleted)' in binary_path:\n      return ''\n\n  # Initialize some helper variables.\n  build_directory = environment.get_value('BUILD_DIR')\n\n  # Try to find the library in the build directory first.\n  local_binary_path = utils.find_binary_path(build_directory, binary_path)\n  if local_binary_path:\n    return local_binary_path\n\n  # We should only download from the build server if we are Android.\n  if environment.is_android():\n    local_binary_path = _get_binary_from_build_or_device(binary_path)\n    if local_binary_path:\n      return local_binary_path\n\n  # Unable to find library.\n  logs.log_error('Unable to find library %s for symbolization.' % binary_path)\n  return ''\n", "hunk": "@@ -135,13 +135,7 @@ def download_system_symbols_if_needed(symbols_directory):\n     logs.log_error('Unable to determine build parameters.')\n     return\n \n-  build_params_check_path = os.path.join(symbols_directory,\n-                                         '.cached_build_params')\n-  # Check if we already have the symbols locally.\n-  cached_build_params = utils.read_data_from_file(\n-      build_params_check_path, eval_data=True)\n-  if cached_build_params and cached_build_params == build_params:\n-    # No work to do, same system symbols already in local.\n+  if check_symbols_cached(symbols_directory, build_params):\n     return\n \n   build_id = build_params.get('build_id')\n", "comment": "Can you add a helper for these rather than duplicate code? i.e. roughly: ```python def check_symbols_cached(symbols_directory, build_params): build_params_check_path = os.path.join(symbols_directory, ...) cached_build_params = ... return cached_build_params and cached_build_params == build_params) ```", "ids": ["16994", "068b877a06ce05a2674240f45171db9219073121", "00417498cf479b091374336001344c0010929f77"], "repo": "google/clusterfuzz", "ghid": 2496, "old": "     logs.log_error('Unable to determine build parameters.')\n     return\n-  build_params_check_path = os.path.join(symbols_directory,\n-                                         '.cached_build_params')\n-  # Check if we already have the symbols locally.\n-  cached_build_params = utils.read_data_from_file(\n-      build_params_check_path, eval_data=True)\n-  if cached_build_params and cached_build_params == build_params:\n-    # No work to do, same system symbols already in local.\n     return\n   build_id = build_params.get('build_id')", "new": "     logs.log_error('Unable to determine build parameters.')\n     return\n+  if check_symbols_cached(symbols_directory, build_params):\n     return\n   build_id = build_params.get('build_id')", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -328,9 +328,9 @@ TEST_F(TransferAsset, BigPrecision) {\n   const std::string kNewAssetId =\n       kNewAsset + \"#\" + IntegrationTestFramework::kDefaultDomain;\n   const auto kPrecision = 5;\n-  const std::string kInitial = \"500\";\n-  const std::string kForTransfer = \"1\";\n-  const std::string kLeft = \"499\";\n+  const std::string kInitial = \"500.00000\";", "oldf": "/**\n * Copyright Soramitsu Co., Ltd. All Rights Reserved.\n * SPDX-License-Identifier: Apache-2.0\n */\n\n#include <gtest/gtest.h>\n#include \"acceptance_fixture.hpp\"\n#include \"backend/protobuf/transaction.hpp\"\n#include \"builders/protobuf/queries.hpp\"\n#include \"builders/protobuf/transaction.hpp\"\n#include \"cryptography/crypto_provider/crypto_defaults.hpp\"\n#include \"framework/integration_framework/integration_test_framework.hpp\"\n#include \"framework/specified_visitor.hpp\"\n#include \"utils/query_error_response_visitor.hpp\"\n\nusing namespace integration_framework;\nusing namespace shared_model;\n\nclass TransferAsset : public AcceptanceFixture {\n public:\n  /**\n   * Creates the transaction with the first user creation commands\n   * @param perms are the permissions of the user\n   * @return built tx\n   */\n  auto makeFirstUser(const interface::RolePermissionSet &perms = {\n                         interface::permissions::Role::kTransfer}) {\n    auto new_perms = perms;\n    new_perms.set(interface::permissions::Role::kAddAssetQty);\n    const std::string kRole1 = \"roleone\";\n    return AcceptanceFixture::makeUserWithPerms(kRole1, new_perms);\n  }\n\n  /**\n   * Creates the transaction with the second user creation commands\n   * @param perms are the permissions of the user\n   * @return built tx\n   */\n  auto makeSecondUser(const interface::RolePermissionSet &perms = {\n                          interface::permissions::Role::kReceive}) {\n    return createUserWithPerms(kUser2, kUser2Keypair.publicKey(), kRole2, perms)\n        .build()\n        .signAndAddSignature(kAdminKeypair)\n        .finish();\n  }\n\n  proto::Transaction addAssets() {\n    return addAssets(kAmount);\n  }\n\n  proto::Transaction addAssets(const std::string &amount) {\n    return complete(baseTx().addAssetQuantity(kAsset, amount));\n  }\n\n  proto::Transaction makeTransfer(const std::string &amount) {\n    return complete(\n        baseTx().transferAsset(kUserId, kUser2Id, kAsset, kDesc, amount));\n  }\n\n  proto::Transaction makeTransfer() {\n    return makeTransfer(kAmount);\n  }\n\n  const std::string kAmount = \"1.0\";\n  const std::string kDesc = \"description\";\n  const std::string kRole2 = \"roletwo\";\n  const std::string kUser2 = \"usertwo\";\n  const std::string kUser2Id = kUser2 + \"@test\";\n  const crypto::Keypair kUser2Keypair =\n      crypto::DefaultCryptoAlgorithmType::generateKeypair();\n};\n\n#define check(i) [](auto &block) { ASSERT_EQ(block->transactions().size(), i); }\n\n/**\n * @given pair of users with all required permissions\n * @when execute tx with TransferAsset command\n * @then there is the tx in proposal\n */\nTEST_F(TransferAsset, Basic) {\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser(), check(1))\n      .sendTxAwait(makeSecondUser(), check(1))\n      .sendTxAwait(addAssets(), check(1))\n      .sendTxAwait(makeTransfer(), check(1))\n      .done();\n}\n\n/**\n * @given pair of users\n *        AND the first user without can_transfer permission\n * @when execute tx with TransferAsset command\n * @then there is an empty proposal\n */\nTEST_F(TransferAsset, WithoutCanTransfer) {\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser({}), check(1))\n      .sendTxAwait(makeSecondUser(), check(1))\n      .sendTxAwait(addAssets(), check(1))\n      .sendTxAwait(makeTransfer(), check(0))\n      .done();\n}\n\n/**\n * @given pair of users\n *        AND the second user without can_receive permission\n * @when execute tx with TransferAsset command\n * @then there is an empty proposal\n */\nTEST_F(TransferAsset, WithoutCanReceive) {\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser(), check(1))\n      // TODO(@l4l) 23/06/18: remove permission with IR-1367\n      .sendTxAwait(makeSecondUser({interface::permissions::Role::kAddPeer}),\n                   check(1))\n      .sendTxAwait(addAssets(), check(1))\n      .sendTxAwait(makeTransfer(), check(0))\n      .done();\n}\n\n/**\n * @given some user with all required permissions\n * @when execute tx with TransferAsset command to nonexistent destination\n * @then there is an empty proposal\n */\nTEST_F(TransferAsset, NonexistentDest) {\n  std::string nonexistent = \"inexist@test\";\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser(), check(1))\n      .sendTxAwait(addAssets(), check(1))\n      .sendTxAwait(complete(baseTx().transferAsset(\n                       kUserId, nonexistent, kAsset, kDesc, kAmount)),\n                   check(0))\n      .done();\n}\n\n/**\n * @given pair of users with all required permissions\n * @when execute tx with TransferAsset command with nonexistent asset\n * @then there is an empty proposal\n */\nTEST_F(TransferAsset, NonexistentAsset) {\n  std::string nonexistent = \"inexist#test\";\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser(), check(1))\n      .sendTxAwait(makeSecondUser(), check(1))\n      .sendTxAwait(addAssets(), check(1))\n      .sendTxAwait(complete(baseTx().transferAsset(\n                       kUserId, kUser2Id, nonexistent, kDesc, kAmount)),\n                   check(0))\n      .done();\n}\n\n/**\n * @given pair of users with all required permissions\n * @when execute tx with TransferAsset command with negative amount\n * @then the tx hasn't passed stateless validation\n *       (aka skipProposal throws)\n */\nTEST_F(TransferAsset, NegativeAmount) {\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser(), check(1))\n      .sendTxAwait(makeSecondUser(), check(1))\n      .sendTxAwait(addAssets(), check(1))\n      .sendTx(makeTransfer(\"-1.0\"), checkStatelessInvalid)\n      .done();\n}\n\n/**\n * @given pair of users with all required permissions\n * @when execute tx with TransferAsset command with zero amount\n * @then the tx hasn't passed stateless validation\n *       (aka skipProposal throws)\n */\nTEST_F(TransferAsset, ZeroAmount) {\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser(), check(1))\n      .sendTxAwait(makeSecondUser(), check(1))\n      .sendTxAwait(addAssets(), check(1))\n      .sendTx(makeTransfer(\"0.0\"), checkStatelessInvalid)\n      .done();\n}\n\n/**\n * @given pair of users with all required permissions\n * @when execute tx with TransferAsset command with empty-str description\n * @then it passed to the proposal\n */\nTEST_F(TransferAsset, EmptyDesc) {\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser(), check(1))\n      .sendTxAwait(makeSecondUser(), check(1))\n      .sendTxAwait(addAssets(), check(1))\n      .sendTxAwait(complete(baseTx().transferAsset(\n                       kUserId, kUser2Id, kAsset, \"\", kAmount)),\n                   check(1))\n      .done();\n}\n\n/**\n * @given pair of users with all required permissions\n * @when execute tx with TransferAsset command with very long description\n * @then the tx hasn't passed stateless validation\n *       (aka skipProposal throws)\n */\nTEST_F(TransferAsset, LongDesc) {\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser(), check(1))\n      .sendTxAwait(makeSecondUser(), check(1))\n      .sendTxAwait(addAssets(), check(1))\n      .sendTx(\n          complete(baseTx().transferAsset(\n              kUserId, kUser2Id, kAsset, std::string(100000, 'a'), kAmount)),\n          checkStatelessInvalid)\n      .done();\n}\n\n/**\n * @given pair of users with all required permissions\n * @when execute tx with TransferAsset command with amount more, than user has\n * @then there is an empty proposal\n */\nTEST_F(TransferAsset, MoreThanHas) {\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser(), check(1))\n      .sendTxAwait(makeSecondUser(), check(1))\n      .sendTxAwait(addAssets(\"50.0\"), check(1))\n      .sendTxAwait(makeTransfer(\"100.0\"), check(0))\n      .done();\n}\n\n/**\n * @given pair of users with all required permissions, and tx sender's balance\n * is replenished if required\n * @when execute two txes with TransferAsset command with amount more than a\n * uint256 max half\n * @then first transaction is commited and there is an empty proposal for the\n * second\n */\nTEST_F(TransferAsset, Uint256DestOverflow) {\n  std::string uint256_halfmax =\n      \"723700557733226221397318656304299424082937404160253525246609900049457060\"\n      \"2495.0\";  // 2**252 - 1\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser(), check(1))\n      .sendTxAwait(makeSecondUser(), check(1))\n      .sendTxAwait(addAssets(uint256_halfmax), check(1))\n      // Send first half of the maximum\n      .sendTxAwait(makeTransfer(uint256_halfmax), check(1))\n      // Restore self balance\n      .sendTxAwait(addAssets(uint256_halfmax), check(1))\n      // Send second half of the maximum\n      .sendTxAwait(makeTransfer(uint256_halfmax), check(0))\n      .done();\n}\n\n/**\n * @given some user with all required permissions\n * @when execute tx with TransferAsset command where the source and destination\n * accounts are the same\n * @then the tx hasn't passed stateless validation\n *       (aka skipProposal throws)\n */\nTEST_F(TransferAsset, SourceIsDest) {\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser(), check(1))\n      .sendTxAwait(addAssets(), check(1))\n      .sendTx(complete(baseTx().transferAsset(\n                  kUserId, kUserId, kAsset, kDesc, kAmount)),\n              checkStatelessInvalid);\n}\n\n/**\n * @given some user with all required permission\n * @when execute tx with TransferAsset command where the destination user's\n * domain differ from the source user one\n * @then the tx is commited\n */\nTEST_F(TransferAsset, InterDomain) {\n  const std::string kNewDomain = \"newdom\";\n  const std::string kUser2Id = kUser2 + \"@\" + kNewDomain;\n  const std::string kNewAssetId =\n      IntegrationTestFramework::kAssetName + \"#\" + kNewDomain;\n\n  auto make_second_user =\n      baseTx()\n          .creatorAccountId(IntegrationTestFramework::kAdminId)\n          .createRole(kRole2, {interface::permissions::Role::kReceive})\n          .createDomain(kNewDomain, kRole2)\n          .createAccount(kUser2, kNewDomain, kUser2Keypair.publicKey())\n          .createAsset(IntegrationTestFramework::kAssetName, kNewDomain, 1)\n          .build()\n          .signAndAddSignature(kAdminKeypair)\n          .finish();\n  auto add_assets = complete(baseTx().addAssetQuantity(kNewAssetId, kAmount));\n  auto make_transfer = complete(\n      baseTx().transferAsset(kUserId, kUser2Id, kNewAssetId, kDesc, kAmount));\n\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser(), check(1))\n      .sendTxAwait(make_second_user, check(1))\n      .sendTxAwait(add_assets, check(1))\n      .sendTxAwait(make_transfer, check(1))\n      .done();\n}\n\n/**\n * @given a pair of users with all required permissions\n *        AND asset with big precision\n * @when asset is added and then TransferAsset is called\n * @then txes passed commit and the state as intented\n */\nTEST_F(TransferAsset, BigPrecision) {\n  const std::string kNewAsset = IntegrationTestFramework::kAssetName + \"a\";\n  const std::string kNewAssetId =\n      kNewAsset + \"#\" + IntegrationTestFramework::kDefaultDomain;\n  const auto kPrecision = 5;\n  const std::string kInitial = \"500.00000\";\n  const std::string kForTransfer = \"1.00000\";\n  const std::string kLeft = \"499.00000\";\n\n  auto create_asset =\n      baseTx()\n          .creatorAccountId(\n              integration_framework::IntegrationTestFramework::kAdminId)\n          .createAsset(\n              kNewAsset, IntegrationTestFramework::kDefaultDomain, kPrecision)\n          .build()\n          .signAndAddSignature(kAdminKeypair)\n          .finish();\n  auto add_assets = complete(baseTx().addAssetQuantity(kNewAssetId, kInitial));\n  auto make_transfer = complete(baseTx().transferAsset(\n      kUserId, kUser2Id, kNewAssetId, kDesc, kForTransfer));\n\n  auto check_balance = [](std::string account_id, std::string val) {\n    return [a = std::move(account_id),\n            v = val](auto &resp) {\n      auto &acc_ast = boost::apply_visitor(\n          framework::SpecifiedVisitor<interface::AccountAssetResponse>(),\n          resp.get());\n      for (auto &ast : acc_ast.accountAssets()) {\n        if (ast.accountId() == a) {\n          ASSERT_EQ(v, ast.balance().toStringRepr());\n        }\n      }\n    };\n  };\n\n  auto make_query = [this](std::string account_id) {\n    return baseQry()\n        .creatorAccountId(IntegrationTestFramework::kAdminId)\n        .getAccountAssets(account_id)\n        .build()\n        .signAndAddSignature(kAdminKeypair)\n        .finish();\n  };\n\n  IntegrationTestFramework(1)\n      .setInitialState(kAdminKeypair)\n      .sendTxAwait(makeFirstUser(), check(1))\n      .sendTxAwait(makeSecondUser(), check(1))\n      .sendTxAwait(create_asset, check(1))\n      .sendTxAwait(add_assets, check(1))\n      .sendTxAwait(make_transfer, check(1))\n      .sendQuery(make_query(kUserId), check_balance(kUserId, kLeft))\n      .sendQuery(make_query(kUser2Id), check_balance(kUser2Id, kForTransfer))\n      .done();\n}\n", "hunk": "@@ -328,9 +328,9 @@ TEST_F(TransferAsset, BigPrecision) {\n   const std::string kNewAssetId =\n       kNewAsset + \"#\" + IntegrationTestFramework::kDefaultDomain;\n   const auto kPrecision = 5;\n-  const std::string kInitial = \"500.00000\";\n-  const std::string kForTransfer = \"1.00000\";\n-  const std::string kLeft = \"499.00000\";\n+  const std::string kInitial = \"500\";\n+  const std::string kForTransfer = \"1\";\n+  const std::string kLeft = \"499\";\n \n   auto create_asset =\n       baseTx()\n", "comment": "Please use `db.precision >= command.precision` instead of `=` so that previous value would be valid.", "ids": ["35604", "522152763e5aad167c3e5edfb1853010309e888f", "e932876799bf44c54faac7d0651cc3fb177b2e7b"], "repo": "hyperledger-archives/iroha", "ghid": 1557, "old": "   const std::string kNewAssetId =\n       kNewAsset + \"#\" + IntegrationTestFramework::kDefaultDomain;\n   const auto kPrecision = 5;\n-  const std::string kInitial = \"500.00000\";\n-  const std::string kForTransfer = \"1.00000\";\n-  const std::string kLeft = \"499.00000\";\n   auto create_asset =\n       baseTx()", "new": "   const std::string kNewAssetId =\n       kNewAsset + \"#\" + IntegrationTestFramework::kDefaultDomain;\n   const auto kPrecision = 5;\n+  const std::string kInitial = \"500\";\n+  const std::string kForTransfer = \"1\";\n+  const std::string kLeft = \"499\";\n   auto create_asset =\n       baseTx()", "lang": "cpp", "norm_lang": "cpp"}
{"old_hunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/env python\n+\n+import os\n+import sys\n+from time import time, sleep\n+import random\n+sys.path.append(os.path.dirname(__file__))  # noqa\n+\n+import hypothesistooling as tools\n+\n+\n+if __name__ == '__main__':\n+    last_release = tools.latest_version()\n+\n+    print(\"Current version: %s. Latest released version: %s\" % (\n+        tools.__version__, last_release\n+    ))\n+\n+    start_time = time()\n+\n+    prev_pending = None\n+\n+    while time() <= start_time + 60 * 60:\n+        jobs = tools.build_jobs()\n+        if jobs[\"failed\"]:\n+            print(\"Failing this due to failure of jobs %s\" % (\n+                ', '.join(jobs[\"failed\"]),\n+            ))\n+            sys.exit(1)\n+        else:\n+            pending = jobs[\"pending\"]\n+            pending.remove(\"deploy\")\n+            if pending:\n+                still_pending = set(pending)\n+                if prev_pending is None:\n+                    print(\"Waiting for the following jobs to complete:\")\n+                    for p in sorted(still_pending):\n+                        print(\" * %s\" % (p,))\n+                    print()\n+                else:\n+                    completed = prev_pending - still_pending\n+                    if completed:\n+                        print(\"%s completed since last check.\" % (\n+                            ', '.join(sorted(completed)),))\n+                prev_pending = still_pending\n+                naptime = 10.0 * (2 + random.random())\n+                print(\"Waiting %.2fs for %d more job%s to complete\" % (\n+                    naptime, len(pending), \"s\" if len(pending) > 1 else \"\",))\n+                sleep(naptime)\n+            else:\n+                break\n+    else:\n+        print(\"We've been waiting for an hour. That seems bad. Failing now\")\n+        sys.exit(1)\n+\n+    if not tools.on_master():", "oldf": "#!/usr/bin/env python\n\nimport os\nimport sys\nfrom time import time, sleep\nimport random\nsys.path.append(os.path.dirname(__file__))  # noqa\n\nimport hypothesistooling as tools\n\n\nif __name__ == '__main__':\n    last_release = tools.latest_version()\n\n    print(\"Current version: %s. Latest released version: %s\" % (\n        tools.__version__, last_release\n    ))\n\n    start_time = time()\n\n    prev_pending = None\n\n    while time() <= start_time + 60 * 60:\n        jobs = tools.build_jobs()\n        if jobs[\"failed\"]:\n            print(\"Failing this due to failure of jobs %s\" % (\n                ', '.join(jobs[\"failed\"]),\n            ))\n            sys.exit(1)\n        else:\n            pending = jobs[\"pending\"]\n            pending.remove(\"deploy\")\n            if pending:\n                still_pending = set(pending)\n                if prev_pending is None:\n                    print(\"Waiting for the following jobs to complete:\")\n                    for p in sorted(still_pending):\n                        print(\" * %s\" % (p,))\n                    print()\n                else:\n                    completed = prev_pending - still_pending\n                    if completed:\n                        print(\"%s completed since last check.\" % (\n                            ', '.join(sorted(completed)),))\n                prev_pending = still_pending\n                naptime = 10.0 * (2 + random.random())\n                print(\"Waiting %.2fs for %d more job%s to complete\" % (\n                    naptime, len(pending), \"s\" if len(pending) > 1 else \"\",))\n                sleep(naptime)\n            else:\n                break\n    else:\n        print(\"We've been waiting for an hour. That seems bad. Failing now\")\n        sys.exit(1)\n\n    if not tools.on_master():\n        print(\"Not deploying due to not being on master\")\n        sys.exit(0)\n\n    if not tools.has_source_changes(last_release):\n        print(\"Not deploying due to no source changes\")\n        sys.exit(0)\n\n    print(\"Looks good to release! Pushing the tag now.\")\n    tools.create_tag()\n    sys.exit(0)\n", "hunk": "@@ -15,6 +15,13 @@ if __name__ == '__main__':\n     print(\"Current version: %s. Latest released version: %s\" % (\n         tools.__version__, last_release\n     ))\n+    if not tools.on_master():\n+        print(\"Not deploying due to not being on master\")\n+        sys.exit(0)\n+\n+    if not tools.has_source_changes(last_release):\n+        print(\"Not deploying due to no source changes\")\n+        sys.exit(0)\n \n     start_time = time()\n \n", "comment": "Why don't these checks run first? Won't this just tie up a container in Travis otherwise?", "ids": ["11925", "30ce5445729b8cce23e54aa0149b8e1ad81944fb", "775daabf6e9931311365111ee90d40528410d2e7"], "repo": "HypothesisWorks/hypothesis", "ghid": 540, "old": "     print(\"Current version: %s. Latest released version: %s\" % (\n         tools.__version__, last_release\n     ))\n     start_time = time()", "new": "     print(\"Current version: %s. Latest released version: %s\" % (\n         tools.__version__, last_release\n     ))\n+    if not tools.on_master():\n+        print(\"Not deploying due to not being on master\")\n+        sys.exit(0)\n+\n+    if not tools.has_source_changes(last_release):\n+        print(\"Not deploying due to no source changes\")\n+        sys.exit(0)\n     start_time = time()", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -103,8 +103,8 @@ public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n     OutputFileFactory fileFactory = new OutputFileFactory(\n         spec, format, locations, io.value(), encryptionManager.value(), partitionId, taskId);\n \n-    TaskWriter<InternalRow> writer;\n-    if (spec.fields().isEmpty()) {\n+    final TaskWriter<InternalRow> writer;", "oldf": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\n\npackage org.apache.iceberg.spark.source;\n\nimport java.io.Serializable;\nimport java.util.Collection;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport java.util.stream.Collectors;\nimport org.apache.iceberg.CombinedScanTask;\nimport org.apache.iceberg.DataFile;\nimport org.apache.iceberg.FileFormat;\nimport org.apache.iceberg.PartitionSpec;\nimport org.apache.iceberg.Schema;\nimport org.apache.iceberg.Table;\nimport org.apache.iceberg.TableProperties;\nimport org.apache.iceberg.encryption.EncryptionManager;\nimport org.apache.iceberg.io.FileIO;\nimport org.apache.iceberg.io.LocationProvider;\nimport org.apache.iceberg.io.OutputFileFactory;\nimport org.apache.iceberg.io.TaskWriter;\nimport org.apache.iceberg.io.UnpartitionedWriter;\nimport org.apache.iceberg.relocated.com.google.common.collect.Lists;\nimport org.apache.iceberg.spark.SparkSchemaUtil;\nimport org.apache.iceberg.util.PropertyUtil;\nimport org.apache.spark.TaskContext;\nimport org.apache.spark.api.java.JavaRDD;\nimport org.apache.spark.broadcast.Broadcast;\nimport org.apache.spark.sql.catalyst.InternalRow;\nimport org.apache.spark.sql.types.StructType;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n\npublic class RowDataRewriter implements Serializable {\n\n  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n\n  private final Schema schema;\n  private final PartitionSpec spec;\n  private final Map<String, String> properties;\n  private final FileFormat format;\n  private final Broadcast<FileIO> io;\n  private final Broadcast<EncryptionManager> encryptionManager;\n  private final LocationProvider locations;\n  private final String nameMapping;\n  private final boolean caseSensitive;\n\n  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n                         Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager) {\n    this.schema = table.schema();\n    this.spec = spec;\n    this.locations = table.locationProvider();\n    this.properties = table.properties();\n    this.io = io;\n    this.encryptionManager = encryptionManager;\n\n    this.caseSensitive = caseSensitive;\n    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n\n    String formatString = table.properties().getOrDefault(\n        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n  }\n\n  public List<DataFile> rewriteDataForTasks(JavaRDD<CombinedScanTask> taskRDD) {\n    JavaRDD<List<DataFile>> dataFilesRDD = taskRDD.map(this::rewriteDataForTask);\n\n    return dataFilesRDD.collect().stream()\n        .flatMap(Collection::stream)\n        .collect(Collectors.toList());\n  }\n\n  private List<DataFile> rewriteDataForTask(CombinedScanTask task) throws Exception {\n    TaskContext context = TaskContext.get();\n    int partitionId = context.partitionId();\n    long taskId = context.taskAttemptId();\n\n    RowDataReader dataReader = new RowDataReader(\n        task, schema, schema, nameMapping, io.value(), encryptionManager.value(), caseSensitive);\n\n    StructType structType = SparkSchemaUtil.convert(schema);\n    SparkAppenderFactory appenderFactory = new SparkAppenderFactory(properties, schema, structType, spec);\n    OutputFileFactory fileFactory = new OutputFileFactory(\n        spec, format, locations, io.value(), encryptionManager.value(), partitionId, taskId);\n\n    final TaskWriter<InternalRow> writer;\n    if (spec.isUnpartitioned()) {\n      writer = new UnpartitionedWriter<>(spec, format, appenderFactory, fileFactory, io.value(),\n          Long.MAX_VALUE);\n    } else if (PropertyUtil.propertyAsBoolean(properties,\n        TableProperties.SPARK_WRITE_PARTITIONED_FANOUT_ENABLED,\n        TableProperties.SPARK_WRITE_PARTITIONED_FANOUT_ENABLED_DEFAULT)) {\n      writer = new SparkPartitionedFanoutWriter(\n          spec, format, appenderFactory, fileFactory, io.value(), Long.MAX_VALUE, schema,\n          structType);\n    } else {\n      writer = new SparkPartitionedWriter(\n          spec, format, appenderFactory, fileFactory, io.value(), Long.MAX_VALUE, schema,\n          structType);\n    }\n\n    try {\n      while (dataReader.next()) {\n        InternalRow row = dataReader.get();\n        writer.write(row);\n      }\n\n      dataReader.close();\n      dataReader = null;\n\n      writer.close();\n      return Lists.newArrayList(writer.dataFiles());\n\n    } catch (Throwable originalThrowable) {\n      try {\n        LOG.error(\"Aborting task\", originalThrowable);\n        context.markTaskFailed(originalThrowable);\n\n        LOG.error(\"Aborting commit for partition {} (task {}, attempt {}, stage {}.{})\",\n            partitionId, taskId, context.attemptNumber(), context.stageId(), context.stageAttemptNumber());\n        if (dataReader != null) {\n          dataReader.close();\n        }\n        writer.abort();\n        LOG.error(\"Aborted commit for partition {} (task {}, attempt {}, stage {}.{})\",\n            partitionId, taskId, context.taskAttemptId(), context.stageId(), context.stageAttemptNumber());\n\n      } catch (Throwable inner) {\n        if (originalThrowable != inner) {\n          originalThrowable.addSuppressed(inner);\n          LOG.warn(\"Suppressing exception in catch: {}\", inner.getMessage(), inner);\n        }\n      }\n\n      if (originalThrowable instanceof Exception) {\n        throw originalThrowable;\n      } else {\n        throw new RuntimeException(originalThrowable);\n      }\n    }\n  }\n}\n", "hunk": "@@ -103,7 +103,7 @@ public class RowDataRewriter implements Serializable {\n     OutputFileFactory fileFactory = new OutputFileFactory(\n         spec, format, locations, io.value(), encryptionManager.value(), partitionId, taskId);\n \n-    final TaskWriter<InternalRow> writer;\n+    TaskWriter<InternalRow> writer;\n     if (spec.isUnpartitioned()) {\n       writer = new UnpartitionedWriter<>(spec, format, appenderFactory, fileFactory, io.value(),\n           Long.MAX_VALUE);\n", "comment": "Iceberg doesn't use `final` because it is unlikely that this actually helps. In Java 8, final detection is quite good, which is why non-final variables can be used in closures and lambdas. And final doesn't produce different byte code so it can't do much to help at runtime.", "ids": ["32296", "2f93e89905a9fc68b72b82ed3e89aa79c05ca849", "e404fbe3cdec0ee3439fe942791aee12c58f55f0"], "repo": "apache/iceberg", "ghid": 2114, "old": "     OutputFileFactory fileFactory = new OutputFileFactory(\n         spec, format, locations, io.value(), encryptionManager.value(), partitionId, taskId);\n-    final TaskWriter<InternalRow> writer;\n     if (spec.isUnpartitioned()) {\n       writer = new UnpartitionedWriter<>(spec, format, appenderFactory, fileFactory, io.value(),\n           Long.MAX_VALUE);", "new": "     OutputFileFactory fileFactory = new OutputFileFactory(\n         spec, format, locations, io.value(), encryptionManager.value(), partitionId, taskId);\n+    TaskWriter<InternalRow> writer;\n     if (spec.isUnpartitioned()) {\n       writer = new UnpartitionedWriter<>(spec, format, appenderFactory, fileFactory, io.value(),\n           Long.MAX_VALUE);", "lang": "java", "norm_lang": "java"}
{"old_hunk": "@@ -239,7 +239,6 @@ def request_item(request, locale=None):\n             to=settings.PROJECT_MANAGERS,\n             cc=locale.managers_group.user_set.exclude(pk=user.pk)\n             .values_list('email', flat=True) if locale else '',\n-            reply_to=[user.email],", "oldf": "import json\n\nfrom django.contrib.auth.decorators import login_required\nfrom django.contrib.auth.models import User\nfrom django.conf import settings\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.core.mail import EmailMessage\nfrom django.db import transaction\nfrom django.db.models import Q, Count\nfrom django.http import Http404, HttpResponse, HttpResponseBadRequest\nfrom django.shortcuts import get_object_or_404, render\nfrom django.template.loader import get_template\nfrom django.views.decorators.http import require_POST\nfrom django.views.generic.detail import DetailView\n\nimport bleach\nfrom guardian.decorators import permission_required_or_403\n\nfrom pontoon.base import forms\nfrom pontoon.base.models import Locale, Project\nfrom pontoon.base.utils import require_AJAX\nfrom pontoon.contributors.views import ContributorsMixin\nfrom pontoon.teams.forms import LocaleRequestForm\n\n\ndef teams(request):\n    \"\"\"List all active localization teams.\"\"\"\n    locales = (\n        Locale.objects.visible()\n        .prefetch_related('latest_translation__user')\n    )\n\n    form = LocaleRequestForm()\n\n    if not locales:\n        raise Http404\n\n    return render(request, 'teams/teams.html', {\n        'locales': locales,\n        'form': form,\n        'top_instances': locales.get_top_instances(),\n    })\n\n\ndef team(request, locale):\n    \"\"\"Team dashboard.\"\"\"\n    locale = get_object_or_404(Locale, code=locale)\n    count = locale.project_set.visible().count()\n\n    if not count:\n        raise Http404\n\n    return render(request, 'teams/team.html', {\n        'count': count,\n        'locale': locale,\n    })\n\n\n@require_AJAX\ndef ajax_projects(request, locale):\n    \"\"\"Projects tab.\"\"\"\n    locale = get_object_or_404(Locale, code=locale)\n\n    projects = (\n        Project.objects.visible()\n        .filter(Q(locales=locale) | Q(can_be_requested=True))\n        .prefetch_project_locale(locale)\n        .order_by('name')\n        .annotate(enabled_locales=Count('project_locale', distinct=True))\n    )\n\n    if not projects:\n        raise Http404\n\n    return render(request, 'teams/includes/projects.html', {\n        'locale': locale,\n        'projects': projects,\n    })\n\n\n@require_AJAX\ndef ajax_info(request, locale):\n    \"\"\"Info tab.\"\"\"\n    locale = get_object_or_404(Locale, code=locale)\n\n    return render(request, 'teams/includes/info.html', {\n        'locale': locale,\n    })\n\n\n@require_POST\n@permission_required_or_403('base.can_manage_locale', (Locale, 'code', 'locale'))\n@transaction.atomic\ndef ajax_update_info(request, locale):\n    team_description = request.POST.get('team_info', None)\n    team_description = bleach.clean(\n        team_description, strip=True,\n        tags=settings.ALLOWED_TAGS, attributes=settings.ALLOWED_ATTRIBUTES\n    )\n    locale = get_object_or_404(Locale, code=locale)\n    locale.team_description = team_description\n    locale.save()\n    return HttpResponse(team_description)\n\n\n@permission_required_or_403('base.can_manage_locale', (Locale, 'code', 'locale'))\n@transaction.atomic\ndef ajax_permissions(request, locale):\n    locale = get_object_or_404(Locale, code=locale)\n    project_locales = locale.project_locale.visible()\n\n    if request.method == 'POST':\n        locale_form = forms.LocalePermsForm(\n            request.POST,\n            instance=locale,\n            prefix='general',\n            user=request.user\n        )\n        project_locale_form = forms.ProjectLocalePermsFormsSet(\n            request.POST,\n            prefix='project-locale',\n            queryset=project_locales,\n            form_kwargs={\n                'user': request.user\n            }\n        )\n\n        if locale_form.is_valid() and project_locale_form.is_valid():\n            locale_form.save()\n            project_locale_form.save()\n\n        else:\n            errors = locale_form.errors\n            errors.update(project_locale_form.errors_dict)\n            return HttpResponseBadRequest(json.dumps(errors))\n\n    else:\n        project_locale_form = forms.ProjectLocalePermsFormsSet(\n            prefix='project-locale',\n            queryset=project_locales,\n            form_kwargs={\n                'user': request.user\n            }\n        )\n\n    managers = locale.managers_group.user_set.order_by('email')\n    translators = locale.translators_group.user_set.exclude(pk__in=managers).order_by('email')\n    all_users = (\n        User.objects\n            .exclude(pk__in=managers | translators)\n            .exclude(email='')\n            .order_by('email')\n    )\n\n    contributors_emails = set(\n        contributor.email\n        for contributor in User.translators.with_translation_counts(None, Q(locale=locale), None)\n    )\n\n    locale_projects = locale.projects_permissions\n\n    return render(request, 'teams/includes/permissions.html', {\n        'locale': locale,\n        'all_users': all_users,\n        'contributors_emails': contributors_emails,\n        'translators': translators,\n        'managers': managers,\n        'locale_projects': locale_projects,\n        'project_locale_form': project_locale_form,\n        'all_projects_in_translation': all([x[5] for x in locale_projects])\n    })\n\n\n@login_required(redirect_field_name='', login_url='/403')\n@require_POST\ndef request_item(request, locale=None):\n    \"\"\"Request projects and teams to be added.\"\"\"\n    user = request.user\n\n    # Request projects to be enabled for team\n    if locale:\n        slug_list = request.POST.getlist('projects[]')\n        locale = get_object_or_404(Locale, code=locale)\n\n        # Validate projects\n        project_list = (\n            Project.objects.visible()\n            .filter(slug__in=slug_list, can_be_requested=True)\n        )\n        if not project_list:\n            return HttpResponseBadRequest('Bad Request: Non-existent projects specified')\n\n        projects = ''.join('- {} ({})\\n'.format(p.name, p.slug) for p in project_list)\n\n        mail_subject = u'Project request for {locale} ({code})'.format(\n            locale=locale.name, code=locale.code\n        )\n\n        payload = {\n            'locale': locale.name,\n            'code': locale.code,\n            'projects': projects,\n            'user': user.display_name_and_email,\n            'user_role': user.locale_role(locale),\n            'user_url': request.build_absolute_uri(user.profile_url)\n        }\n\n    # Request new teams to be enabled\n    else:\n        form = LocaleRequestForm(request.POST)\n        if not form.is_valid():\n            if form.has_error('code', 'unique'):\n                return HttpResponse('This team already exists.', status=409)\n            return HttpResponseBadRequest(form.errors.as_json())\n\n        code = form.cleaned_data['code']\n        name = form.cleaned_data['name']\n\n        mail_subject = u'New team request: {locale} ({code})'.format(\n            locale=name, code=code\n        )\n\n        payload = {\n            'locale': name,\n            'code': code,\n            'user': user.display_name_and_email,\n            'user_role': user.role(),\n            'user_url': request.build_absolute_uri(user.profile_url)\n        }\n\n    if settings.PROJECT_MANAGERS[0] != '':\n        template = get_template('teams/email_request_item.jinja')\n        mail_body = template.render(payload)\n\n        EmailMessage(\n            subject=mail_subject,\n            body=mail_body,\n            from_email='pontoon@mozilla.com',\n            to=settings.PROJECT_MANAGERS,\n            cc=locale.managers_group.user_set.exclude(pk=user.pk)\n            .values_list('email', flat=True) if locale else '',\n        ).send()\n    else:\n        raise ImproperlyConfigured(\n            \"PROJECT_MANAGERS not defined in settings. Email recipient unknown.\"\n        )\n\n    return HttpResponse('ok')\n\n\nclass LocaleContributorsView(ContributorsMixin, DetailView):\n    \"\"\"\n    Renders view of contributors for the team.\n    \"\"\"\n    template_name = 'teams/includes/contributors.html'\n    model = Locale\n    slug_field = 'code'\n    slug_url_kwarg = 'code'\n\n    def get_context_object_name(self, obj):\n        return 'locale'\n\n    def contributors_filter(self, **kwargs):\n        return Q(locale=self.object)\n", "hunk": "@@ -239,6 +239,7 @@ def request_item(request, locale=None):\n             to=settings.PROJECT_MANAGERS,\n             cc=locale.managers_group.user_set.exclude(pk=user.pk)\n             .values_list('email', flat=True) if locale else '',\n+            reply_to=[user.email],\n         ).send()\n     else:\n         raise ImproperlyConfigured(\n", "comment": "What's the purpose of this change?", "ids": ["15308", "cbb3f204fb173ad66e35e6cae5f920a05c38acec", "fb4450070bbdbe12edf56ed1e228c0646e76f756"], "repo": "mozilla/pontoon", "ghid": 1142, "old": "             to=settings.PROJECT_MANAGERS,\n             cc=locale.managers_group.user_set.exclude(pk=user.pk)\n             .values_list('email', flat=True) if locale else '',\n         ).send()\n     else:\n         raise ImproperlyConfigured(", "new": "             to=settings.PROJECT_MANAGERS,\n             cc=locale.managers_group.user_set.exclude(pk=user.pk)\n             .values_list('email', flat=True) if locale else '',\n+            reply_to=[user.email],\n         ).send()\n     else:\n         raise ImproperlyConfigured(", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -381,7 +381,7 @@ def as_dict(self:DcmDataset, px_summ=True, window=dicom_windows.brain):\n     return res\n \n # Cell\n-def _dcm2dict(fn, **kwargs): return fn.dcmread().as_dict(**kwargs)\n+def _dcm2dict(fn,window,**kwargs): return fn.dcmread().as_dict(window=window, **kwargs)", "oldf": "# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/60_medical.imaging.ipynb (unless otherwise specified).\n\n__all__ = ['DcmDataset', 'DcmTag', 'DcmMultiValue', 'dcmread', 'get_dicom_files', 'DicomSegmentationDataLoaders',\n           'get_dicom_files', 'TensorDicom', 'PILDicom', 'pixels', 'scaled_px', 'array_freqhist_bins', 'dicom_windows',\n           'TensorCTScan', 'PILCTScan', 'uniform_blur2d', 'gauss_blur2d', 'mask2bbox', 'crop_resize', 'shape',\n           'DicomSegmentationDataLoaders']\n\n# Cell\nfrom ..basics import *\nfrom ..vision.all import *\nfrom ..data.transforms import *\n\nimport pydicom,kornia,skimage\nfrom pydicom.dataset import Dataset as DcmDataset\nfrom pydicom.tag import BaseTag as DcmTag\nfrom pydicom.multival import MultiValue as DcmMultiValue\nfrom PIL import Image\n\ntry:\n    import cv2\n    cv2.setNumThreads(0)\nexcept: pass\n\n# Cell\n#nbdev_comment _all_ = ['DcmDataset', 'DcmTag', 'DcmMultiValue', 'dcmread', 'get_dicom_files', 'DicomSegmentationDataLoaders']\n\n# Cell\ndef get_dicom_files(path, recurse=True, folders=None):\n    \"Get dicom files in `path` recursively, only in `folders`, if specified.\"\n    return get_files(path, extensions=[\".dcm\",\".dicom\"], recurse=recurse, folders=folders)\n\n# Cell\n@patch\ndef dcmread(fn:Path, force = False):\n    \"Open a `DICOM` file\"\n    return pydicom.dcmread(str(fn), force)\n\n# Cell\nclass TensorDicom(TensorImage):\n    \"Inherits from `TensorImage` and converts the `pixel_array` into a `TensorDicom`\"\n    _show_args = {'cmap':'gray'}\n\n# Cell\nclass PILDicom(PILBase):\n    _open_args,_tensor_cls,_show_args = {},TensorDicom,TensorDicom._show_args\n    @classmethod\n    def create(cls, fn:(Path,str,bytes), mode=None)->None:\n        \"Open a `DICOM file` from path `fn` or bytes `fn` and load it as a `PIL Image`\"\n        if isinstance(fn,bytes): im = Image.fromarray(pydicom.dcmread(pydicom.filebase.DicomBytesIO(fn)).pixel_array)\n        if isinstance(fn,(Path,str)): im = Image.fromarray(pydicom.dcmread(fn).pixel_array)\n        im.load()\n        im = im._new(im.im)\n        return cls(im.convert(mode) if mode else im)\n\nPILDicom._tensor_cls = TensorDicom\n\n# Cell\n@patch\ndef png16read(self:Path): return array(Image.open(self), dtype=np.uint16)\n\n# Cell\n@patch(as_prop=True)\ndef pixels(self:DcmDataset):\n    \"`pixel_array` as a tensor\"\n    return tensor(self.pixel_array.astype(np.float32))\n\n# Cell\n@patch(as_prop=True)\ndef scaled_px(self:DcmDataset):\n    \"`pixels` scaled by `RescaleSlope` and `RescaleIntercept`\"\n    img = self.pixels\n    if hasattr(self, 'RescaleSlope') and hasattr(self, 'RescaleIntercept') is not None:\n        return img * self.RescaleSlope + self.RescaleIntercept\n    else: return img\n\n# Cell\ndef array_freqhist_bins(self, n_bins=100):\n    \"A numpy based function to split the range of pixel values into groups, such that each group has around the same number of pixels\"\n    imsd = np.sort(self.flatten())\n    t = np.array([0.001])\n    t = np.append(t, np.arange(n_bins)/n_bins+(1/2/n_bins))\n    t = np.append(t, 0.999)\n    t = (len(imsd)*t+0.5).astype(np.int)\n    return np.unique(imsd[t])\n\n# Cell\n@patch\ndef freqhist_bins(self:Tensor, n_bins=100):\n    \"A function to split the range of pixel values into groups, such that each group has around the same number of pixels\"\n    imsd = self.view(-1).sort()[0]\n    t = torch.cat([tensor([0.001]),\n                   torch.arange(n_bins).float()/n_bins+(1/2/n_bins),\n                   tensor([0.999])])\n    t = (len(imsd)*t).long()\n    return imsd[t].unique()\n\n# Cell\n@patch\ndef hist_scaled_pt(self:Tensor, brks=None):\n    # Pytorch-only version - switch to this if/when interp_1d can be optimized\n    if brks is None: brks = self.freqhist_bins()\n    brks = brks.to(self.device)\n    ys = torch.linspace(0., 1., len(brks)).to(self.device)\n    return self.flatten().interp_1d(brks, ys).reshape(self.shape).clamp(0.,1.)\n\n# Cell\n@patch\ndef hist_scaled(self:Tensor, brks=None):\n    \"Scales a tensor using `freqhist_bins` to values between 0 and 1\"\n    if self.device.type=='cuda': return self.hist_scaled_pt(brks)\n    if brks is None: brks = self.freqhist_bins()\n    ys = np.linspace(0., 1., len(brks))\n    x = self.numpy().flatten()\n    x = np.interp(x, brks.numpy(), ys)\n    return tensor(x).reshape(self.shape).clamp(0.,1.)\n\n# Cell\n@patch\ndef hist_scaled(self:DcmDataset, brks=None, min_px=None, max_px=None):\n    \"Pixels scaled to a `min_px` and `max_px` value\"\n    px = self.scaled_px\n    if min_px is not None: px[px<min_px] = min_px\n    if max_px is not None: px[px>max_px] = max_px\n    return px.hist_scaled(brks=brks)\n\n# Cell\n@patch\ndef windowed(self:Tensor, w, l):\n    \"Scale pixel intensity by window width and window level\"\n    px = self.clone()\n    px_min = l - w//2\n    px_max = l + w//2\n    px[px<px_min] = px_min\n    px[px>px_max] = px_max\n    return (px-px_min) / (px_max-px_min)\n\n# Cell\n@patch\ndef windowed(self:DcmDataset, w, l):\n    return self.scaled_px.windowed(w,l)\n\n# Cell\n# From https://radiopaedia.org/articles/windowing-ct\ndicom_windows = types.SimpleNamespace(\n    brain=(80,40),\n    subdural=(254,100),\n    stroke=(8,32),\n    brain_bone=(2800,600),\n    brain_soft=(375,40),\n    lungs=(1500,-600),\n    mediastinum=(350,50),\n    abdomen_soft=(400,50),\n    liver=(150,30),\n    spine_soft=(250,50),\n    spine_bone=(1800,400)\n)\n\n# Cell\nclass TensorCTScan(TensorImageBW):\n    \"Inherits from `TensorImageBW` and converts the `pixel_array` into a `TensorCTScan`\"\n    _show_args = {'cmap':'bone'}\n\n# Cell\nclass PILCTScan(PILBase): _open_args,_tensor_cls,_show_args = {},TensorCTScan,TensorCTScan._show_args\n\n# Cell\n@patch\n@delegates(show_image)\ndef show(self:DcmDataset, scale=True, cmap=plt.cm.bone, min_px=-1100, max_px=None, **kwargs):\n    \"Display a normalized dicom image by default\"\n    px = (self.windowed(*scale) if isinstance(scale,tuple)\n          else self.hist_scaled(min_px=min_px,max_px=max_px,brks=scale) if isinstance(scale,(ndarray,Tensor))\n          else self.hist_scaled(min_px=min_px,max_px=max_px) if scale\n          else self.scaled_px)\n    show_image(px, cmap=cmap, **kwargs)\n\n# Cell\n@patch\ndef show(self:DcmDataset, frames=1, scale=True, cmap=plt.cm.bone, min_px=-1100, max_px=None, **kwargs):\n    \"Adds functionality to view dicom images where each file may have more than 1 frame\"\n    px = (self.windowed(*scale) if isinstance(scale,tuple)\n          else self.hist_scaled(min_px=min_px,max_px=max_px,brks=scale) if isinstance(scale,(ndarray,Tensor))\n          else self.hist_scaled(min_px=min_px,max_px=max_px) if scale\n          else self.scaled_px)\n    if px.ndim > 2:\n        gh=[]\n        p = px.shape; print(f'{p[0]} frames per file')\n        for i in range(frames): u = px[i]; gh.append(u)\n        show_images(gh, **kwargs)\n    else: show_image(px, cmap=cmap, **kwargs)\n\n# Cell\n@patch\ndef pct_in_window(dcm:DcmDataset, w, l):\n    \"% of pixels in the window `(w,l)`\"\n    px = dcm.scaled_px\n    return ((px > l-w//2) & (px < l+w//2)).float().mean().item()\n\n# Cell\ndef uniform_blur2d(x,s):\n    \"Uniformly apply blurring\"\n    w = x.new_ones(1,1,1,s)/s\n    # Factor 2d conv into 2 1d convs\n    x = unsqueeze(x, dim=0, n=4-x.dim())\n    r = (F.conv2d(x, w, padding=s//2))\n    r = (F.conv2d(r, w.transpose(-1,-2), padding=s//2)).cpu()[:,0]\n    return r.squeeze()\n\n# Cell\ndef gauss_blur2d(x,s):\n    \"Apply gaussian_blur2d kornia filter\"\n    s2 = int(s/4)*2+1\n    x2 = unsqueeze(x, dim=0, n=4-x.dim())\n    res = kornia.filters.gaussian_blur2d(x2, (s2,s2), (s,s), 'replicate')\n    return res.squeeze()\n\n# Cell\n@patch\ndef mask_from_blur(x:Tensor, window, sigma=0.3, thresh=0.05, remove_max=True):\n    \"Create a mask from the blurred image\"\n    p = x.windowed(*window)\n    if remove_max: p[p==1] = 0\n    return gauss_blur2d(p, s=sigma*x.shape[-1])>thresh\n\n# Cell\n@patch\ndef mask_from_blur(x:DcmDataset, window, sigma=0.3, thresh=0.05, remove_max=True):\n    \"Create a mask from the blurred image\"\n    return to_device(x.scaled_px).mask_from_blur(window, sigma, thresh, remove_max=remove_max)\n\n# Cell\ndef _px_bounds(x, dim):\n    c = x.sum(dim).nonzero().cpu()\n    idxs,vals = torch.unique(c[:,0],return_counts=True)\n    vs = torch.split_with_sizes(c[:,1],tuple(vals))\n    d = {k.item():v for k,v in zip(idxs,vs)}\n    default_u = tensor([0,x.shape[-1]-1])\n    b = [d.get(o,default_u) for o in range(x.shape[0])]\n    b = [tensor([o.min(),o.max()]) for o in b]\n    return torch.stack(b)\n\n# Cell\ndef mask2bbox(mask):\n    no_batch = mask.dim()==2\n    if no_batch: mask = mask[None]\n    bb1 = _px_bounds(mask,-1).t()\n    bb2 = _px_bounds(mask,-2).t()\n    res = torch.stack([bb1,bb2],dim=1).to(mask.device)\n    return res[...,0] if no_batch else res\n\n# Cell\ndef _bbs2sizes(crops, init_sz, use_square=True):\n    bb = crops.flip(1)\n    szs = (bb[1]-bb[0])\n    if use_square: szs = szs.max(0)[0][None].repeat((2,1))\n    overs = (szs+bb[0])>init_sz\n    bb[0][overs] = init_sz-szs[overs]\n    lows = (bb[0]/float(init_sz))\n    return lows,szs/float(init_sz)\n\n# Cell\ndef crop_resize(x, crops, new_sz):\n    # NB assumes square inputs. Not tested for non-square anythings!\n    bs = x.shape[0]\n    lows,szs = _bbs2sizes(crops, x.shape[-1])\n    if not isinstance(new_sz,(list,tuple)): new_sz = (new_sz,new_sz)\n    id_mat = tensor([[1.,0,0],[0,1,0]])[None].repeat((bs,1,1)).to(x.device)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=UserWarning)\n        sp = F.affine_grid(id_mat, (bs,1,*new_sz))+1.\n        grid = sp*unsqueeze(szs.t(),1,n=2)+unsqueeze(lows.t()*2.,1,n=2)\n        return F.grid_sample(x.unsqueeze(1), grid-1)\n\n# Cell\n@patch\ndef to_nchan(x:Tensor, wins, bins=None):\n    res = [x.windowed(*win) for win in wins]\n    if not isinstance(bins,int) or bins!=0: res.append(x.hist_scaled(bins).clamp(0,1))\n    dim = [0,1][x.dim()==3]\n    return TensorCTScan(torch.stack(res, dim=dim))\n\n# Cell\n@patch\ndef to_nchan(x:DcmDataset, wins, bins=None):\n    return x.scaled_px.to_nchan(wins, bins)\n\n# Cell\n@patch\ndef to_3chan(x:Tensor, win1, win2, bins=None):\n    return x.to_nchan([win1,win2],bins=bins)\n\n# Cell\n@patch\ndef to_3chan(x:DcmDataset, win1, win2, bins=None):\n    return x.scaled_px.to_3chan(win1, win2, bins)\n\n# Cell\n@patch\ndef save_jpg(x:(Tensor,DcmDataset), path, wins, bins=None, quality=90):\n    \"Save tensor or dicom image into `jpg` format\"\n    fn = Path(path).with_suffix('.jpg')\n    x = (x.to_nchan(wins, bins)*255).byte()\n    im = Image.fromarray(x.permute(1,2,0).numpy(), mode=['RGB','CMYK'][x.shape[0]==4])\n    im.save(fn, quality=quality)\n\n# Cell\n@patch\ndef to_uint16(x:(Tensor,DcmDataset), bins=None):\n    \"Convert into a unit16 array\"\n    d = x.hist_scaled(bins).clamp(0,1) * 2**16\n    return d.numpy().astype(np.uint16)\n\n# Cell\n@patch\ndef save_tif16(x:(Tensor,DcmDataset), path, bins=None, compress=True):\n    \"Save tensor or dicom image into `tiff` format\"\n    fn = Path(path).with_suffix('.tif')\n    Image.fromarray(x.to_uint16(bins)).save(str(fn), compression='tiff_deflate' if compress else None)\n\n# Cell\n@patch\ndef set_pixels(self:DcmDataset, px):\n    self.PixelData = px.tobytes()\n    self.Rows,self.Columns = px.shape\nDcmDataset.pixel_array = property(DcmDataset.pixel_array.fget, set_pixels)\n\n# Cell\n@patch\ndef zoom(self:DcmDataset, ratio):\n    \"Zoom image by specified ratio\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        self.set_pixels(ndimage.zoom(self.pixel_array, ratio))\n\n# Cell\n@patch\ndef zoom_to(self:DcmDataset, sz):\n    \"Change image size to specified pixel size\"\n    if not isinstance(sz,(list,tuple)): sz=(sz,sz)\n    rows,cols = sz\n    self.zoom((rows/self.Rows,cols/self.Columns))\n\n# Cell\n@patch(as_prop=True)\ndef shape(self:DcmDataset):\n    \"Returns the shape of a dicom image as rows and columns\"\n    return self.Rows,self.Columns\n\n# Cell\ndef _cast_dicom_special(x):\n    cls = type(x)\n    if not cls.__module__.startswith('pydicom'): return x\n    if cls.__base__ == object: return x\n    return cls.__base__(x)\n\ndef _split_elem(res,k,v):\n    if not isinstance(v,DcmMultiValue): return\n    res[f'Multi{k}'] = 1\n    for i,o in enumerate(v): res[f'{k}{\"\" if i==0 else i}']=o\n\n# Cell\n@patch\ndef as_dict(self:DcmDataset, px_summ=True, window=dicom_windows.brain):\n    \"Convert the header of a dicom into a dictionary\"\n    pxdata = (0x7fe0,0x0010)\n    vals = [self[o] for o in self.keys() if o != pxdata]\n    its = [(v.keyword,v.value) for v in vals]\n    res = dict(its)\n    res['fname'] = self.filename\n    for k,v in its: _split_elem(res,k,v)\n    if not px_summ: return res\n    stats = 'min','max','mean','std'\n    try:\n        pxs = self.pixel_array\n        for f in stats: res['img_'+f] = getattr(pxs,f)()\n        res['img_pct_window'] = self.pct_in_window(*window)\n    except Exception as e:\n        for f in stats: res['img_'+f] = 0\n        print(res,e)\n    for k in res: res[k] = _cast_dicom_special(res[k])\n    return res\n\n# Cell\ndef _dcm2dict(fn,window,**kwargs): return fn.dcmread().as_dict(window=window, **kwargs)\n\n# Cell\n@delegates(parallel)\ndef _from_dicoms(cls, fns, n_workers=0, **kwargs):\n    return pd.DataFrame(parallel(_dcm2dict, fns, n_workers=n_workers, **kwargs))\npd.DataFrame.from_dicoms = classmethod(_from_dicoms)\n\n# Cell\nclass DicomSegmentationDataLoaders(DataLoaders):\n    \"Basic wrapper around DICOM `DataLoaders` with factory methods for segmentation problems\"\n    @classmethod\n    @delegates(DataLoaders.from_dblock)\n    def from_label_func(cls, path, fnames, label_func, valid_pct=0.2, seed=None, codes=None, item_tfms=None, batch_tfms=None, **kwargs):\n        \"Create from list of `fnames` in `path`s with `label_func`.\"\n        dblock = DataBlock(blocks=(ImageBlock(cls=PILDicom), MaskBlock(codes=codes)),\n                           splitter=RandomSplitter(valid_pct, seed=seed),\n                           get_y=label_func,\n                           item_tfms=item_tfms,\n                           batch_tfms=batch_tfms)\n        res = cls.from_dblock(dblock, fnames, path=path, **kwargs)\n        return res", "hunk": "@@ -381,7 +381,7 @@ def as_dict(self:DcmDataset, px_summ=True, window=dicom_windows.brain):\n     return res\n \n # Cell\n-def _dcm2dict(fn,window,**kwargs): return fn.dcmread().as_dict(window=window, **kwargs)\n+def _dcm2dict(fn, window=dicom_windows.brain, **kwargs): return fn.dcmread().as_dict(window=window, **kwargs)\n \n # Cell\n @delegates(parallel)\n", "comment": "should `window` have a default argument?", "ids": ["10641", "f39ba0493443860969acd9a1808a90fc1aaa25e0", "206e12571745af990d58cc98006084fa5556d3f1"], "repo": "fastai/fastai", "ghid": 3136, "old": "     return res\n # Cell\n-def _dcm2dict(fn,window,**kwargs): return fn.dcmread().as_dict(window=window, **kwargs)\n # Cell\n @delegates(parallel)", "new": "     return res\n # Cell\n+def _dcm2dict(fn, window=dicom_windows.brain, **kwargs): return fn.dcmread().as_dict(window=window, **kwargs)\n # Cell\n @delegates(parallel)", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -410,6 +408,49 @@\n       \"The number of threads for the metadata table scan executor.\"),\n   TSERV_MIGRATE_MAXCONCURRENT(\"tserver.migrations.concurrent.max\", \"1\", PropertyType.COUNT,\n       \"The maximum number of concurrent tablet migrations for a tablet server\"),\n+  TSERV_MAJC_DELAY(\"tserver.compaction.major.delay\", \"30s\", PropertyType.TIMEDURATION,\n+      \"Time a tablet server will sleep between checking which tablets need compaction.\"),\n+  TSERV_COMPACTION_SERVICE_PREFIX(\"tserver.compaction.service.\", null, PropertyType.PREFIX,", "oldf": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\npackage org.apache.accumulo.core.conf;\n\nimport java.lang.annotation.Annotation;\nimport java.util.EnumSet;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Map;\nimport java.util.Map.Entry;\n\nimport org.apache.accumulo.core.Constants;\nimport org.apache.accumulo.core.client.security.tokens.PasswordToken;\nimport org.apache.accumulo.core.constraints.NoDeleteConstraint;\nimport org.apache.accumulo.core.file.rfile.RFile;\nimport org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\nimport org.apache.accumulo.core.iteratorsImpl.system.DeletingIterator;\nimport org.apache.accumulo.core.metadata.MetadataTable;\nimport org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner;\nimport org.apache.accumulo.core.spi.compaction.SimpleCompactionDispatcher;\nimport org.apache.accumulo.core.spi.scan.ScanDispatcher;\nimport org.apache.accumulo.core.spi.scan.ScanPrioritizer;\nimport org.apache.accumulo.core.spi.scan.SimpleScanDispatcher;\nimport org.apache.accumulo.core.util.format.DefaultFormatter;\nimport org.apache.accumulo.core.util.interpret.DefaultScanInterpreter;\nimport org.apache.accumulo.start.classloader.AccumuloClassLoader;\nimport org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport com.google.common.base.Preconditions;\n\npublic enum Property {\n  // SSL properties local to each node (see also instance.ssl.enabled which must be consistent\n  // across all nodes in an instance)\n  RPC_PREFIX(\"rpc.\", null, PropertyType.PREFIX,\n      \"Properties in this category related to the configuration of SSL keys for\"\n          + \" RPC. See also instance.ssl.enabled\"),\n  RPC_SSL_KEYSTORE_PATH(\"rpc.javax.net.ssl.keyStore\", \"\", PropertyType.PATH,\n      \"Path of the keystore file for the server's private SSL key\"),\n  @Sensitive\n  RPC_SSL_KEYSTORE_PASSWORD(\"rpc.javax.net.ssl.keyStorePassword\", \"\", PropertyType.STRING,\n      \"Password used to encrypt the SSL private keystore. \"\n          + \"Leave blank to use the Accumulo instance secret\"),\n  RPC_SSL_KEYSTORE_TYPE(\"rpc.javax.net.ssl.keyStoreType\", \"jks\", PropertyType.STRING,\n      \"Type of SSL keystore\"),\n  RPC_SSL_TRUSTSTORE_PATH(\"rpc.javax.net.ssl.trustStore\", \"\", PropertyType.PATH,\n      \"Path of the truststore file for the root cert\"),\n  @Sensitive\n  RPC_SSL_TRUSTSTORE_PASSWORD(\"rpc.javax.net.ssl.trustStorePassword\", \"\", PropertyType.STRING,\n      \"Password used to encrypt the SSL truststore. Leave blank to use no password\"),\n  RPC_SSL_TRUSTSTORE_TYPE(\"rpc.javax.net.ssl.trustStoreType\", \"jks\", PropertyType.STRING,\n      \"Type of SSL truststore\"),\n  RPC_USE_JSSE(\"rpc.useJsse\", \"false\", PropertyType.BOOLEAN,\n      \"Use JSSE system properties to configure SSL rather than the \" + RPC_PREFIX.getKey()\n          + \"javax.net.ssl.* Accumulo properties\"),\n  RPC_SSL_CIPHER_SUITES(\"rpc.ssl.cipher.suites\", \"\", PropertyType.STRING,\n      \"Comma separated list of cipher suites that can be used by accepted connections\"),\n  RPC_SSL_ENABLED_PROTOCOLS(\"rpc.ssl.server.enabled.protocols\", \"TLSv1.2\", PropertyType.STRING,\n      \"Comma separated list of protocols that can be used to accept connections\"),\n  RPC_SSL_CLIENT_PROTOCOL(\"rpc.ssl.client.protocol\", \"TLSv1.2\", PropertyType.STRING,\n      \"The protocol used to connect to a secure server, must be in the list of enabled protocols \"\n          + \"on the server side (rpc.ssl.server.enabled.protocols)\"),\n  /**\n   * @since 1.7.0\n   */\n  RPC_SASL_QOP(\"rpc.sasl.qop\", \"auth\", PropertyType.STRING,\n      \"The quality of protection to be used with SASL. Valid values are 'auth', 'auth-int',\"\n          + \" and 'auth-conf'\"),\n\n  // instance properties (must be the same for every node in an instance)\n  INSTANCE_PREFIX(\"instance.\", null, PropertyType.PREFIX,\n      \"Properties in this category must be consistent throughout a cloud. \"\n          + \"This is enforced and servers won't be able to communicate if these differ.\"),\n  INSTANCE_ZK_HOST(\"instance.zookeeper.host\", \"localhost:2181\", PropertyType.HOSTLIST,\n      \"Comma separated list of zookeeper servers\"),\n  INSTANCE_ZK_TIMEOUT(\"instance.zookeeper.timeout\", \"30s\", PropertyType.TIMEDURATION,\n      \"Zookeeper session timeout; \"\n          + \"max value when represented as milliseconds should be no larger than \"\n          + Integer.MAX_VALUE),\n  @Sensitive\n  INSTANCE_SECRET(\"instance.secret\", \"DEFAULT\", PropertyType.STRING,\n      \"A secret unique to a given instance that all servers must know in order\"\n          + \" to communicate with one another. It should be changed prior to the\"\n          + \" initialization of Accumulo. To change it after Accumulo has been\"\n          + \" initialized, use the ChangeSecret tool and then update accumulo.properties\"\n          + \" everywhere. Before using the ChangeSecret tool, make sure Accumulo is not\"\n          + \" running and you are logged in as the user that controls Accumulo files in\"\n          + \" HDFS. To use the ChangeSecret tool, run the command: ./bin/accumulo\"\n          + \" org.apache.accumulo.server.util.ChangeSecret\"),\n  INSTANCE_VOLUMES(\"instance.volumes\", \"\", PropertyType.STRING,\n      \"A comma separated list of dfs uris to use. Files will be stored across\"\n          + \" these filesystems. If this is empty, then instance.dfs.uri will be used.\"\n          + \" After adding uris to this list, run 'accumulo init --add-volume' and then\"\n          + \" restart tservers. If entries are removed from this list then tservers\"\n          + \" will need to be restarted. After a uri is removed from the list Accumulo\"\n          + \" will not create new files in that location, however Accumulo can still\"\n          + \" reference files created at that location before the config change. To use\"\n          + \" a comma or other reserved characters in a URI use standard URI hex\"\n          + \" encoding. For example replace commas with %2C.\"),\n  INSTANCE_VOLUMES_REPLACEMENTS(\"instance.volumes.replacements\", \"\", PropertyType.STRING,\n      \"Since accumulo stores absolute URIs changing the location of a namenode \"\n          + \"could prevent Accumulo from starting. The property helps deal with \"\n          + \"that situation. Provide a comma separated list of uri replacement \"\n          + \"pairs here if a namenode location changes. Each pair should be separated \"\n          + \"with a space. For example, if hdfs://nn1 was replaced with \"\n          + \"hdfs://nnA and hdfs://nn2 was replaced with hdfs://nnB, then set this \"\n          + \"property to 'hdfs://nn1 hdfs://nnA,hdfs://nn2 hdfs://nnB' \"\n          + \"Replacements must be configured for use. To see which volumes are \"\n          + \"currently in use, run 'accumulo admin volumes -l'. To use a comma or \"\n          + \"other reserved characters in a URI use standard URI hex encoding. For \"\n          + \"example replace commas with %2C.\"),\n  INSTANCE_VOLUMES_UPGRADE_RELATIVE(\"instance.volumes.upgrade.relative\", \"\", PropertyType.STRING,\n      \"The volume dfs uri containing relative tablet file paths. Relative paths may exist in the metadata from \"\n          + \"versions prior to 1.6. This property is only required if a relative path is detected \"\n          + \"during the upgrade process and will only be used once.\"),\n  INSTANCE_SECURITY_AUTHENTICATOR(\"instance.security.authenticator\",\n      \"org.apache.accumulo.server.security.handler.ZKAuthenticator\", PropertyType.CLASSNAME,\n      \"The authenticator class that accumulo will use to determine if a user \"\n          + \"has privilege to perform an action\"),\n  INSTANCE_SECURITY_AUTHORIZOR(\"instance.security.authorizor\",\n      \"org.apache.accumulo.server.security.handler.ZKAuthorizor\", PropertyType.CLASSNAME,\n      \"The authorizor class that accumulo will use to determine what labels a \"\n          + \"user has privilege to see\"),\n  INSTANCE_SECURITY_PERMISSION_HANDLER(\"instance.security.permissionHandler\",\n      \"org.apache.accumulo.server.security.handler.ZKPermHandler\", PropertyType.CLASSNAME,\n      \"The permission handler class that accumulo will use to determine if a \"\n          + \"user has privilege to perform an action\"),\n  INSTANCE_RPC_SSL_ENABLED(\"instance.rpc.ssl.enabled\", \"false\", PropertyType.BOOLEAN,\n      \"Use SSL for socket connections from clients and among accumulo services. \"\n          + \"Mutually exclusive with SASL RPC configuration.\"),\n  INSTANCE_RPC_SSL_CLIENT_AUTH(\"instance.rpc.ssl.clientAuth\", \"false\", PropertyType.BOOLEAN,\n      \"Require clients to present certs signed by a trusted root\"),\n  /**\n   * @since 1.7.0\n   */\n  INSTANCE_RPC_SASL_ENABLED(\"instance.rpc.sasl.enabled\", \"false\", PropertyType.BOOLEAN,\n      \"Configures Thrift RPCs to require SASL with GSSAPI which supports \"\n          + \"Kerberos authentication. Mutually exclusive with SSL RPC configuration.\"),\n  INSTANCE_RPC_SASL_ALLOWED_USER_IMPERSONATION(\"instance.rpc.sasl.allowed.user.impersonation\", \"\",\n      PropertyType.STRING,\n      \"One-line configuration property controlling what users are allowed to \"\n          + \"impersonate other users\"),\n  INSTANCE_RPC_SASL_ALLOWED_HOST_IMPERSONATION(\"instance.rpc.sasl.allowed.host.impersonation\", \"\",\n      PropertyType.STRING,\n      \"One-line configuration property controlling the network locations \"\n          + \"(hostnames) that are allowed to impersonate other users\"),\n  // Crypto-related properties\n  @Experimental\n  INSTANCE_CRYPTO_PREFIX(\"instance.crypto.opts.\", null, PropertyType.PREFIX,\n      \"Properties related to on-disk file encryption.\"),\n  @Experimental\n  @Sensitive\n  INSTANCE_CRYPTO_SENSITIVE_PREFIX(\"instance.crypto.opts.sensitive.\", null, PropertyType.PREFIX,\n      \"Sensitive properties related to on-disk file encryption.\"),\n  @Experimental\n  INSTANCE_CRYPTO_SERVICE(\"instance.crypto.service\",\n      \"org.apache.accumulo.core.cryptoImpl.NoCryptoService\", PropertyType.CLASSNAME,\n      \"The class which executes on-disk file encryption. The default does nothing. To enable \"\n          + \"encryption, replace this classname with an implementation of the\"\n          + \"org.apache.accumulo.core.spi.crypto.CryptoService interface.\"),\n\n  // general properties\n  GENERAL_PREFIX(\"general.\", null, PropertyType.PREFIX,\n      \"Properties in this category affect the behavior of accumulo overall, but\"\n          + \" do not have to be consistent throughout a cloud.\"),\n  @Deprecated\n  GENERAL_DYNAMIC_CLASSPATHS(AccumuloVFSClassLoader.DYNAMIC_CLASSPATH_PROPERTY_NAME,\n      AccumuloVFSClassLoader.DEFAULT_DYNAMIC_CLASSPATH_VALUE, PropertyType.STRING,\n      \"This property is deprecated since 2.0.0. A list of all of the places where changes \"\n          + \"in jars or classes will force a reload of the classloader. Built-in dynamic class \"\n          + \"loading will be removed in a future version. If this is needed, consider overriding \"\n          + \"the Java system class loader with one that has this feature \"\n          + \"(https://docs.oracle.com/javase/8/docs/api/java/lang/ClassLoader.html#getSystemClassLoader--). \"\n          + \"Additionally, this property no longer does property interpolation of environment \"\n          + \"variables, such as '$ACCUMULO_HOME'. Use commons-configuration syntax,\"\n          + \"'${env:ACCUMULO_HOME}' instead.\"),\n  GENERAL_RPC_TIMEOUT(\"general.rpc.timeout\", \"120s\", PropertyType.TIMEDURATION,\n      \"Time to wait on I/O for simple, short RPC calls\"),\n  @Experimental\n  GENERAL_RPC_SERVER_TYPE(\"general.rpc.server.type\", \"\", PropertyType.STRING,\n      \"Type of Thrift server to instantiate, see \"\n          + \"org.apache.accumulo.server.rpc.ThriftServerType for more information. \"\n          + \"Only useful for benchmarking thrift servers\"),\n  GENERAL_KERBEROS_KEYTAB(\"general.kerberos.keytab\", \"\", PropertyType.PATH,\n      \"Path to the kerberos keytab to use. Leave blank if not using kerberoized hdfs\"),\n  GENERAL_KERBEROS_PRINCIPAL(\"general.kerberos.principal\", \"\", PropertyType.STRING,\n      \"Name of the kerberos principal to use. _HOST will automatically be \"\n          + \"replaced by the machines hostname in the hostname portion of the \"\n          + \"principal. Leave blank if not using kerberoized hdfs\"),\n  GENERAL_KERBEROS_RENEWAL_PERIOD(\"general.kerberos.renewal.period\", \"30s\",\n      PropertyType.TIMEDURATION,\n      \"The amount of time between attempts to perform Kerberos ticket renewals.\"\n          + \" This does not equate to how often tickets are actually renewed (which is\"\n          + \" performed at 80% of the ticket lifetime).\"),\n  GENERAL_MAX_MESSAGE_SIZE(\"general.server.message.size.max\", \"1G\", PropertyType.BYTES,\n      \"The maximum size of a message that can be sent to a server.\"),\n  GENERAL_SIMPLETIMER_THREADPOOL_SIZE(\"general.server.simpletimer.threadpool.size\", \"1\",\n      PropertyType.COUNT, \"The number of threads to use for \" + \"server-internal scheduled tasks\"),\n  // If you update the default type, be sure to update the default used for initialization failures\n  // in VolumeManagerImpl\n  @Experimental\n  GENERAL_VOLUME_CHOOSER(\"general.volume.chooser\",\n      \"org.apache.accumulo.server.fs.RandomVolumeChooser\", PropertyType.CLASSNAME,\n      \"The class that will be used to select which volume will be used to create new files.\"),\n  GENERAL_SECURITY_CREDENTIAL_PROVIDER_PATHS(\"general.security.credential.provider.paths\", \"\",\n      PropertyType.STRING, \"Comma-separated list of paths to CredentialProviders\"),\n  GENERAL_ARBITRARY_PROP_PREFIX(\"general.custom.\", null, PropertyType.PREFIX,\n      \"Prefix to be used for user defined system-wide properties. This may be\"\n          + \" particularly useful for system-wide configuration for various\"\n          + \" user-implementations of pluggable Accumulo features, such as the balancer\"\n          + \" or volume chooser.\"),\n  GENERAL_DELEGATION_TOKEN_LIFETIME(\"general.delegation.token.lifetime\", \"7d\",\n      PropertyType.TIMEDURATION,\n      \"The length of time that delegation tokens and secret keys are valid\"),\n  GENERAL_DELEGATION_TOKEN_UPDATE_INTERVAL(\"general.delegation.token.update.interval\", \"1d\",\n      PropertyType.TIMEDURATION, \"The length of time between generation of new secret keys\"),\n  GENERAL_MAX_SCANNER_RETRY_PERIOD(\"general.max.scanner.retry.period\", \"5s\",\n      PropertyType.TIMEDURATION,\n      \"The maximum amount of time that a Scanner should wait before retrying a failed RPC\"),\n\n  // properties that are specific to master server behavior\n  MASTER_PREFIX(\"master.\", null, PropertyType.PREFIX,\n      \"Properties in this category affect the behavior of the master server\"),\n  MASTER_CLIENTPORT(\"master.port.client\", \"9999\", PropertyType.PORT,\n      \"The port used for handling client connections on the master\"),\n  MASTER_TABLET_BALANCER(\"master.tablet.balancer\",\n      \"org.apache.accumulo.server.master.balancer.TableLoadBalancer\", PropertyType.CLASSNAME,\n      \"The balancer class that accumulo will use to make tablet assignment and \"\n          + \"migration decisions.\"),\n  MASTER_BULK_RETRIES(\"master.bulk.retries\", \"3\", PropertyType.COUNT,\n      \"The number of attempts to bulk import a RFile before giving up.\"),\n  MASTER_BULK_THREADPOOL_SIZE(\"master.bulk.threadpool.size\", \"5\", PropertyType.COUNT,\n      \"The number of threads to use when coordinating a bulk import.\"),\n  MASTER_BULK_TIMEOUT(\"master.bulk.timeout\", \"5m\", PropertyType.TIMEDURATION,\n      \"The time to wait for a tablet server to process a bulk import request\"),\n  MASTER_RENAME_THREADS(\"master.rename.threadpool.size\", \"20\", PropertyType.COUNT,\n      \"The number of threads to use when renaming user files during table import or bulk ingest.\"),\n  @Deprecated\n  @ReplacedBy(property = MASTER_RENAME_THREADS)\n  MASTER_BULK_RENAME_THREADS(\"master.bulk.rename.threadpool.size\", \"20\", PropertyType.COUNT,\n      \"This property is deprecated since 2.1.0. The number of threads to use when moving user files to bulk ingest \"\n          + \"directories under accumulo control\"),\n  MASTER_BULK_TSERVER_REGEX(\"master.bulk.tserver.regex\", \"\", PropertyType.STRING,\n      \"Regular expression that defines the set of Tablet Servers that will perform bulk imports\"),\n  MASTER_MINTHREADS(\"master.server.threads.minimum\", \"20\", PropertyType.COUNT,\n      \"The minimum number of threads to use to handle incoming requests.\"),\n  MASTER_THREADCHECK(\"master.server.threadcheck.time\", \"1s\", PropertyType.TIMEDURATION,\n      \"The time between adjustments of the server thread pool.\"),\n  MASTER_RECOVERY_DELAY(\"master.recovery.delay\", \"10s\", PropertyType.TIMEDURATION,\n      \"When a tablet server's lock is deleted, it takes time for it to \"\n          + \"completely quit. This delay gives it time before log recoveries begin.\"),\n  MASTER_LEASE_RECOVERY_WAITING_PERIOD(\"master.lease.recovery.interval\", \"5s\",\n      PropertyType.TIMEDURATION,\n      \"The amount of time to wait after requesting a write-ahead log to be recovered\"),\n  MASTER_WALOG_CLOSER_IMPLEMETATION(\"master.walog.closer.implementation\",\n      \"org.apache.accumulo.server.master.recovery.HadoopLogCloser\", PropertyType.CLASSNAME,\n      \"A class that implements a mechanism to steal write access to a write-ahead log\"),\n  MASTER_FATE_METRICS_ENABLED(\"master.fate.metrics.enabled\", \"true\", PropertyType.BOOLEAN,\n      \"Enable reporting of FATE metrics in JMX (and logging with Hadoop Metrics2\"),\n  MASTER_FATE_METRICS_MIN_UPDATE_INTERVAL(\"master.fate.metrics.min.update.interval\", \"60s\",\n      PropertyType.TIMEDURATION, \"Limit calls from metric sinks to zookeeper to update interval\"),\n  MASTER_FATE_THREADPOOL_SIZE(\"master.fate.threadpool.size\", \"4\", PropertyType.COUNT,\n      \"The number of threads used to run fault-tolerant executions (FATE).\"\n          + \" These are primarily table operations like merge.\"),\n  MASTER_REPLICATION_SCAN_INTERVAL(\"master.replication.status.scan.interval\", \"30s\",\n      PropertyType.TIMEDURATION,\n      \"Amount of time to sleep before scanning the status section of the \"\n          + \"replication table for new data\"),\n  MASTER_REPLICATION_COORDINATOR_PORT(\"master.replication.coordinator.port\", \"10001\",\n      PropertyType.PORT, \"Port for the replication coordinator service\"),\n  MASTER_REPLICATION_COORDINATOR_MINTHREADS(\"master.replication.coordinator.minthreads\", \"4\",\n      PropertyType.COUNT, \"Minimum number of threads dedicated to answering coordinator requests\"),\n  MASTER_REPLICATION_COORDINATOR_THREADCHECK(\"master.replication.coordinator.threadcheck.time\",\n      \"5s\", PropertyType.TIMEDURATION,\n      \"The time between adjustments of the coordinator thread pool\"),\n  MASTER_STATUS_THREAD_POOL_SIZE(\"master.status.threadpool.size\", \"0\", PropertyType.COUNT,\n      \"The number of threads to use when fetching the tablet server status for balancing.  Zero \"\n          + \"indicates an unlimited number of threads will be used.\"),\n  MASTER_METADATA_SUSPENDABLE(\"master.metadata.suspendable\", \"false\", PropertyType.BOOLEAN,\n      \"Allow tablets for the \" + MetadataTable.NAME\n          + \" table to be suspended via table.suspend.duration.\"),\n  MASTER_STARTUP_TSERVER_AVAIL_MIN_COUNT(\"master.startup.tserver.avail.min.count\", \"0\",\n      PropertyType.COUNT,\n      \"Minimum number of tservers that need to be registered before master will \"\n          + \"start tablet assignment - checked at master initialization, when master gets lock. \"\n          + \" When set to 0 or less, no blocking occurs. Default is 0 (disabled) to keep original \"\n          + \" behaviour. Added with version 1.10\"),\n  MASTER_STARTUP_TSERVER_AVAIL_MAX_WAIT(\"master.startup.tserver.avail.max.wait\", \"0\",\n      PropertyType.TIMEDURATION,\n      \"Maximum time master will wait for tserver available threshold \"\n          + \"to be reached before continuing. When set to 0 or less, will block \"\n          + \"indefinitely. Default is 0 to block indefinitely. Only valid when tserver available \"\n          + \"threshold is set greater than 0. Added with version 1.10\"),\n  // properties that are specific to tablet server behavior\n  TSERV_PREFIX(\"tserver.\", null, PropertyType.PREFIX,\n      \"Properties in this category affect the behavior of the tablet servers\"),\n  TSERV_CLIENT_TIMEOUT(\"tserver.client.timeout\", \"3s\", PropertyType.TIMEDURATION,\n      \"Time to wait for clients to continue scans before closing a session.\"),\n  TSERV_DEFAULT_BLOCKSIZE(\"tserver.default.blocksize\", \"1M\", PropertyType.BYTES,\n      \"Specifies a default blocksize for the tserver caches\"),\n  TSERV_CACHE_MANAGER_IMPL(\"tserver.cache.manager.class\",\n      \"org.apache.accumulo.core.file.blockfile.cache.lru.LruBlockCacheManager\", PropertyType.STRING,\n      \"Specifies the class name of the block cache factory implementation.\"\n          + \" Alternative implementation is\"\n          + \" org.apache.accumulo.core.file.blockfile.cache.tinylfu.TinyLfuBlockCacheManager\"),\n  TSERV_DATACACHE_SIZE(\"tserver.cache.data.size\", \"10%\", PropertyType.MEMORY,\n      \"Specifies the size of the cache for RFile data blocks.\"),\n  TSERV_INDEXCACHE_SIZE(\"tserver.cache.index.size\", \"25%\", PropertyType.MEMORY,\n      \"Specifies the size of the cache for RFile index blocks.\"),\n  TSERV_SUMMARYCACHE_SIZE(\"tserver.cache.summary.size\", \"10%\", PropertyType.MEMORY,\n      \"Specifies the size of the cache for summary data on each tablet server.\"),\n  TSERV_PORTSEARCH(\"tserver.port.search\", \"false\", PropertyType.BOOLEAN,\n      \"if the ports above are in use, search higher ports until one is available\"),\n  TSERV_CLIENTPORT(\"tserver.port.client\", \"9997\", PropertyType.PORT,\n      \"The port used for handling client connections on the tablet servers\"),\n  TSERV_TOTAL_MUTATION_QUEUE_MAX(\"tserver.total.mutation.queue.max\", \"5%\", PropertyType.MEMORY,\n      \"The amount of memory used to store write-ahead-log mutations before flushing them.\"),\n  TSERV_TABLET_SPLIT_FINDMIDPOINT_MAXOPEN(\"tserver.tablet.split.midpoint.files.max\", \"300\",\n      PropertyType.COUNT,\n      \"To find a tablets split points, all RFiles are opened and their indexes\"\n          + \" are read. This setting determines how many RFiles can be opened at once.\"\n          + \" When there are more RFiles than this setting multiple passes must be\"\n          + \" made, which is slower. However opening too many RFiles at once can cause\"\n          + \" problems.\"),\n  TSERV_WALOG_MAX_REFERENCED(\"tserver.walog.max.referenced\", \"3\", PropertyType.COUNT,\n      \"When a tablet server has more than this many write ahead logs, any tablet referencing older \"\n          + \"logs over this threshold is minor compacted.  Also any tablet referencing this many \"\n          + \"logs or more will be compacted.\"),\n  TSERV_WALOG_MAX_SIZE(\"tserver.walog.max.size\", \"1g\", PropertyType.BYTES,\n      \"The maximum size for each write-ahead log. See comment for property\"\n          + \" tserver.memory.maps.max\"),\n  TSERV_WALOG_MAX_AGE(\"tserver.walog.max.age\", \"24h\", PropertyType.TIMEDURATION,\n      \"The maximum age for each write-ahead log.\"),\n  TSERV_WALOG_TOLERATED_CREATION_FAILURES(\"tserver.walog.tolerated.creation.failures\", \"50\",\n      PropertyType.COUNT,\n      \"The maximum number of failures tolerated when creating a new write-ahead\"\n          + \" log. Negative values will allow unlimited creation failures. Exceeding this\"\n          + \" number of failures consecutively trying to create a new write-ahead log\"\n          + \" causes the TabletServer to exit.\"),\n  TSERV_WALOG_TOLERATED_WAIT_INCREMENT(\"tserver.walog.tolerated.wait.increment\", \"1000ms\",\n      PropertyType.TIMEDURATION,\n      \"The amount of time to wait between failures to create or write a write-ahead log.\"),\n  // Never wait longer than 5 mins for a retry\n  TSERV_WALOG_TOLERATED_MAXIMUM_WAIT_DURATION(\"tserver.walog.maximum.wait.duration\", \"5m\",\n      PropertyType.TIMEDURATION,\n      \"The maximum amount of time to wait after a failure to create or write a write-ahead log.\"),\n  TSERV_SCAN_MAX_OPENFILES(\"tserver.scan.files.open.max\", \"100\", PropertyType.COUNT,\n      \"Maximum total RFiles that all tablets in a tablet server can open for scans. \"),\n  TSERV_MAX_IDLE(\"tserver.files.open.idle\", \"1m\", PropertyType.TIMEDURATION,\n      \"Tablet servers leave previously used RFiles open for future queries.\"\n          + \" This setting determines how much time an unused RFile should be kept open\"\n          + \" until it is closed.\"),\n  TSERV_NATIVEMAP_ENABLED(\"tserver.memory.maps.native.enabled\", \"true\", PropertyType.BOOLEAN,\n      \"An in-memory data store for accumulo implemented in c++ that increases\"\n          + \" the amount of data accumulo can hold in memory and avoids Java GC\" + \" pauses.\"),\n  TSERV_MAXMEM(\"tserver.memory.maps.max\", \"33%\", PropertyType.MEMORY,\n      \"Maximum amount of memory that can be used to buffer data written to a\"\n          + \" tablet server. There are two other properties that can effectively limit\"\n          + \" memory usage table.compaction.minor.logs.threshold and\"\n          + \" tserver.walog.max.size. Ensure that table.compaction.minor.logs.threshold\"\n          + \" * tserver.walog.max.size >= this property.\"),\n  TSERV_MEM_MGMT(\"tserver.memory.manager\",\n      \"org.apache.accumulo.server.tabletserver.LargestFirstMemoryManager\", PropertyType.CLASSNAME,\n      \"An implementation of MemoryManger that accumulo will use.\"),\n  TSERV_SESSION_MAXIDLE(\"tserver.session.idle.max\", \"1m\", PropertyType.TIMEDURATION,\n      \"When a tablet server's SimpleTimer thread triggers to check idle\"\n          + \" sessions, this configurable option will be used to evaluate scan sessions\"\n          + \" to determine if they can be closed due to inactivity\"),\n  TSERV_UPDATE_SESSION_MAXIDLE(\"tserver.session.update.idle.max\", \"1m\", PropertyType.TIMEDURATION,\n      \"When a tablet server's SimpleTimer thread triggers to check idle\"\n          + \" sessions, this configurable option will be used to evaluate update\"\n          + \" sessions to determine if they can be closed due to inactivity\"),\n  TSERV_SCAN_EXECUTORS_PREFIX(\"tserver.scan.executors.\", null, PropertyType.PREFIX,\n      \"Prefix for defining executors to service scans. See \"\n          + \"[scan executors]({% durl administration/scan-executors %}) for an overview of why and\"\n          + \" how to use this property. For each executor the number of threads, thread priority, \"\n          + \"and an optional prioritizer can be configured. To configure a new executor, set \"\n          + \"`tserver.scan.executors.<name>.threads=<number>`.  Optionally, can also set \"\n          + \"`tserver.scan.executors.<name>.priority=<number 1 to 10>`, \"\n          + \"`tserver.scan.executors.<name>.prioritizer=<class name>`, and \"\n          + \"`tserver.scan.executors.<name>.prioritizer.opts.<key>=<value>`\"),\n  TSERV_SCAN_EXECUTORS_DEFAULT_THREADS(\"tserver.scan.executors.default.threads\", \"16\",\n      PropertyType.COUNT,\n      \"The number of threads for the scan executor that tables use by default.\"),\n  TSERV_SCAN_EXECUTORS_DEFAULT_PRIORITIZER(\"tserver.scan.executors.default.prioritizer\", \"\",\n      PropertyType.STRING,\n      \"Prioritizer for the default scan executor.  Defaults to none which \"\n          + \"results in FIFO priority.  Set to a class that implements \"\n          + ScanPrioritizer.class.getName() + \" to configure one.\"),\n  TSERV_SCAN_EXECUTORS_META_THREADS(\"tserver.scan.executors.meta.threads\", \"8\", PropertyType.COUNT,\n      \"The number of threads for the metadata table scan executor.\"),\n  TSERV_MIGRATE_MAXCONCURRENT(\"tserver.migrations.concurrent.max\", \"1\", PropertyType.COUNT,\n      \"The maximum number of concurrent tablet migrations for a tablet server\"),\n  TSERV_MAJC_DELAY(\"tserver.compaction.major.delay\", \"30s\", PropertyType.TIMEDURATION,\n      \"Time a tablet server will sleep between checking which tablets need compaction.\"),\n  TSERV_COMPACTION_SERVICE_PREFIX(\"tserver.compaction.service.\", null, PropertyType.PREFIX,\n      \"Prefix for compaction services.\"),\n  TSERV_COMPACTION_SERVICE_ROOT_PLANNER(\"tserver.compaction.service.root.planner\",\n      DefaultCompactionPlanner.class.getName(), PropertyType.CLASSNAME,\n      \"Compaction planner for root tablet service\"),\n  TSERV_COMPACTION_SERVICE_ROOT_MAX_OPEN(\"tserver.compaction.service.root.planner.opts.maxOpen\",\n      \"30\", PropertyType.COUNT, \"The maximum number of files a compaction will open\"),\n  TSERV_COMPACTION_SERVICE_ROOT_EXECUTORS(\"tserver.compaction.service.root.planner.opts.executors\",\n      \"[{'name':'small','maxSize':'32M','numThreads':1},\"\n          + \"{'name':'huge','numThreads':1}]\".replaceAll(\"'\", \"\\\"\"),\n      PropertyType.STRING,\n      \"See {% jlink -f org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner %} \"),\n  TSERV_COMPACTION_SERVICE_META_PLANNER(\"tserver.compaction.service.meta.planner\",\n      DefaultCompactionPlanner.class.getName(), PropertyType.CLASSNAME,\n      \"Compaction planner for metadatat table\"),\n  TSERV_COMPACTION_SERVICE_META_MAX_OPEN(\"tserver.compaction.service.meta.planner.opts.maxOpen\",\n      \"30\", PropertyType.COUNT, \"The maximum number of files a compaction will open\"),\n  TSERV_COMPACTION_SERVICE_META_EXECUTORS(\"tserver.compaction.service.meta.planner.opts.executors\",\n      \"[{'name':'small','maxSize':'32M','numThreads':2},\"\n          + \"{'name':'huge','numThreads':2}]\".replaceAll(\"'\", \"\\\"\"),\n      PropertyType.STRING,\n      \"See {% jlink -f org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner %} \"),\n  TSERV_COMPACTION_SERVICE_DEFAULT_PLANNER(\"tserver.compaction.service.default.planner\",\n      DefaultCompactionPlanner.class.getName(), PropertyType.CLASSNAME,\n      \"Planner for default compaction service.\"),\n  TSERV_COMPACTION_SERVICE_DEFAULT_MAX_OPEN(\n      \"tserver.compaction.service.default.planner.opts.maxOpen\", \"30\", PropertyType.COUNT,\n      \"The maximum number of files a compaction will open\"),\n  TSERV_COMPACTION_SERVICE_DEFAULT_EXECUTORS(\n      \"tserver.compaction.service.default.planner.opts.executors\",\n      \"[{'name':'small','maxSize':'32M','numThreads':2},\"\n          + \"{'name':'medium','maxSize':'128M','numThreads':2},\"\n          + \"{'name':'large','numThreads':2}]\".replaceAll(\"'\", \"\\\"\"),\n      PropertyType.STRING,\n      \"See {% jlink -f org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner %} \"),\n  @Deprecated(since = \"2.1.0\", forRemoval = true)\n  @ReplacedBy(property = Property.TSERV_COMPACTION_SERVICE_DEFAULT_MAX_OPEN)\n  TSERV_MAJC_THREAD_MAXOPEN(\"tserver.compaction.major.thread.files.open.max\", \"10\",\n      PropertyType.COUNT, \"Max number of RFiles a major compaction thread can open at once. \"),\n  @Deprecated(since = \"2.1.0\", forRemoval = true)\n  @ReplacedBy(property = Property.TSERV_COMPACTION_SERVICE_DEFAULT_EXECUTORS)\n  TSERV_MAJC_MAXCONCURRENT(\"tserver.compaction.major.concurrent.max\", \"3\", PropertyType.COUNT,\n      \"The maximum number of concurrent major compactions for a tablet server\"),\n  TSERV_MAJC_THROUGHPUT(\"tserver.compaction.major.throughput\", \"0B\", PropertyType.BYTES,\n      \"Maximum number of bytes to read or write per second over all major\"\n          + \" compactions on a TabletServer, or 0B for unlimited.\"),\n  TSERV_MINC_MAXCONCURRENT(\"tserver.compaction.minor.concurrent.max\", \"4\", PropertyType.COUNT,\n      \"The maximum number of concurrent minor compactions for a tablet server\"),\n  TSERV_MAJC_TRACE_PERCENT(\"tserver.compaction.major.trace.percent\", \"0.1\", PropertyType.FRACTION,\n      \"The percent of major compactions to trace\"),\n  TSERV_MINC_TRACE_PERCENT(\"tserver.compaction.minor.trace.percent\", \"0.1\", PropertyType.FRACTION,\n      \"The percent of minor compactions to trace\"),\n  TSERV_COMPACTION_WARN_TIME(\"tserver.compaction.warn.time\", \"10m\", PropertyType.TIMEDURATION,\n      \"When a compaction has not made progress for this time period, a warning will be logged\"),\n  TSERV_BLOOM_LOAD_MAXCONCURRENT(\"tserver.bloom.load.concurrent.max\", \"4\", PropertyType.COUNT,\n      \"The number of concurrent threads that will load bloom filters in the background. \"\n          + \"Setting this to zero will make bloom filters load in the foreground.\"),\n  TSERV_MONITOR_FS(\"tserver.monitor.fs\", \"false\", PropertyType.BOOLEAN,\n      \"When enabled the tserver will monitor file systems and kill itself when\"\n          + \" one switches from rw to ro. This is usually and indication that Linux has\"\n          + \" detected a bad disk.\"),\n  TSERV_MEMDUMP_DIR(\"tserver.dir.memdump\", \"/tmp\", PropertyType.PATH,\n      \"A long running scan could possibly hold memory that has been minor\"\n          + \" compacted. To prevent this, the in memory map is dumped to a local file\"\n          + \" and the scan is switched to that local file. We can not switch to the\"\n          + \" minor compacted file because it may have been modified by iterators. The\"\n          + \" file dumped to the local dir is an exact copy of what was in memory.\"),\n  TSERV_BULK_PROCESS_THREADS(\"tserver.bulk.process.threads\", \"1\", PropertyType.COUNT,\n      \"The master will task a tablet server with pre-processing a bulk import\"\n          + \" RFile prior to assigning it to the appropriate tablet servers. This\"\n          + \" configuration value controls the number of threads used to process the\" + \" files.\"),\n  TSERV_BULK_ASSIGNMENT_THREADS(\"tserver.bulk.assign.threads\", \"1\", PropertyType.COUNT,\n      \"The master delegates bulk import RFile processing and assignment to\"\n          + \" tablet servers. After file has been processed, the tablet server will\"\n          + \" assign the file to the appropriate tablets on all servers. This property\"\n          + \" controls the number of threads used to communicate to the other\" + \" servers.\"),\n  TSERV_BULK_RETRY(\"tserver.bulk.retry.max\", \"5\", PropertyType.COUNT,\n      \"The number of times the tablet server will attempt to assign a RFile to\"\n          + \" a tablet as it migrates and splits.\"),\n  TSERV_BULK_TIMEOUT(\"tserver.bulk.timeout\", \"5m\", PropertyType.TIMEDURATION,\n      \"The time to wait for a tablet server to process a bulk import request.\"),\n  TSERV_MINTHREADS(\"tserver.server.threads.minimum\", \"20\", PropertyType.COUNT,\n      \"The minimum number of threads to use to handle incoming requests.\"),\n  TSERV_THREADCHECK(\"tserver.server.threadcheck.time\", \"1s\", PropertyType.TIMEDURATION,\n      \"The time between adjustments of the server thread pool.\"),\n  TSERV_MAX_MESSAGE_SIZE(\"tserver.server.message.size.max\", \"1G\", PropertyType.BYTES,\n      \"The maximum size of a message that can be sent to a tablet server.\"),\n  TSERV_LOG_BUSY_TABLETS_COUNT(\"tserver.log.busy.tablets.count\", \"0\", PropertyType.COUNT,\n      \"Number of busiest tablets to log. Logged at interval controlled by \"\n          + \"tserver.log.busy.tablets.interval. If <= 0, logging of busy tablets is disabled\"),\n  TSERV_LOG_BUSY_TABLETS_INTERVAL(\"tserver.log.busy.tablets.interval\", \"1h\",\n      PropertyType.TIMEDURATION, \"Time interval between logging out busy tablets information.\"),\n  TSERV_HOLD_TIME_SUICIDE(\"tserver.hold.time.max\", \"5m\", PropertyType.TIMEDURATION,\n      \"The maximum time for a tablet server to be in the \\\"memory full\\\" state.\"\n          + \" If the tablet server cannot write out memory in this much time, it will\"\n          + \" assume there is some failure local to its node, and quit. A value of zero\"\n          + \" is equivalent to forever.\"),\n  TSERV_WAL_BLOCKSIZE(\"tserver.wal.blocksize\", \"0\", PropertyType.BYTES,\n      \"The size of the HDFS blocks used to write to the Write-Ahead log. If\"\n          + \" zero, it will be 110% of tserver.walog.max.size (that is, try to use just\"\n          + \" one block)\"),\n  TSERV_WAL_REPLICATION(\"tserver.wal.replication\", \"0\", PropertyType.COUNT,\n      \"The replication to use when writing the Write-Ahead log to HDFS. If\"\n          + \" zero, it will use the HDFS default replication setting.\"),\n  TSERV_RECOVERY_MAX_CONCURRENT(\"tserver.recovery.concurrent.max\", \"2\", PropertyType.COUNT,\n      \"The maximum number of threads to use to sort logs during\" + \" recovery\"),\n  TSERV_SORT_BUFFER_SIZE(\"tserver.sort.buffer.size\", \"10%\", PropertyType.MEMORY,\n      \"The amount of memory to use when sorting logs during recovery.\"),\n  TSERV_WORKQ_THREADS(\"tserver.workq.threads\", \"2\", PropertyType.COUNT,\n      \"The number of threads for the distributed work queue. These threads are\"\n          + \" used for copying failed bulk import RFiles.\"),\n  TSERV_WAL_SYNC(\"tserver.wal.sync\", \"true\", PropertyType.BOOLEAN,\n      \"Use the SYNC_BLOCK create flag to sync WAL writes to disk. Prevents\"\n          + \" problems recovering from sudden system resets.\"),\n  TSERV_ASSIGNMENT_DURATION_WARNING(\"tserver.assignment.duration.warning\", \"10m\",\n      PropertyType.TIMEDURATION,\n      \"The amount of time an assignment can run before the server will print a\"\n          + \" warning along with the current stack trace. Meant to help debug stuck\"\n          + \" assignments\"),\n  TSERV_REPLICATION_REPLAYERS(\"tserver.replication.replayer.\", null, PropertyType.PREFIX,\n      \"Allows configuration of implementation used to apply replicated data\"),\n  TSERV_REPLICATION_DEFAULT_HANDLER(\"tserver.replication.default.replayer\",\n      \"org.apache.accumulo.tserver.replication.BatchWriterReplicationReplayer\",\n      PropertyType.CLASSNAME, \"Default AccumuloReplicationReplayer implementation\"),\n  TSERV_REPLICATION_BW_REPLAYER_MEMORY(\"tserver.replication.batchwriter.replayer.memory\", \"50M\",\n      PropertyType.BYTES, \"Memory to provide to batchwriter to replay mutations for replication\"),\n  TSERV_ASSIGNMENT_MAXCONCURRENT(\"tserver.assignment.concurrent.max\", \"2\", PropertyType.COUNT,\n      \"The number of threads available to load tablets. Recoveries are still performed serially.\"),\n  TSERV_SLOW_FLUSH_MILLIS(\"tserver.slow.flush.time\", \"100ms\", PropertyType.TIMEDURATION,\n      \"If a flush to the write-ahead log takes longer than this period of time,\"\n          + \" debugging information will written, and may result in a log rollover.\"),\n  TSERV_SLOW_FILEPERMIT_MILLIS(\"tserver.slow.filepermit.time\", \"100ms\", PropertyType.TIMEDURATION,\n      \"If a thread blocks more than this period of time waiting to get file permits,\"\n          + \" debugging information will be written.\"),\n  TSERV_SUMMARY_PARTITION_THREADS(\"tserver.summary.partition.threads\", \"10\", PropertyType.COUNT,\n      \"Summary data must be retrieved from RFiles. For a large number of\"\n          + \" RFiles, the files are broken into partitions of 100K files. This setting\"\n          + \" determines how many of these groups of 100K RFiles will be processed\"\n          + \" concurrently.\"),\n  TSERV_SUMMARY_REMOTE_THREADS(\"tserver.summary.remote.threads\", \"128\", PropertyType.COUNT,\n      \"For a partitioned group of 100K RFiles, those files are grouped by\"\n          + \" tablet server. Then a remote tablet server is asked to gather summary\"\n          + \" data. This setting determines how many concurrent request are made per\"\n          + \" partition.\"),\n  TSERV_SUMMARY_RETRIEVAL_THREADS(\"tserver.summary.retrieval.threads\", \"10\", PropertyType.COUNT,\n      \"The number of threads on each tablet server available to retrieve\"\n          + \" summary data, that is not currently in cache, from RFiles.\"),\n\n  // accumulo garbage collector properties\n  GC_PREFIX(\"gc.\", null, PropertyType.PREFIX,\n      \"Properties in this category affect the behavior of the accumulo garbage collector.\"),\n  GC_CYCLE_START(\"gc.cycle.start\", \"30s\", PropertyType.TIMEDURATION,\n      \"Time to wait before attempting to garbage collect any old RFiles or write-ahead logs.\"),\n  GC_CYCLE_DELAY(\"gc.cycle.delay\", \"5m\", PropertyType.TIMEDURATION,\n      \"Time between garbage collection cycles. In each cycle, old RFiles or write-ahead logs \"\n          + \"no longer in use are removed from the filesystem.\"),\n  GC_PORT(\"gc.port.client\", \"9998\", PropertyType.PORT,\n      \"The listening port for the garbage collector's monitor service\"),\n  GC_DELETE_THREADS(\"gc.threads.delete\", \"16\", PropertyType.COUNT,\n      \"The number of threads used to delete RFiles and write-ahead logs\"),\n  GC_TRASH_IGNORE(\"gc.trash.ignore\", \"false\", PropertyType.BOOLEAN,\n      \"Do not use the Trash, even if it is configured.\"),\n  GC_TRACE_PERCENT(\"gc.trace.percent\", \"0.01\", PropertyType.FRACTION,\n      \"Percent of gc cycles to trace\"),\n  GC_SAFEMODE(\"gc.safemode\", \"false\", PropertyType.BOOLEAN,\n      \"Provides listing of files to be deleted but does not delete any files\"),\n  GC_USE_FULL_COMPACTION(\"gc.post.metadata.action\", \"flush\", PropertyType.GC_POST_ACTION,\n      \"When the gc runs it can make a lot of changes to the metadata, on completion, \"\n          + \" to force the changes to be written to disk, the metadata and root tables can be flushed\"\n          + \" and possibly compacted. Legal values are: compact - which both flushes and compacts the\"\n          + \" metadata; flush - which flushes only (compactions may be triggered if required); or none\"),\n  GC_METRICS_ENABLED(\"gc.metrics.enabled\", \"true\", PropertyType.BOOLEAN,\n      \"Enable detailed gc metrics reporting with hadoop metrics.\"),\n\n  // properties that are specific to the monitor server behavior\n  MONITOR_PREFIX(\"monitor.\", null, PropertyType.PREFIX,\n      \"Properties in this category affect the behavior of the monitor web server.\"),\n  MONITOR_PORT(\"monitor.port.client\", \"9995\", PropertyType.PORT,\n      \"The listening port for the monitor's http service\"),\n  MONITOR_SSL_KEYSTORE(\"monitor.ssl.keyStore\", \"\", PropertyType.PATH,\n      \"The keystore for enabling monitor SSL.\"),\n  @Sensitive\n  MONITOR_SSL_KEYSTOREPASS(\"monitor.ssl.keyStorePassword\", \"\", PropertyType.STRING,\n      \"The keystore password for enabling monitor SSL.\"),\n  MONITOR_SSL_KEYSTORETYPE(\"monitor.ssl.keyStoreType\", \"jks\", PropertyType.STRING,\n      \"Type of SSL keystore\"),\n  @Sensitive\n  MONITOR_SSL_KEYPASS(\"monitor.ssl.keyPassword\", \"\", PropertyType.STRING,\n      \"Optional: the password for the private key in the keyStore. When not provided, this \"\n          + \"defaults to the keystore password.\"),\n  MONITOR_SSL_TRUSTSTORE(\"monitor.ssl.trustStore\", \"\", PropertyType.PATH,\n      \"The truststore for enabling monitor SSL.\"),\n  @Sensitive\n  MONITOR_SSL_TRUSTSTOREPASS(\"monitor.ssl.trustStorePassword\", \"\", PropertyType.STRING,\n      \"The truststore password for enabling monitor SSL.\"),\n  MONITOR_SSL_TRUSTSTORETYPE(\"monitor.ssl.trustStoreType\", \"jks\", PropertyType.STRING,\n      \"Type of SSL truststore\"),\n  MONITOR_SSL_INCLUDE_CIPHERS(\"monitor.ssl.include.ciphers\", \"\", PropertyType.STRING,\n      \"A comma-separated list of allows SSL Ciphers, see\"\n          + \" monitor.ssl.exclude.ciphers to disallow ciphers\"),\n  MONITOR_SSL_EXCLUDE_CIPHERS(\"monitor.ssl.exclude.ciphers\", \"\", PropertyType.STRING,\n      \"A comma-separated list of disallowed SSL Ciphers, see\"\n          + \" monitor.ssl.include.ciphers to allow ciphers\"),\n  MONITOR_SSL_INCLUDE_PROTOCOLS(\"monitor.ssl.include.protocols\", \"TLSv1.2\", PropertyType.STRING,\n      \"A comma-separate list of allowed SSL protocols\"),\n  MONITOR_LOCK_CHECK_INTERVAL(\"monitor.lock.check.interval\", \"5s\", PropertyType.TIMEDURATION,\n      \"The amount of time to sleep between checking for the Montior ZooKeeper lock\"),\n  MONITOR_RESOURCES_EXTERNAL(\"monitor.resources.external\", \"\", PropertyType.STRING,\n      \"A JSON Map of Strings. Each String should be an HTML tag of an external\"\n          + \" resource (JS or CSS) to be imported by the Monitor. Be sure to wrap\"\n          + \" with CDATA tags. If this value is set, all of the external resources\"\n          + \" in the `<head>` tag of the Monitor will be replaced with the tags set here.\"\n          + \" Be sure the jquery tag is first since other scripts will depend on it.\"\n          + \" The resources that are used by default can be seen in\"\n          + \" accumulo/server/monitor/src/main/resources/templates/default.ftl\"),\n\n  TRACE_PREFIX(\"trace.\", null, PropertyType.PREFIX,\n      \"Properties in this category affect the behavior of distributed tracing.\"),\n  TRACE_SPAN_RECEIVERS(\"trace.span.receivers\", \"org.apache.accumulo.tracer.ZooTraceClient\",\n      PropertyType.CLASSNAMELIST, \"A list of span receiver classes to send trace spans\"),\n  TRACE_SPAN_RECEIVER_PREFIX(\"trace.span.receiver.\", null, PropertyType.PREFIX,\n      \"Prefix for span receiver configuration properties\"),\n  TRACE_ZK_PATH(\"trace.zookeeper.path\", Constants.ZTRACERS, PropertyType.STRING,\n      \"The zookeeper node where tracers are registered\"),\n  TRACE_PORT(\"trace.port.client\", \"12234\", PropertyType.PORT,\n      \"The listening port for the trace server\"),\n  TRACE_TABLE(\"trace.table\", \"trace\", PropertyType.STRING,\n      \"The name of the table to store distributed traces\"),\n  TRACE_USER(\"trace.user\", \"root\", PropertyType.STRING,\n      \"The name of the user to store distributed traces\"),\n  @Sensitive\n  TRACE_PASSWORD(\"trace.password\", \"secret\", PropertyType.STRING,\n      \"The password for the user used to store distributed traces\"),\n  @Sensitive\n  TRACE_TOKEN_PROPERTY_PREFIX(\"trace.token.property.\", null, PropertyType.PREFIX,\n      \"The prefix used to create a token for storing distributed traces. For\"\n          + \" each property required by trace.token.type, place this prefix in front of it.\"),\n  TRACE_TOKEN_TYPE(\"trace.token.type\", PasswordToken.class.getName(), PropertyType.CLASSNAME,\n      \"An AuthenticationToken type supported by the authorizer\"),\n\n  // per table properties\n  TABLE_PREFIX(\"table.\", null, PropertyType.PREFIX,\n      \"Properties in this category affect tablet server treatment of tablets,\"\n          + \" but can be configured on a per-table basis. Setting these properties in\"\n          + \" accumulo.properties will override the default globally for all tables and not\"\n          + \" any specific table. However, both the default and the global setting can\"\n          + \" be overridden per table using the table operations API or in the shell,\"\n          + \" which sets the overridden value in zookeeper. Restarting accumulo tablet\"\n          + \" servers after setting these properties in accumulo.properties will cause the\"\n          + \" global setting to take effect. However, you must use the API or the shell\"\n          + \" to change properties in zookeeper that are set on a table.\"),\n  TABLE_ARBITRARY_PROP_PREFIX(\"table.custom.\", null, PropertyType.PREFIX,\n      \"Prefix to be used for user defined arbitrary properties.\"),\n  TABLE_MAJC_RATIO(\"table.compaction.major.ratio\", \"3\", PropertyType.FRACTION,\n      \"Minimum ratio of total input size to maximum input RFile size for\"\n          + \" running a major compaction. \"),\n  @Deprecated(since = \"2.1.0\", forRemoval = true)\n  TABLE_MAJC_COMPACTALL_IDLETIME(\"table.compaction.major.everything.idle\", \"1h\",\n      PropertyType.TIMEDURATION,\n      \"After a tablet has been idle (no mutations) for this time period it may\"\n          + \" have all of its RFiles compacted into one. There is no guarantee an idle\"\n          + \" tablet will be compacted. Compactions of idle tablets are only started\"\n          + \" when regular compactions are not running. Idle compactions only take\"\n          + \" place for tablets that have one or more RFiles.\"),\n  TABLE_SPLIT_THRESHOLD(\"table.split.threshold\", \"1G\", PropertyType.BYTES,\n      \"A tablet is split when the combined size of RFiles exceeds this amount.\"),\n  TABLE_MAX_END_ROW_SIZE(\"table.split.endrow.size.max\", \"10K\", PropertyType.BYTES,\n      \"Maximum size of end row\"),\n  @Deprecated\n  @ReplacedBy(property = Property.TSERV_WALOG_MAX_REFERENCED)\n  TABLE_MINC_LOGS_MAX(\"table.compaction.minor.logs.threshold\", \"3\", PropertyType.COUNT,\n      \"This property is deprecated since 2.0.0.\"),\n  TABLE_MINC_COMPACT_IDLETIME(\"table.compaction.minor.idle\", \"5m\", PropertyType.TIMEDURATION,\n      \"After a tablet has been idle (no mutations) for this time period it may have its \"\n          + \"in-memory map flushed to disk in a minor compaction. There is no guarantee an idle \"\n          + \"tablet will be compacted.\"),\n  TABLE_COMPACTION_DISPATCHER(\"table.compaction.dispatcher\",\n      SimpleCompactionDispatcher.class.getName(), PropertyType.CLASSNAME,\n      \"A configurable dispatcher that decides what comaction service a table should use.\"),\n  TABLE_COMPACTION_DISPATCHER_OPTS(\"table.compaction.dispatcher.opts.\", null, PropertyType.PREFIX,\n      \"Options for the table compaction dispatcher\"),\n  TABLE_COMPACTION_SELECTOR(\"table.compaction.selector\", \"\", PropertyType.CLASSNAME,\n      \"A configurable selector for a table that can periodically select file for mandatory \"\n          + \"compaction, even if the files do not meet the compaction ratio.\"),\n  TABLE_COMPACTION_SELECTOR_OPTS(\"table.compaction.selector.opts.\", null, PropertyType.PREFIX,\n      \"Options for the table compaction dispatcher\"),\n  TABLE_COMPACTION_CONFIGURER(\"table.compaction.configurer\", \"\", PropertyType.CLASSNAME,\n      \"A plugin that can dynamically configure compaction output files based on input files.\"),\n  TABLE_COMPACTION_CONFIGURER_OPTS(\"table.compaction.configurer.opts.\", null, PropertyType.PREFIX,\n      \"Options for the table compaction configuror\"),\n  @Deprecated(since = \"2.1.0\", forRemoval = true)\n  @ReplacedBy(property = TABLE_COMPACTION_SELECTOR)\n  TABLE_COMPACTION_STRATEGY(\"table.majc.compaction.strategy\",\n      \"org.apache.accumulo.tserver.compaction.DefaultCompactionStrategy\", PropertyType.CLASSNAME,\n      \"Deprecated since 2.1.0 See {% jlink -f org.apache.accumulo.core.spi.compaction}\"),\n  @Deprecated(since = \"2.1.0\", forRemoval = true)\n  @ReplacedBy(property = TABLE_COMPACTION_SELECTOR_OPTS)\n  TABLE_COMPACTION_STRATEGY_PREFIX(\"table.majc.compaction.strategy.opts.\", null,\n      PropertyType.PREFIX,\n      \"Properties in this category are used to configure the compaction strategy.\"),\n  TABLE_SCAN_DISPATCHER(\"table.scan.dispatcher\", SimpleScanDispatcher.class.getName(),\n      PropertyType.CLASSNAME,\n      \"This class is used to dynamically dispatch scans to configured scan executors.  Configured \"\n          + \"classes must implement {% jlink \" + ScanDispatcher.class.getName() + \" %} See \"\n          + \"[scan executors]({% durl administration/scan-executors %}) for an overview of why\"\n          + \" and how to use this property. This property is ignored for the root and metadata\"\n          + \" table.  The metadata table always dispatches to a scan executor named `meta`.\"),\n  TABLE_SCAN_DISPATCHER_OPTS(\"table.scan.dispatcher.opts.\", null, PropertyType.PREFIX,\n      \"Options for the table scan dispatcher\"),\n  TABLE_SCAN_MAXMEM(\"table.scan.max.memory\", \"512K\", PropertyType.BYTES,\n      \"The maximum amount of memory that will be used to cache results of a client query/scan. \"\n          + \"Once this limit is reached, the buffered data is sent to the client.\"),\n  TABLE_FILE_TYPE(\"table.file.type\", RFile.EXTENSION, PropertyType.STRING,\n      \"Change the type of file a table writes\"),\n  TABLE_LOAD_BALANCER(\"table.balancer\",\n      \"org.apache.accumulo.server.master.balancer.DefaultLoadBalancer\", PropertyType.STRING,\n      \"This property can be set to allow the LoadBalanceByTable load balancer\"\n          + \" to change the called Load Balancer for this table\"),\n  TABLE_FILE_COMPRESSION_TYPE(\"table.file.compress.type\", \"gz\", PropertyType.STRING,\n      \"Compression algorithm used on index and data blocks before they are\"\n          + \" written. Possible values: zstd, gz, snappy, lzo, none\"),\n  TABLE_FILE_COMPRESSED_BLOCK_SIZE(\"table.file.compress.blocksize\", \"100K\", PropertyType.BYTES,\n      \"The maximum size of data blocks in RFiles before they are compressed and written.\"),\n  TABLE_FILE_COMPRESSED_BLOCK_SIZE_INDEX(\"table.file.compress.blocksize.index\", \"128K\",\n      PropertyType.BYTES,\n      \"The maximum size of index blocks in RFiles before they are compressed and written.\"),\n  TABLE_FILE_BLOCK_SIZE(\"table.file.blocksize\", \"0B\", PropertyType.BYTES,\n      \"The HDFS block size used when writing RFiles. When set to 0B, the\"\n          + \" value/defaults of HDFS property 'dfs.block.size' will be used.\"),\n  TABLE_FILE_REPLICATION(\"table.file.replication\", \"0\", PropertyType.COUNT,\n      \"The number of replicas for a table's RFiles in HDFS. When set to 0, HDFS\"\n          + \" defaults are used.\"),\n  TABLE_FILE_MAX(\"table.file.max\", \"15\", PropertyType.COUNT,\n      \"The maximum number of RFiles each tablet in a table can have. When\"\n          + \" adjusting this property you may want to consider adjusting\"\n          + \" table.compaction.major.ratio also. Setting this property to 0 will make\"\n          + \" it default to tserver.scan.files.open.max-1, this will prevent a tablet\"\n          + \" from having more RFiles than can be opened. Setting this property low may\"\n          + \" throttle ingest and increase query performance.\"),\n  TABLE_FILE_SUMMARY_MAX_SIZE(\"table.file.summary.maxSize\", \"256K\", PropertyType.BYTES,\n      \"The maximum size summary that will be stored. The number of RFiles that\"\n          + \" had summary data exceeding this threshold is reported by\"\n          + \" Summary.getFileStatistics().getLarge(). When adjusting this consider the\"\n          + \" expected number RFiles with summaries on each tablet server and the\"\n          + \" summary cache size.\"),\n  TABLE_BLOOM_ENABLED(\"table.bloom.enabled\", \"false\", PropertyType.BOOLEAN,\n      \"Use bloom filters on this table.\"),\n  TABLE_BLOOM_LOAD_THRESHOLD(\"table.bloom.load.threshold\", \"1\", PropertyType.COUNT,\n      \"This number of seeks that would actually use a bloom filter must occur\"\n          + \" before a RFile's bloom filter is loaded. Set this to zero to initiate\"\n          + \" loading of bloom filters when a RFile is opened.\"),\n  TABLE_BLOOM_SIZE(\"table.bloom.size\", \"1048576\", PropertyType.COUNT,\n      \"Bloom filter size, as number of keys.\"),\n  TABLE_BLOOM_ERRORRATE(\"table.bloom.error.rate\", \"0.5%\", PropertyType.FRACTION,\n      \"Bloom filter error rate.\"),\n  TABLE_BLOOM_KEY_FUNCTOR(\"table.bloom.key.functor\",\n      \"org.apache.accumulo.core.file.keyfunctor.RowFunctor\", PropertyType.CLASSNAME,\n      \"A function that can transform the key prior to insertion and check of\"\n          + \" bloom filter. org.apache.accumulo.core.file.keyfunctor.RowFunctor,\"\n          + \" org.apache.accumulo.core.file.keyfunctor.ColumnFamilyFunctor, and\"\n          + \" org.apache.accumulo.core.file.keyfunctor.ColumnQualifierFunctor are\"\n          + \" allowable values. One can extend any of the above mentioned classes to\"\n          + \" perform specialized parsing of the key. \"),\n  TABLE_BLOOM_HASHTYPE(\"table.bloom.hash.type\", \"murmur\", PropertyType.STRING,\n      \"The bloom filter hash type\"),\n  TABLE_DURABILITY(\"table.durability\", \"sync\", PropertyType.DURABILITY,\n      \"The durability used to write to the write-ahead log. Legal values are:\"\n          + \" none, which skips the write-ahead log; log, which sends the data to the\"\n          + \" write-ahead log, but does nothing to make it durable; flush, which pushes\"\n          + \" data to the file system; and sync, which ensures the data is written to disk.\"),\n\n  TABLE_FAILURES_IGNORE(\"table.failures.ignore\", \"false\", PropertyType.BOOLEAN,\n      \"If you want queries for your table to hang or fail when data is missing\"\n          + \" from the system, then set this to false. When this set to true missing\"\n          + \" data will be reported but queries will still run possibly returning a\"\n          + \" subset of the data.\"),\n  TABLE_DEFAULT_SCANTIME_VISIBILITY(\"table.security.scan.visibility.default\", \"\",\n      PropertyType.STRING,\n      \"The security label that will be assumed at scan time if an entry does\"\n          + \" not have a visibility expression.\\n\"\n          + \"Note: An empty security label is displayed as []. The scan results\"\n          + \" will show an empty visibility even if the visibility from this\"\n          + \" setting is applied to the entry.\\n\"\n          + \"CAUTION: If a particular key has an empty security label AND its\"\n          + \" table's default visibility is also empty, access will ALWAYS be\"\n          + \" granted for users with permission to that table. Additionally, if this\"\n          + \" field is changed, all existing data with an empty visibility label\"\n          + \" will be interpreted with the new label on the next scan.\"),\n  TABLE_LOCALITY_GROUPS(\"table.groups.enabled\", \"\", PropertyType.STRING,\n      \"A comma separated list of locality group names to enable for this table.\"),\n  TABLE_CONSTRAINT_PREFIX(\"table.constraint.\", null, PropertyType.PREFIX,\n      \"Properties in this category are per-table properties that add\"\n          + \" constraints to a table. These properties start with the category\"\n          + \" prefix, followed by a number, and their values correspond to a fully\"\n          + \" qualified Java class that implements the Constraint interface.\\n\" + \"For example:\\n\"\n          + \"table.constraint.1 = org.apache.accumulo.core.constraints.MyCustomConstraint\\n\"\n          + \"and:\\n\" + \" table.constraint.2 = my.package.constraints.MySecondConstraint\"),\n  TABLE_INDEXCACHE_ENABLED(\"table.cache.index.enable\", \"true\", PropertyType.BOOLEAN,\n      \"Determines whether index block cache is enabled for a table.\"),\n  TABLE_BLOCKCACHE_ENABLED(\"table.cache.block.enable\", \"false\", PropertyType.BOOLEAN,\n      \"Determines whether data block cache is enabled for a table.\"),\n  TABLE_ITERATOR_PREFIX(\"table.iterator.\", null, PropertyType.PREFIX,\n      \"Properties in this category specify iterators that are applied at\"\n          + \" various stages (scopes) of interaction with a table. These properties\"\n          + \" start with the category prefix, followed by a scope (minc, majc, scan,\"\n          + \" etc.), followed by a period, followed by a name, as in\"\n          + \" table.iterator.scan.vers, or table.iterator.scan.custom. The values for\"\n          + \" these properties are a number indicating the ordering in which it is\"\n          + \" applied, and a class name such as:\\n\"\n          + \"table.iterator.scan.vers = 10,org.apache.accumulo.core.iterators.VersioningIterator\\n\"\n          + \"These iterators can take options if additional properties are set that\"\n          + \" look like this property, but are suffixed with a period, followed by 'opt'\"\n          + \" followed by another period, and a property name.\\n\"\n          + \"For example, table.iterator.minc.vers.opt.maxVersions = 3\"),\n  TABLE_ITERATOR_SCAN_PREFIX(TABLE_ITERATOR_PREFIX.getKey() + IteratorScope.scan.name() + \".\", null,\n      PropertyType.PREFIX, \"Convenience prefix to find options for the scan iterator scope\"),\n  TABLE_ITERATOR_MINC_PREFIX(TABLE_ITERATOR_PREFIX.getKey() + IteratorScope.minc.name() + \".\", null,\n      PropertyType.PREFIX, \"Convenience prefix to find options for the minc iterator scope\"),\n  TABLE_ITERATOR_MAJC_PREFIX(TABLE_ITERATOR_PREFIX.getKey() + IteratorScope.majc.name() + \".\", null,\n      PropertyType.PREFIX, \"Convenience prefix to find options for the majc iterator scope\"),\n  TABLE_LOCALITY_GROUP_PREFIX(\"table.group.\", null, PropertyType.PREFIX,\n      \"Properties in this category are per-table properties that define\"\n          + \" locality groups in a table. These properties start with the category\"\n          + \" prefix, followed by a name, followed by a period, and followed by a\"\n          + \" property for that group.\\n\"\n          + \"For example table.group.group1=x,y,z sets the column families for a\"\n          + \" group called group1. Once configured, group1 can be enabled by adding\"\n          + \" it to the list of groups in the \" + TABLE_LOCALITY_GROUPS.getKey() + \" property.\\n\"\n          + \"Additional group options may be specified for a named group by setting\"\n          + \" `table.group.<name>.opt.<key>=<value>`.\"),\n  TABLE_FORMATTER_CLASS(\"table.formatter\", DefaultFormatter.class.getName(), PropertyType.STRING,\n      \"The Formatter class to apply on results in the shell\"),\n  TABLE_INTERPRETER_CLASS(\"table.interepreter\", DefaultScanInterpreter.class.getName(),\n      PropertyType.STRING, \"The ScanInterpreter class to apply on scan arguments in the shell\"),\n  TABLE_CLASSPATH(\"table.classpath.context\", \"\", PropertyType.STRING,\n      \"Per table classpath context\"),\n  TABLE_REPLICATION(\"table.replication\", \"false\", PropertyType.BOOLEAN,\n      \"Is replication enabled for the given table\"),\n  TABLE_REPLICATION_TARGET(\"table.replication.target.\", null, PropertyType.PREFIX,\n      \"Enumerate a mapping of other systems which this table should replicate\"\n          + \" their data to. The key suffix is the identifying cluster name and the\"\n          + \" value is an identifier for a location on the target system, e.g. the ID\"\n          + \" of the table on the target to replicate to\"),\n  TABLE_SAMPLER(\"table.sampler\", \"\", PropertyType.CLASSNAME,\n      \"The name of a class that implements org.apache.accumulo.core.Sampler.\"\n          + \" Setting this option enables storing a sample of data which can be\"\n          + \" scanned. Always having a current sample can useful for query optimization\"\n          + \" and data comprehension. After enabling sampling for an existing table,\"\n          + \" a compaction is needed to compute the sample for existing data. The\"\n          + \" compact command in the shell has an option to only compact RFiles without\"\n          + \" sample data.\"),\n  TABLE_SAMPLER_OPTS(\"table.sampler.opt.\", null, PropertyType.PREFIX,\n      \"The property is used to set options for a sampler. If a sample had two\"\n          + \" options like hasher and modulous, then the two properties\"\n          + \" table.sampler.opt.hasher=${hash algorithm} and\"\n          + \" table.sampler.opt.modulous=${mod} would be set.\"),\n  TABLE_SUSPEND_DURATION(\"table.suspend.duration\", \"0s\", PropertyType.TIMEDURATION,\n      \"For tablets belonging to this table: When a tablet server dies, allow\"\n          + \" the tablet server this duration to revive before reassigning its tablets\"\n          + \" to other tablet servers.\"),\n  TABLE_SUMMARIZER_PREFIX(\"table.summarizer.\", null, PropertyType.PREFIX,\n      \"Prefix for configuring summarizers for a table. Using this prefix\"\n          + \" multiple summarizers can be configured with options for each one. Each\"\n          + \" summarizer configured should have a unique id, this id can be anything.\"\n          + \" To add a summarizer set \"\n          + \"`table.summarizer.<unique id>=<summarizer class name>.` If the summarizer has options\"\n          + \", then for each option set `table.summarizer.<unique id>.opt.<key>=<value>`.\"),\n  @Experimental\n  TABLE_DELETE_BEHAVIOR(\"table.delete.behavior\",\n      DeletingIterator.Behavior.PROCESS.name().toLowerCase(), PropertyType.STRING,\n      \"This determines what action to take when a delete marker is seen.\"\n          + \" Valid values are `process` and `fail` with `process` being the default.  When set to \"\n          + \"`process`, deletes will supress data.  When set to `fail`, any deletes seen will cause\"\n          + \" an exception. The purpose of `fail` is to support tables that never delete data and\"\n          + \" need fast seeks within the timestamp range of a column. When setting this to fail, \"\n          + \"also consider configuring the `\" + NoDeleteConstraint.class.getName() + \"` \"\n          + \"constraint.\"),\n\n  // VFS ClassLoader properties\n\n  // this property shouldn't be used directly; it exists solely to document the default value\n  // defined by its use in AccumuloVFSClassLoader when generating the property documentation\n  VFS_CLASSLOADER_SYSTEM_CLASSPATH_PROPERTY(\n      AccumuloVFSClassLoader.VFS_CLASSLOADER_SYSTEM_CLASSPATH_PROPERTY, \"\", PropertyType.STRING,\n      \"Configuration for a system level vfs classloader. Accumulo jar can be\"\n          + \" configured here and loaded out of HDFS.\"),\n  VFS_CONTEXT_CLASSPATH_PROPERTY(AccumuloVFSClassLoader.VFS_CONTEXT_CLASSPATH_PROPERTY, null,\n      PropertyType.PREFIX,\n      \"Properties in this category are define a classpath. These properties\"\n          + \" start  with the category prefix, followed by a context name. The value is\"\n          + \" a comma separated list of URIs. Supports full regex on filename alone.\"\n          + \" For example, general.vfs.context.classpath.cx1=hdfs://nn1:9902/mylibdir/*.jar.\"\n          + \" You can enable post delegation for a context, which will load classes from the\"\n          + \" context first instead of the parent first. Do this by setting\"\n          + \" `general.vfs.context.classpath.<name>.delegation=post`, where `<name>` is\"\n          + \" your context name. If delegation is not specified, it defaults to loading\"\n          + \" from parent classloader first.\"),\n\n  // this property shouldn't be used directly; it exists solely to document the default value\n  // defined by its use in AccumuloVFSClassLoader when generating the property documentation\n  VFS_CLASSLOADER_CACHE_DIR(AccumuloVFSClassLoader.VFS_CACHE_DIR, \"${java.io.tmpdir}\",\n      PropertyType.ABSOLUTEPATH,\n      \"The base directory to use for the vfs cache. The actual cached files will be located\"\n          + \" in a subdirectory, `accumulo-vfs-cache-<jvmProcessName>-${user.name}`, where\"\n          + \" `<jvmProcessName>` is determined by the JVM's internal management engine.\"\n          + \" The cache will keep a soft reference to all of the classes loaded in the VM.\"\n          + \" This should be on local disk on each node with sufficient space.\"),\n\n  // General properties for configuring replication\n  REPLICATION_PREFIX(\"replication.\", null, PropertyType.PREFIX,\n      \"Properties in this category affect the replication of data to other Accumulo instances.\"),\n  REPLICATION_PEERS(\"replication.peer.\", null, PropertyType.PREFIX,\n      \"Properties in this category control what systems data can be replicated to\"),\n  REPLICATION_PEER_USER(\"replication.peer.user.\", null, PropertyType.PREFIX,\n      \"The username to provide when authenticating with the given peer\"),\n  @Sensitive\n  REPLICATION_PEER_PASSWORD(\"replication.peer.password.\", null, PropertyType.PREFIX,\n      \"The password to provide when authenticating with the given peer\"),\n  REPLICATION_PEER_KEYTAB(\"replication.peer.keytab.\", null, PropertyType.PREFIX,\n      \"The keytab to use when authenticating with the given peer\"),\n  REPLICATION_NAME(\"replication.name\", \"\", PropertyType.STRING,\n      \"Name of this cluster with respect to replication. Used to identify this\"\n          + \" instance from other peers\"),\n  REPLICATION_MAX_WORK_QUEUE(\"replication.max.work.queue\", \"1000\", PropertyType.COUNT,\n      \"Upper bound of the number of files queued for replication\"),\n  REPLICATION_WORK_ASSIGNMENT_SLEEP(\"replication.work.assignment.sleep\", \"30s\",\n      PropertyType.TIMEDURATION, \"Amount of time to sleep between replication work assignment\"),\n  REPLICATION_WORKER_THREADS(\"replication.worker.threads\", \"4\", PropertyType.COUNT,\n      \"Size of the threadpool that each tabletserver devotes to replicating data\"),\n  REPLICATION_RECEIPT_SERVICE_PORT(\"replication.receipt.service.port\", \"10002\", PropertyType.PORT,\n      \"Listen port used by thrift service in tserver listening for replication\"),\n  REPLICATION_WORK_ATTEMPTS(\"replication.work.attempts\", \"10\", PropertyType.COUNT,\n      \"Number of attempts to try to replicate some data before giving up and\"\n          + \" letting it naturally be retried later\"),\n  REPLICATION_MIN_THREADS(\"replication.receiver.min.threads\", \"1\", PropertyType.COUNT,\n      \"Minimum number of threads for replication\"),\n  REPLICATION_THREADCHECK(\"replication.receiver.threadcheck.time\", \"30s\", PropertyType.TIMEDURATION,\n      \"The time between adjustments of the replication thread pool.\"),\n  REPLICATION_MAX_UNIT_SIZE(\"replication.max.unit.size\", \"64M\", PropertyType.BYTES,\n      \"Maximum size of data to send in a replication message\"),\n  REPLICATION_WORK_ASSIGNER(\"replication.work.assigner\",\n      \"org.apache.accumulo.master.replication.UnorderedWorkAssigner\", PropertyType.CLASSNAME,\n      \"Replication WorkAssigner implementation to use\"),\n  REPLICATION_DRIVER_DELAY(\"replication.driver.delay\", \"0s\", PropertyType.TIMEDURATION,\n      \"Amount of time to wait before the replication work loop begins in the master.\"),\n  REPLICATION_WORK_PROCESSOR_DELAY(\"replication.work.processor.delay\", \"0s\",\n      PropertyType.TIMEDURATION,\n      \"Amount of time to wait before first checking for replication work, not\"\n          + \" useful outside of tests\"),\n  REPLICATION_WORK_PROCESSOR_PERIOD(\"replication.work.processor.period\", \"0s\",\n      PropertyType.TIMEDURATION,\n      \"Amount of time to wait before re-checking for replication work, not\"\n          + \" useful outside of tests\"),\n  REPLICATION_TRACE_PERCENT(\"replication.trace.percent\", \"0.1\", PropertyType.FRACTION,\n      \"The sampling percentage to use for replication traces\"),\n  REPLICATION_RPC_TIMEOUT(\"replication.rpc.timeout\", \"2m\", PropertyType.TIMEDURATION,\n      \"Amount of time for a single replication RPC call to last before failing\"\n          + \" the attempt. See replication.work.attempts.\"),\n  // deprecated properties grouped at the end to reference property that replaces them\n  @Deprecated\n  @ReplacedBy(property = INSTANCE_VOLUMES)\n  INSTANCE_DFS_URI(\"instance.dfs.uri\", \"\", PropertyType.URI,\n      \"This property is deprecated since 1.6.0. \"\n          + \"A url accumulo should use to connect to DFS. If this is empty, accumulo\"\n          + \" will obtain this information from the hadoop configuration. This property\"\n          + \" will only be used when creating new files if instance.volumes is empty.\"\n          + \" After an upgrade to 1.6.0 Accumulo will start using absolute paths to\"\n          + \" reference files. Files created before a 1.6.0 upgrade are referenced via\"\n          + \" relative paths. Relative paths will always be resolved using this config\"\n          + \" (if empty using the hadoop config).\"),\n  @Deprecated\n  @ReplacedBy(property = INSTANCE_VOLUMES)\n  INSTANCE_DFS_DIR(\"instance.dfs.dir\", \"/accumulo\", PropertyType.ABSOLUTEPATH,\n      \"This property is deprecated since 1.6.0. \"\n          + \"HDFS directory in which accumulo instance will run. \"\n          + \"Do not change after accumulo is initialized.\"),\n  @Deprecated\n  GENERAL_CLASSPATHS(AccumuloClassLoader.GENERAL_CLASSPATHS, \"\", PropertyType.STRING,\n      \"This property is deprecated since 2.0.0. The class path should instead be configured\"\n          + \" by the launch environment (for example, accumulo-env.sh). A list of all\"\n          + \" of the places to look for a class. Order does matter, as it will look for\"\n          + \" the jar starting in the first location to the last. Supports full regex\"\n          + \" on filename alone.\"),\n  @Deprecated\n  @ReplacedBy(property = TABLE_DURABILITY)\n  TSERV_WAL_SYNC_METHOD(\"tserver.wal.sync.method\", \"hsync\", PropertyType.STRING,\n      \"This property is deprecated since 1.7.0. Use table.durability instead.\"),\n  @Deprecated\n  @ReplacedBy(property = TABLE_DURABILITY)\n  TABLE_WALOG_ENABLED(\"table.walog.enabled\", \"true\", PropertyType.BOOLEAN,\n      \"This setting is deprecated since 1.7.0. Use table.durability=none instead.\"),\n  @Deprecated\n  @ReplacedBy(property = TSERV_SCAN_EXECUTORS_DEFAULT_THREADS)\n  TSERV_READ_AHEAD_MAXCONCURRENT(\"tserver.readahead.concurrent.max\", \"16\", PropertyType.COUNT,\n      \"This property is deprecated since 2.0.0, use tserver.scan.executors.default.threads \"\n          + \"instead. The maximum number of concurrent read ahead that will execute. This \"\n          + \"effectively limits the number of long running scans that can run concurrently \"\n          + \"per tserver.\\\"\"),\n  @Deprecated\n  @ReplacedBy(property = TSERV_SCAN_EXECUTORS_META_THREADS)\n  TSERV_METADATA_READ_AHEAD_MAXCONCURRENT(\"tserver.metadata.readahead.concurrent.max\", \"8\",\n      PropertyType.COUNT,\n      \"This property is deprecated since 2.0.0, use tserver.scan.executors.meta.threads instead. \"\n          + \"The maximum number of concurrent metadata read ahead that will execute.\");\n\n  private String key;\n  private String defaultValue;\n  private String description;\n  private boolean annotationsComputed = false;\n  private boolean isSensitive;\n  private boolean isDeprecated;\n  private boolean isExperimental;\n  private Property replacedBy = null;\n  private PropertyType type;\n\n  private Property(String name, String defaultValue, PropertyType type, String description) {\n    this.key = name;\n    this.defaultValue = defaultValue;\n    this.description = description;\n    this.type = type;\n  }\n\n  @Override\n  public String toString() {\n    return this.key;\n  }\n\n  /**\n   * Gets the key (string) for this property.\n   *\n   * @return key\n   */\n  public String getKey() {\n    return this.key;\n  }\n\n  /**\n   * Gets the default value for this property. System properties are interpolated into the value if\n   * necessary.\n   *\n   * @return default value\n   */\n  public String getDefaultValue() {\n    return this.defaultValue;\n  }\n\n  /**\n   * Gets the type of this property.\n   *\n   * @return property type\n   */\n  public PropertyType getType() {\n    return this.type;\n  }\n\n  /**\n   * Gets the description of this property.\n   *\n   * @return description\n   */\n  public String getDescription() {\n    return this.description;\n  }\n\n  /**\n   * Checks if this property is experimental.\n   *\n   * @return true if this property is experimental\n   */\n  public boolean isExperimental() {\n    Preconditions.checkState(annotationsComputed,\n        \"precomputeAnnotations() must be called before calling this method\");\n    return isExperimental;\n  }\n\n  /**\n   * Checks if this property is deprecated.\n   *\n   * @return true if this property is deprecated\n   */\n  public boolean isDeprecated() {\n    Preconditions.checkState(annotationsComputed,\n        \"precomputeAnnotations() must be called before calling this method\");\n    return isDeprecated;\n  }\n\n  /**\n   * Checks if this property is sensitive.\n   *\n   * @return true if this property is sensitive\n   */\n  public boolean isSensitive() {\n    Preconditions.checkState(annotationsComputed,\n        \"precomputeAnnotations() must be called before calling this method\");\n    return isSensitive;\n  }\n\n  public Property replacedBy() {\n    Preconditions.checkState(annotationsComputed,\n        \"precomputeAnnotations() must be called before calling this method\");\n    return replacedBy;\n  }\n\n  private void precomputeAnnotations() {\n    isSensitive =\n        hasAnnotation(Sensitive.class) || hasPrefixWithAnnotation(getKey(), Sensitive.class);\n    isDeprecated =\n        hasAnnotation(Deprecated.class) || hasPrefixWithAnnotation(getKey(), Deprecated.class);\n    isExperimental =\n        hasAnnotation(Experimental.class) || hasPrefixWithAnnotation(getKey(), Experimental.class);\n    if (hasAnnotation(ReplacedBy.class)) {\n      ReplacedBy rb = getAnnotation(ReplacedBy.class);\n      if (rb != null) {\n        replacedBy = rb.property();\n      }\n    }\n    annotationsComputed = true;\n  }\n\n  /**\n   * Checks if a property with the given key is sensitive. The key must be for a valid property, and\n   * must either itself be annotated as sensitive or have a prefix annotated as sensitive.\n   *\n   * @param key\n   *          property key\n   * @return true if property is sensitive\n   */\n  public static boolean isSensitive(String key) {\n    Property prop = propertiesByKey.get(key);\n    if (prop != null) {\n      return prop.isSensitive();\n    } else {\n      for (String prefix : validPrefixes) {\n        if (key.startsWith(prefix)) {\n          if (propertiesByKey.get(prefix).isSensitive()) {\n            return true;\n          }\n        }\n      }\n    }\n    return false;\n  }\n\n  private <T extends Annotation> boolean hasAnnotation(Class<T> annotationType) {\n    Logger log = LoggerFactory.getLogger(getClass());\n    try {\n      for (Annotation a : getClass().getField(name()).getAnnotations()) {\n        if (annotationType.isInstance(a)) {\n          return true;\n        }\n      }\n    } catch (SecurityException | NoSuchFieldException e) {\n      log.error(\"{}\", e.getMessage(), e);\n    }\n    return false;\n  }\n\n  private <T extends Annotation> T getAnnotation(Class<T> annotationType) {\n    Logger log = LoggerFactory.getLogger(getClass());\n    try {\n      for (Annotation a : getClass().getField(name()).getAnnotations()) {\n        if (annotationType.isInstance(a)) {\n          @SuppressWarnings(\"unchecked\")\n          T uncheckedA = (T) a;\n          return uncheckedA;\n        }\n      }\n    } catch (SecurityException | NoSuchFieldException e) {\n      log.error(\"{}\", e.getMessage(), e);\n    }\n    return null;\n  }\n\n  private static <T extends Annotation> boolean hasPrefixWithAnnotation(String key,\n      Class<T> annotationType) {\n    for (String prefix : validPrefixes) {\n      if (key.startsWith(prefix)) {\n        if (propertiesByKey.get(prefix).hasAnnotation(annotationType)) {\n          return true;\n        }\n      }\n    }\n\n    return false;\n  }\n\n  private static HashSet<String> validTableProperties = null;\n  private static HashSet<String> validProperties = null;\n  private static HashSet<String> validPrefixes = null;\n  private static HashMap<String,Property> propertiesByKey = null;\n\n  private static boolean isKeyValidlyPrefixed(String key) {\n    for (String prefix : validPrefixes) {\n      if (key.startsWith(prefix)) {\n        return true;\n      }\n    }\n\n    return false;\n  }\n\n  /**\n   * Checks if the given property key is valid. A valid property key is either equal to the key of\n   * some defined property or has a prefix matching some prefix defined in this class.\n   *\n   * @param key\n   *          property key\n   * @return true if key is valid (recognized, or has a recognized prefix)\n   */\n  public static boolean isValidPropertyKey(String key) {\n    return validProperties.contains(key) || isKeyValidlyPrefixed(key);\n  }\n\n  /**\n   * Checks if the given property key is a valid property and is of type boolean.\n   *\n   * @param key\n   *          property key\n   * @return true if key is valid and is of type boolean, false otherwise\n   */\n  public static boolean isValidBooleanPropertyKey(String key) {\n    return validProperties.contains(key) && getPropertyByKey(key).getType() == PropertyType.BOOLEAN;\n  }\n\n  /**\n   * Checks if the given property key is for a valid table property. A valid table property key is\n   * either equal to the key of some defined table property (which each start with\n   * {@link #TABLE_PREFIX}) or has a prefix matching {@link #TABLE_CONSTRAINT_PREFIX},\n   * {@link #TABLE_ITERATOR_PREFIX}, or {@link #TABLE_LOCALITY_GROUP_PREFIX}.\n   *\n   * @param key\n   *          property key\n   * @return true if key is valid for a table property (recognized, or has a recognized prefix)\n   */\n  public static boolean isValidTablePropertyKey(String key) {\n    return validTableProperties.contains(key) || (key.startsWith(Property.TABLE_PREFIX.getKey())\n        && (key.startsWith(Property.TABLE_CONSTRAINT_PREFIX.getKey())\n            || key.startsWith(Property.TABLE_ITERATOR_PREFIX.getKey())\n            || key.startsWith(Property.TABLE_LOCALITY_GROUP_PREFIX.getKey())\n            || key.startsWith(Property.TABLE_COMPACTION_STRATEGY_PREFIX.getKey())\n            || key.startsWith(Property.TABLE_REPLICATION_TARGET.getKey())\n            || key.startsWith(Property.TABLE_ARBITRARY_PROP_PREFIX.getKey())\n            || key.startsWith(TABLE_SAMPLER_OPTS.getKey())\n            || key.startsWith(TABLE_SUMMARIZER_PREFIX.getKey())\n            || key.startsWith(TABLE_SCAN_DISPATCHER_OPTS.getKey())\n            || key.startsWith(TABLE_COMPACTION_DISPATCHER_OPTS.getKey())\n            || key.startsWith(TABLE_COMPACTION_CONFIGURER_OPTS.getKey())\n            || key.startsWith(TABLE_COMPACTION_SELECTOR_OPTS.getKey())));\n  }\n\n  private static final EnumSet<Property> fixedProperties =\n      EnumSet.of(Property.TSERV_CLIENTPORT, Property.TSERV_NATIVEMAP_ENABLED,\n          Property.TSERV_SCAN_MAX_OPENFILES, Property.MASTER_CLIENTPORT, Property.GC_PORT);\n\n  /**\n   * Checks if the given property may be changed via Zookeeper, but not recognized until the restart\n   * of some relevant daemon.\n   *\n   * @param key\n   *          property key\n   * @return true if property may be changed via Zookeeper but only heeded upon some restart\n   */\n  public static boolean isFixedZooPropertyKey(Property key) {\n    return fixedProperties.contains(key);\n  }\n\n  /**\n   * Checks if the given property key is valid for a property that may be changed via Zookeeper.\n   *\n   * @param key\n   *          property key\n   * @return true if key's property may be changed via Zookeeper\n   */\n  public static boolean isValidZooPropertyKey(String key) {\n    // white list prefixes\n    return key.startsWith(Property.TABLE_PREFIX.getKey())\n        || key.startsWith(Property.TSERV_PREFIX.getKey())\n        || key.startsWith(Property.MASTER_PREFIX.getKey())\n        || key.startsWith(Property.GC_PREFIX.getKey())\n        || key.startsWith(Property.GENERAL_ARBITRARY_PROP_PREFIX.getKey())\n        || key.startsWith(VFS_CONTEXT_CLASSPATH_PROPERTY.getKey())\n        || key.startsWith(REPLICATION_PREFIX.getKey());\n  }\n\n  /**\n   * Gets a {@link Property} instance with the given key.\n   *\n   * @param key\n   *          property key\n   * @return property, or null if not found\n   */\n  public static Property getPropertyByKey(String key) {\n    return propertiesByKey.get(key);\n  }\n\n  /**\n   * Checks if this property is expected to have a Java class as a value.\n   *\n   * @return true if this is property is a class property\n   */\n  public static boolean isClassProperty(String key) {\n    return (key.startsWith(Property.TABLE_CONSTRAINT_PREFIX.getKey())\n        && key.substring(Property.TABLE_CONSTRAINT_PREFIX.getKey().length()).split(\"\\\\.\").length\n            == 1)\n        || (key.startsWith(Property.TABLE_ITERATOR_PREFIX.getKey())\n            && key.substring(Property.TABLE_ITERATOR_PREFIX.getKey().length()).split(\"\\\\.\").length\n                == 2)\n        || key.equals(Property.TABLE_LOAD_BALANCER.getKey());\n  }\n\n  /**\n   * Creates a new instance of a class specified in a configuration property. The table classpath\n   * context is used if set.\n   *\n   * @param conf\n   *          configuration containing property\n   * @param property\n   *          property specifying class name\n   * @param base\n   *          base class of type\n   * @param defaultInstance\n   *          instance to use if creation fails\n   * @return new class instance, or default instance if creation failed\n   * @see AccumuloVFSClassLoader\n   */\n  public static <T> T createTableInstanceFromPropertyName(AccumuloConfiguration conf,\n      Property property, Class<T> base, T defaultInstance) {\n    String clazzName = conf.get(property);\n    String context = conf.get(TABLE_CLASSPATH);\n    return ConfigurationTypeHelper.getClassInstance(context, clazzName, base, defaultInstance);\n  }\n\n  /**\n   * Creates a new instance of a class specified in a configuration property.\n   *\n   * @param conf\n   *          configuration containing property\n   * @param property\n   *          property specifying class name\n   * @param base\n   *          base class of type\n   * @param defaultInstance\n   *          instance to use if creation fails\n   * @return new class instance, or default instance if creation failed\n   * @see AccumuloVFSClassLoader\n   */\n  public static <T> T createInstanceFromPropertyName(AccumuloConfiguration conf, Property property,\n      Class<T> base, T defaultInstance) {\n    String clazzName = conf.get(property);\n    return ConfigurationTypeHelper.getClassInstance(null, clazzName, base, defaultInstance);\n  }\n\n  /**\n   * Collects together properties from the given configuration pertaining to compaction strategies.\n   * The relevant properties all begin with the prefix in {@link #TABLE_COMPACTION_STRATEGY_PREFIX}.\n   * In the returned map, the prefix is removed from each property's key.\n   *\n   * @param tableConf\n   *          configuration\n   * @return map of compaction strategy property keys and values, with the detection prefix removed\n   *         from each key\n   */\n  public static Map<String,String> getCompactionStrategyOptions(AccumuloConfiguration tableConf) {\n    Map<String,String> longNames =\n        tableConf.getAllPropertiesWithPrefix(Property.TABLE_COMPACTION_STRATEGY_PREFIX);\n    Map<String,String> result = new HashMap<>();\n    for (Entry<String,String> entry : longNames.entrySet()) {\n      result.put(\n          entry.getKey().substring(Property.TABLE_COMPACTION_STRATEGY_PREFIX.getKey().length()),\n          entry.getValue());\n    }\n    return result;\n  }\n\n  static {\n    // Precomputing information here avoids :\n    // * Computing it each time a method is called\n    // * Using synch to compute the first time a method is called\n    propertiesByKey = new HashMap<>();\n    validPrefixes = new HashSet<>();\n    validProperties = new HashSet<>();\n\n    for (Property p : Property.values()) {\n      if (p.getType().equals(PropertyType.PREFIX)) {\n        validPrefixes.add(p.getKey());\n      } else {\n        validProperties.add(p.getKey());\n      }\n      propertiesByKey.put(p.getKey(), p);\n    }\n\n    validTableProperties = new HashSet<>();\n    for (Property p : Property.values()) {\n      if (!p.getType().equals(PropertyType.PREFIX)\n          && p.getKey().startsWith(Property.TABLE_PREFIX.getKey())) {\n        validTableProperties.add(p.getKey());\n      }\n    }\n\n    // order is very important here the following code relies on the maps and sets populated above\n    for (Property p : Property.values()) {\n      p.precomputeAnnotations();\n    }\n  }\n}\n", "hunk": "@@ -436,7 +436,7 @@ public enum Property {\n       DefaultCompactionPlanner.class.getName(), PropertyType.CLASSNAME,\n       \"Planner for default compaction service.\"),\n   TSERV_COMPACTION_SERVICE_DEFAULT_MAX_OPEN(\n-      \"tserver.compaction.service.default.planner.opts.maxOpen\", \"30\", PropertyType.COUNT,\n+      \"tserver.compaction.service.default.planner.opts.maxOpen\", \"10\", PropertyType.COUNT,\n       \"The maximum number of files a compaction will open\"),\n   TSERV_COMPACTION_SERVICE_DEFAULT_EXECUTORS(\n       \"tserver.compaction.service.default.planner.opts.executors\",\n", "comment": "Presumably, these new properties are for major compactions only. The property prefix could be made consistent with other `tserver.compaction.major` properties.", "ids": ["17465", "bd206be8ed50ebf1fee76dc8cf1e9820ef8ca5b8", "34bc2a8ebb6783268cf4bdb339d41bc46a727cbd"], "repo": "apache/accumulo", "ghid": 1605, "old": "       DefaultCompactionPlanner.class.getName(), PropertyType.CLASSNAME,\n       \"Planner for default compaction service.\"),\n   TSERV_COMPACTION_SERVICE_DEFAULT_MAX_OPEN(\n-      \"tserver.compaction.service.default.planner.opts.maxOpen\", \"30\", PropertyType.COUNT,\n       \"The maximum number of files a compaction will open\"),\n   TSERV_COMPACTION_SERVICE_DEFAULT_EXECUTORS(\n       \"tserver.compaction.service.default.planner.opts.executors\",", "new": "       DefaultCompactionPlanner.class.getName(), PropertyType.CLASSNAME,\n       \"Planner for default compaction service.\"),\n   TSERV_COMPACTION_SERVICE_DEFAULT_MAX_OPEN(\n+      \"tserver.compaction.service.default.planner.opts.maxOpen\", \"10\", PropertyType.COUNT,\n       \"The maximum number of files a compaction will open\"),\n   TSERV_COMPACTION_SERVICE_DEFAULT_EXECUTORS(\n       \"tserver.compaction.service.default.planner.opts.executors\",", "lang": "java", "norm_lang": "java"}
{"old_hunk": "@@ -0,0 +1,181 @@\n+class GraphStorage(object):\n+    @property\n+    def ntypes(self):\n+        \"\"\"The list of node types.\"\"\"\n+        pass\n+\n+    @property\n+    def ndata(self):\n+        \"\"\"Node data.\n+\n+        For graphs with one node type it's a dict whose keys are feature names and values are\n+        either tensors or FeatureStorage objects.\n+\n+        For multiple node types it's a dict of dict.  The outer keys are feature names\n+        and the inner keys are node type names:\n+\n+        .. code::\n+\n+           self.ndata[feature_name][ntype]\n+        \"\"\"\n+        pass\n+\n+    # Required in Link Prediction\n+    @property\n+    def etypes(self):\n+        \"\"\"The list of edge types.\"\"\"\n+        pass\n+\n+    # Required in Link Prediction\n+    @property\n+    def canonical_etypes(self):", "oldf": "\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n<html>\n  <head>\n    <title>503 Backend is unhealthy</title>\n  </head>\n  <body>\n    <h1>Error 503 Backend is unhealthy</h1>\n    <p>Backend is unhealthy</p>\n    <h3>Guru Mediation:</h3>\n    <p>Details: cache-sea4471-SEA 1645545920 4126993929</p>\n    <hr>\n    <p>Varnish cache server</p>\n  </body>\n</html>\n", "hunk": "@@ -10,7 +10,7 @@\n     <h1>Error 503 Backend is unhealthy</h1>\n     <p>Backend is unhealthy</p>\n     <h3>Guru Mediation:</h3>\n-    <p>Details: cache-sea4471-SEA 1645545920 4126993929</p>\n+    <p>Details: cache-sea4449-SEA 1645545920 1526457240</p>\n     <hr>\n     <p>Varnish cache server</p>\n   </body>\n", "comment": "The difference between `etypes` and `canonical_etypes` is pretty subtle, and I'm concerned could be a source of errors for non DGLHeteroGraph implementations. Can we simplify this interface to only support canonical_etypes? Or have a separate set of functions for querying graph metadata? e.g.: ``` def get_src_type(self, etype): ... def get_dst_type(self, etype): ... ``` This would require every etype to be unique however.", "ids": ["45800", "c2e7fcb7b2dd34ee37b4f0f67c8daa5046b729a1", "d97e17baa597a803b7e49a11c90d08d4b83f100d"], "repo": "dmlc/dgl", "ghid": 3665, "old": "     <h1>Error 503 Backend is unhealthy</h1>\n     <p>Backend is unhealthy</p>\n     <h3>Guru Mediation:</h3>\n-    <p>Details: cache-sea4471-SEA 1645545920 4126993929</p>\n     <hr>\n     <p>Varnish cache server</p>\n   </body>", "new": "     <h1>Error 503 Backend is unhealthy</h1>\n     <p>Backend is unhealthy</p>\n     <h3>Guru Mediation:</h3>\n+    <p>Details: cache-sea4449-SEA 1645545920 1526457240</p>\n     <hr>\n     <p>Varnish cache server</p>\n   </body>", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -26,16 +26,22 @@\n import azurelinuxagent.common.logger as logger\n import azurelinuxagent.common.utils.restutil as restutil\n import azurelinuxagent.common.utils.textutil as textutil\n+from azurelinuxagent.common.utils.textutil import parse_doc, findall, find, findtext, \\\n+    getattrib, gettext, remove_bom, get_bytes_from_pem", "oldf": "# Microsoft Azure Linux Agent\n#\n# Copyright 2014 Microsoft Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Requires Python 2.4+ and Openssl 1.0+\n\nimport json\nimport shutil\nimport os\nimport time\nfrom azurelinuxagent.common.exception import ProtocolError, HttpError\nfrom azurelinuxagent.common.future import httpclient, ustr\nimport azurelinuxagent.common.conf as conf\nimport azurelinuxagent.common.logger as logger\nimport azurelinuxagent.common.utils.restutil as restutil\nimport azurelinuxagent.common.utils.textutil as textutil\nfrom azurelinuxagent.common.utils.textutil import parse_doc, findall, find, findtext, \\\n    getattrib, gettext, remove_bom, get_bytes_from_pem\nimport azurelinuxagent.common.utils.fileutil as fileutil\nfrom azurelinuxagent.common.utils.cryptutil import CryptUtil\nfrom azurelinuxagent.common.protocol.restapi import *\nimport azurelinuxagent.common.utils.shellutil as shellutil\n\nMETADATA_ENDPOINT='169.254.169.254'\nAPIVERSION='2015-05-01-preview'\nBASE_URI = \"http://{0}/Microsoft.Compute/{1}?api-version={2}{3}\"\n\nTRANSPORT_PRV_FILE_NAME = \"V2TransportPrivate.pem\"\nTRANSPORT_CERT_FILE_NAME = \"V2TransportCert.pem\"\nP7M_FILE_NAME = \"Certificates.p7m\"\nP7B_FILE_NAME = \"Certificates.p7b\"\nPEM_FILE_NAME = \"Certificates.pem\"\n\n#TODO remote workarround for azure stack \nMAX_PING = 30\nRETRY_PING_INTERVAL = 10\n\ndef _add_content_type(headers):\n    if headers is None:\n        headers = {}\n    headers[\"content-type\"] = \"application/json\"\n    return headers\n\nclass MetadataProtocol(Protocol):\n\n    def __init__(self, apiversion=APIVERSION, endpoint=METADATA_ENDPOINT):\n        self.apiversion = apiversion\n        self.endpoint = endpoint\n        self.identity_uri = BASE_URI.format(self.endpoint, \"identity\",\n                                            self.apiversion, \"&$expand=*\")\n        self.cert_uri = BASE_URI.format(self.endpoint, \"certificates\",\n                                        self.apiversion, \"&$expand=*\")\n        self.ext_uri = BASE_URI.format(self.endpoint, \"extensionHandlers\",\n                                       self.apiversion, \"&$expand=*\")\n        self.vmagent_uri = BASE_URI.format(self.endpoint, \"vmAgentVersions\",\n                                           self.apiversion, \"&$expand=*\")\n        self.provision_status_uri = BASE_URI.format(self.endpoint,\n                                                    \"provisioningStatus\",\n                                                    self.apiversion, \"\")\n        self.vm_status_uri = BASE_URI.format(self.endpoint, \"status/vmagent\",\n                                             self.apiversion, \"\")\n        self.ext_status_uri = BASE_URI.format(self.endpoint, \n                                              \"status/extensions/{0}\",\n                                              self.apiversion, \"\")\n        self.event_uri = BASE_URI.format(self.endpoint, \"status/telemetry\",\n                                         self.apiversion, \"\")\n        self.certs = None\n\n    def _get_data(self, url, headers=None):\n        try:\n            resp = restutil.http_get(url, headers=headers)\n        except HttpError as e:\n            raise ProtocolError(ustr(e))\n\n        if resp.status != httpclient.OK:\n            raise ProtocolError(\"{0} - GET: {1}\".format(resp.status, url))\n\n        data = resp.read()\n        etag = resp.getheader('ETag')\n        if data is None:\n            return None\n        data = json.loads(ustr(data, encoding=\"utf-8\"))\n        return data, etag\n\n    def _put_data(self, url, data, headers=None):\n        headers = _add_content_type(headers) \n        try:\n            resp = restutil.http_put(url, json.dumps(data), headers=headers)\n        except HttpError as e:\n            raise ProtocolError(ustr(e))\n        if resp.status != httpclient.OK:\n            raise ProtocolError(\"{0} - PUT: {1}\".format(resp.status, url))\n\n    def _post_data(self, url, data, headers=None):\n        headers = _add_content_type(headers) \n        try:\n            resp = restutil.http_post(url, json.dumps(data), headers=headers)\n        except HttpError as e:\n            raise ProtocolError(ustr(e))\n        if resp.status != httpclient.CREATED:\n            raise ProtocolError(\"{0} - POST: {1}\".format(resp.status, url))\n    \n    def _get_trans_cert(self):\n        trans_crt_file = os.path.join(conf.get_lib_dir(), \n                                      TRANSPORT_CERT_FILE_NAME)\n        if not os.path.isfile(trans_crt_file):\n            raise ProtocolError(\"{0} is missing.\".format(trans_crt_file))\n        content = fileutil.read_file(trans_crt_file)\n        return textutil.get_bytes_from_pem(content)\n\n    def detect(self):\n        self.get_vminfo()\n        trans_prv_file = os.path.join(conf.get_lib_dir(), \n                                      TRANSPORT_PRV_FILE_NAME)\n        trans_cert_file = os.path.join(conf.get_lib_dir(), \n                                       TRANSPORT_CERT_FILE_NAME)\n        cryptutil = CryptUtil(conf.get_openssl_cmd())\n        cryptutil.gen_transport_cert(trans_prv_file, trans_cert_file)\n\n        #\"Install\" the cert and private key to /var/lib/waagent\n        thumbprint = cryptutil.get_thumbprint_from_crt(trans_cert_file)\n        prv_file = os.path.join(conf.get_lib_dir(), \n                                \"{0}.prv\".format(thumbprint))\n        crt_file = os.path.join(conf.get_lib_dir(), \n                                \"{0}.crt\".format(thumbprint))\n        shutil.copyfile(trans_prv_file, prv_file)\n        shutil.copyfile(trans_cert_file, crt_file)\n        self.update_goal_state(forced=True)\n\n    def get_vminfo(self):\n        vminfo = VMInfo()\n        data, etag = self._get_data(self.identity_uri)\n        set_properties(\"vminfo\", vminfo, data)\n        return vminfo\n\n    def get_certs(self):\n        certlist = CertList()\n        certificatedata = CertificateData()\n        data, etag = self._get_data(self.cert_uri)\n\n        set_properties(\"certlist\", certlist, data)\n\n        cert_list = get_properties(certlist)\n\n        headers = {\n            \"x-ms-vmagent-public-x509-cert\": self._get_trans_cert()\n        }\n\n        for cert_i in cert_list[\"certificates\"]:\n            certificate_data_uri = cert_i['certificateDataUri']\n            data, etag = self._get_data(certificate_data_uri, headers=headers)\n            set_properties(\"certificatedata\", certificatedata, data)\n            json_certificate_data = get_properties(certificatedata)\n\n            self.certs = Certificates(self, json_certificate_data)\n\n        if self.certs is None:\n            return None\n        return self.certs\n\n    def get_vmagent_manifests(self, last_etag=None):\n        manifests = VMAgentManifestList()\n        self.update_goal_state()\n        data, etag = self._get_data(self.vmagent_uri)\n        if last_etag == None or last_etag < etag:\n            set_properties(\"vmAgentManifests\", manifests.vmAgentManifests, data)\n        return manifests, etag\n\n    def get_vmagent_pkgs(self, vmagent_manifest):\n        #Agent package is the same with extension handler\n        vmagent_pkgs = ExtHandlerPackageList()\n        data = None\n        for manifest_uri in vmagent_manifest.versionsManifestUris:\n            try:\n                data = self._get_data(manifest_uri.uri)\n                break\n            except ProtocolError as e:\n                logger.warn(\"Failed to get vmagent versions: {0}\", e)\n                logger.info(\"Retry getting vmagent versions\")\n        if data is None:\n            raise ProtocolError((\"Failed to get versions for vm agent: {0}\"\n                                 \"\").format(vmagent_manifest.family))\n        set_properties(\"vmAgentVersions\", vmagent_pkgs, data)\n        # TODO: What etag should this return?\n        return vmagent_pkgs\n\n    def get_ext_handlers(self, last_etag=None):\n        self.update_goal_state()\n        headers = {\n            \"x-ms-vmagent-public-x509-cert\": self._get_trans_cert()\n        }\n        ext_list = ExtHandlerList()\n        data, etag = self._get_data(self.ext_uri, headers=headers)\n        if last_etag == None or last_etag < etag:\n            set_properties(\"extensionHandlers\", ext_list.extHandlers, data)\n        return ext_list, etag\n\n    def get_ext_handler_pkgs(self, ext_handler):\n        ext_handler_pkgs = ExtHandlerPackageList()\n        data = None\n        for version_uri in ext_handler.versionUris:\n            try:\n                data, etag = self._get_data(version_uri.uri)\n                break\n            except ProtocolError as e:\n                logger.warn(\"Failed to get version uris: {0}\", e)\n                logger.info(\"Retry getting version uris\")\n        set_properties(\"extensionPackages\", ext_handler_pkgs, data)\n        return ext_handler_pkgs\n\n    def report_provision_status(self, provision_status):\n        validate_param('provisionStatus', provision_status, ProvisionStatus)\n        data = get_properties(provision_status)\n        self._put_data(self.provision_status_uri, data)\n\n    def report_vm_status(self, vm_status):\n        validate_param('vmStatus', vm_status, VMStatus)\n        data = get_properties(vm_status)\n        #TODO code field is not implemented for metadata protocol yet. Remove it\n        handler_statuses = data['vmAgent']['extensionHandlers']\n        for handler_status in handler_statuses:\n            try:\n                handler_status.pop('code', None)\n            except KeyError:\n                pass\n\n        self._put_data(self.vm_status_uri, data)\n\n    def report_ext_status(self, ext_handler_name, ext_name, ext_status):\n        validate_param('extensionStatus', ext_status, ExtensionStatus)\n        data = get_properties(ext_status)\n        uri = self.ext_status_uri.format(ext_name)\n        self._put_data(uri, data)\n\n    def report_event(self, events):\n        #TODO disable telemetry for azure stack test\n        #validate_param('events', events, TelemetryEventList)\n        #data = get_properties(events)\n        #self._post_data(self.event_uri, data)\n        pass\n\n    def update_certs(self):\n        logger.info(\"Inside MetadataClient.update_certs\")\n        certificates = self.get_certs()\n        return certificates.cert_list\n\n    def update_goal_state(self, forced=False, max_retry=3):\n        logger.info(\"Inside update_goal_state\")\n        # Start updating goalstate, retry on 410\n        for retry in range(0, max_retry):\n            try:\n                self.update_certs()\n                return\n            except :\n                logger.info(\"Incarnation is out of date. Update goalstate.\")\n\n        raise ProtocolError(\"Exceeded max retry updating goal state\")\n\n\nclass Certificates(object):\n    \"\"\"\n    Object containing certificates of host and provisioned user.\n    \"\"\"\n\n    def __init__(self, client, json_text):\n        self.cert_list = CertList()\n        self.parse(json_text)\n\n    def parse(self, json_text):\n        \"\"\"\n        Parse multiple certificates into seperate files.\n        \"\"\"\n\n        data = json_text[\"certificateData\"]\n        if data is None:\n            logger.verbose(\"No data in json_text received!\")\n            return\n\n        cryptutil = CryptUtil(conf.get_openssl_cmd())\n\n        p7b_file = os.path.join(conf.get_lib_dir(), P7B_FILE_NAME)\n        # Wrapping the certificate lines.\n        shellutil.run_get_output(\"echo \" + data + \" | base64 -d > \" + p7b_file)\n        ret, data = shellutil.run_get_output(\"openssl pkcs7 -text -in \" + p7b_file + \" -inform der | grep -v '^-----' \")\n\n        p7m_file = os.path.join(conf.get_lib_dir(), P7M_FILE_NAME)\n        p7m = (\"MIME-Version:1.0\\n\"\n               \"Content-Disposition: attachment; filename=\\\"{0}\\\"\\n\"\n               \"Content-Type: application/x-pkcs7-mime; name=\\\"{1}\\\"\\n\"\n               \"Content-Transfer-Encoding: base64\\n\"\n               \"\\n\"\n               \"{2}\").format(p7m_file, p7m_file, data)\n\n        self.save_cache(p7m_file, p7m)\n\n        trans_prv_file = os.path.join(conf.get_lib_dir(),\n                                      TRANSPORT_PRV_FILE_NAME)\n        trans_cert_file = os.path.join(conf.get_lib_dir(),\n                                       TRANSPORT_CERT_FILE_NAME)\n        pem_file = os.path.join(conf.get_lib_dir(), PEM_FILE_NAME)\n        # decrypt certificates\n        cryptutil.decrypt_p7m(p7m_file, trans_prv_file, trans_cert_file,\n                              pem_file)\n\n        # The parsing process use public key to match prv and crt.\n        buf = []\n        begin_crt = False\n        begin_prv = False\n        prvs = {}\n        thumbprints = {}\n        index = 0\n        v1_cert_list = []\n        with open(pem_file) as pem:\n            for line in pem.readlines():\n                buf.append(line)\n                if re.match(r'[-]+BEGIN.*KEY[-]+', line):\n                    begin_prv = True\n                elif re.match(r'[-]+BEGIN.*CERTIFICATE[-]+', line):\n                    begin_crt = True\n                elif re.match(r'[-]+END.*KEY[-]+', line):\n                    tmp_file = self.write_to_tmp_file(index, 'prv', buf)\n                    pub = cryptutil.get_pubkey_from_prv(tmp_file)\n                    prvs[pub] = tmp_file\n                    buf = []\n                    index += 1\n                    begin_prv = False\n                elif re.match(r'[-]+END.*CERTIFICATE[-]+', line):\n                    tmp_file = self.write_to_tmp_file(index, 'crt', buf)\n                    pub = cryptutil.get_pubkey_from_crt(tmp_file)\n                    thumbprint = cryptutil.get_thumbprint_from_crt(tmp_file)\n                    thumbprints[pub] = thumbprint\n                    # Rename crt with thumbprint as the file name\n                    crt = \"{0}.crt\".format(thumbprint)\n                    v1_cert_list.append({\n                        \"name\": None,\n                        \"thumbprint\": thumbprint\n                    })\n                    os.rename(tmp_file, os.path.join(conf.get_lib_dir(), crt))\n                    buf = []\n                    index += 1\n                    begin_crt = False\n\n        # Rename prv key with thumbprint as the file name\n        for pubkey in prvs:\n            thumbprint = thumbprints[pubkey]\n            if thumbprint:\n                tmp_file = prvs[pubkey]\n                prv = \"{0}.prv\".format(thumbprint)\n                os.rename(tmp_file, os.path.join(conf.get_lib_dir(), prv))\n\n        for v1_cert in v1_cert_list:\n            cert = Cert()\n            set_properties(\"certs\", cert, v1_cert)\n            self.cert_list.certificates.append(cert)\n\n    def save_cache(self, local_file, data):\n        try:\n            fileutil.write_file(local_file, data)\n        except IOError as e:\n            raise ProtocolError(\"Failed to write cache: {0}\".format(e))\n\n    def write_to_tmp_file(self, index, suffix, buf):\n        file_name = os.path.join(conf.get_lib_dir(),\n                                 \"{0}.{1}\".format(index, suffix))\n        self.save_cache(file_name, \"\".join(buf))\n        return file_name\n", "hunk": "@@ -26,8 +26,6 @@ import azurelinuxagent.common.conf as conf\n import azurelinuxagent.common.logger as logger\n import azurelinuxagent.common.utils.restutil as restutil\n import azurelinuxagent.common.utils.textutil as textutil\n-from azurelinuxagent.common.utils.textutil import parse_doc, findall, find, findtext, \\\n-    getattrib, gettext, remove_bom, get_bytes_from_pem\n import azurelinuxagent.common.utils.fileutil as fileutil\n from azurelinuxagent.common.utils.cryptutil import CryptUtil\n from azurelinuxagent.common.protocol.restapi import *\n", "comment": "@SRIKKANTH We've tended to import that package name and then reference methods within the package by prepending the name. Could we adopt that pattern here as well (e.g., `textutil.parse_doc` vs. `parse_doc`).", "ids": ["11060", "80ae8ad65677ddb4af8fdbf5e208f5084be90f05", "5c0ec65612b5cdf14cba23279c2b96274ca2f21b"], "repo": "Azure/WALinuxAgent", "ghid": 413, "old": " import azurelinuxagent.common.logger as logger\n import azurelinuxagent.common.utils.restutil as restutil\n import azurelinuxagent.common.utils.textutil as textutil\n-from azurelinuxagent.common.utils.textutil import parse_doc, findall, find, findtext, \\\n-    getattrib, gettext, remove_bom, get_bytes_from_pem\n import azurelinuxagent.common.utils.fileutil as fileutil\n from azurelinuxagent.common.utils.cryptutil import CryptUtil\n from azurelinuxagent.common.protocol.restapi import *", "new": " import azurelinuxagent.common.logger as logger\n import azurelinuxagent.common.utils.restutil as restutil\n import azurelinuxagent.common.utils.textutil as textutil\n import azurelinuxagent.common.utils.fileutil as fileutil\n from azurelinuxagent.common.utils.cryptutil import CryptUtil\n from azurelinuxagent.common.protocol.restapi import *", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -1,162 +1,85 @@\n // @flow\n-import React, { Component } from 'react';\n-import { observer } from 'mobx-react';\n+import React, { useState } from 'react';\n import classnames from 'classnames';\n+import { PoolPopOver } from './PoolPopOver';\n import styles from './ThumbPool.scss';\n import { getColorFromRange } from '../../../utils/colors';\n-import TooltipPool from './TooltipPool';\n import StakePool from '../../../domains/StakePool';\n-import { STAKE_POOL_TOOLTIP_HOVER_WAIT } from '../../../config/timingConfig';\n-import { getRelativePosition } from '../../../utils/domManipulation';\n import ThumbSelectedPool from './ThumbSelectedPool';\n import ThumbPoolContent from './ThumbPoolContent';\n \n-type Props = {\n+/**\n+ * Stake pool thumbnail component that uses the PoolPopOver\n+ * to show stake pool information on click (by default) or\n+ * highlightOnHover (configurable via prop).\n+ *\n+ * It also renders differently depending on the isSelected prop\n+ */\n+\n+export function ThumbPool(props: {\n   currentTheme: string,\n-  isHighlighted: boolean,\n-  onClick?: Function,\n-  onClose: Function,\n-  onHover?: Function,\n+  isSelected: boolean,\n+  highlightOnHover?: boolean,\n+  highlightWithDelay?: boolean,\n   onOpenExternalLink: Function,\n-  onSelect: Function,\n+  onSelect?: (poolId: string) => void,\n+  selectOnClick?: boolean,\n   showWithSelectButton?: boolean,\n-  showSelected?: boolean,\n   stakePool: StakePool,\n-  isSelected?: ?Function,\n   containerClassName: string,\n   numberOfRankedStakePools: number,\n   disabledStakePoolId: ?string,\n-};\n-\n-type State = {\n-  top: number,\n-  left: number,\n-};\n-\n-@observer\n-export class ThumbPool extends Component<Props, State> {\n-  state = {\n-    top: 0,\n-    left: 0,\n-  };\n-\n-  hoverWait: TimeoutID;\n-\n-  handleHover = (stakePoolId: string) => {\n-    clearTimeout(this.hoverWait);\n-    this.hoverWait = setTimeout(() => {\n-      if (this.props.onHover) this.props.onHover(stakePoolId);\n-    }, STAKE_POOL_TOOLTIP_HOVER_WAIT);\n-  };\n-\n-  handleClose = (stakePoolId: string) => {\n-    clearTimeout(this.hoverWait);\n-    this.props.onClose(stakePoolId);\n-  };\n-\n-  handleOpen = (event: SyntheticMouseEvent<HTMLElement>) => {\n-    const {\n-      onClose,\n-      onClick,\n-      onHover,\n-      isHighlighted,\n-      stakePool,\n-      containerClassName,\n-    } = this.props;\n-    if (isHighlighted) return onClose();\n-    event.persist();\n-    const targetElement = event.target;\n-    if (targetElement instanceof HTMLElement) {\n-      const { top, left } = getRelativePosition(\n-        targetElement,\n-        `.${containerClassName}`\n-      );\n-      this.setState({ top, left });\n-      if (onHover) {\n-        this.handleHover(stakePool.id);\n-      } else if (onClick) {\n-        onClick(stakePool.id);\n-      }\n-    }\n-    return false;\n-  };\n-\n-  handleSelect = () => {\n-    const { stakePool, onSelect } = this.props;\n-    onSelect(stakePool.id);\n-  };\n-\n-  render() {\n-    const {\n-      currentTheme,\n-      isHighlighted,\n-      isSelected,\n-      onClose,\n-      onHover,\n-      onOpenExternalLink,\n-      showWithSelectButton,\n-      showSelected,\n-      stakePool,\n-      containerClassName,\n-      numberOfRankedStakePools,\n-      disabledStakePoolId,\n-    } = this.props;\n-    const { top, left } = this.state;\n-\n-    const { ranking, id } = stakePool;\n-    const color = getColorFromRange(ranking, numberOfRankedStakePools);\n-    const isDisabled = disabledStakePoolId === id;\n+}) {\n+  const { isSelected, numberOfRankedStakePools, stakePool } = props;\n+  const { ranking, id } = stakePool;\n+  const color = getColorFromRange(ranking, numberOfRankedStakePools);\n+  const isDisabled = props.disabledStakePoolId === id;\n+  const [isHighlighted, setIsHighlighted] = useState(false);\n \n-    const contentClassnames = classnames([\n-      styles.content,\n-      isDisabled ? styles.disabled : null,\n-      isSelected && showSelected ? styles.isSelected : null,\n-      isHighlighted ? styles.isHighlighted : null,\n-      onHover ? styles.isOnHover : null,\n-    ]);\n+  const contentClassnames = classnames([\n+    styles.content,\n+    isDisabled ? styles.disabled : null,\n+    isHighlighted ? styles.isHighlighted : null,\n+    props.highlightOnHover ? styles.isOnHover : null,", "oldf": "// @flow\nimport React, { useState } from 'react';\nimport classnames from 'classnames';\nimport { PoolPopOver } from './PoolPopOver';\nimport styles from './ThumbPool.scss';\nimport { getColorFromRange } from '../../../utils/colors';\nimport StakePool from '../../../domains/StakePool';\nimport ThumbSelectedPool from './ThumbSelectedPool';\nimport ThumbPoolContent from './ThumbPoolContent';\n\n/**\n * Stake pool thumbnail component that uses the PoolPopOver\n * to show stake pool information on click (by default) or\n * highlightOnHover (configurable via prop).\n *\n * It also renders differently depending on the isSelected prop\n */\n\nexport function ThumbPool(props: {\n  currentTheme: string,\n  isSelected: boolean,\n  highlightOnHover?: boolean,\n  highlightWithDelay?: boolean,\n  onOpenExternalLink: Function,\n  onSelect?: (poolId: string) => void,\n  selectOnClick?: boolean,\n  showWithSelectButton?: boolean,\n  stakePool: StakePool,\n  containerClassName: string,\n  numberOfRankedStakePools: number,\n  disabledStakePoolId: ?string,\n}) {\n  const { isSelected, numberOfRankedStakePools, stakePool } = props;\n  const { ranking, id } = stakePool;\n  const color = getColorFromRange(ranking, numberOfRankedStakePools);\n  const isDisabled = props.disabledStakePoolId === id;\n  const [isHighlighted, setIsHighlighted] = useState(false);\n\n  const contentClassnames = classnames([\n    styles.content,\n    isDisabled ? styles.disabled : null,\n    isHighlighted ? styles.isHighlighted : null,\n    props.highlightOnHover ? styles.isOnHover : null,\n  ]);\n\n  const content = isSelected ? (\n    <ThumbSelectedPool\n      stakePool={stakePool}\n      numberOfRankedStakePools={numberOfRankedStakePools}\n    />\n  ) : (\n    <ThumbPoolContent\n      stakePool={stakePool}\n      numberOfRankedStakePools={numberOfRankedStakePools}\n    />\n  );\n\n  return (\n    <div className={styles.component}>\n      <PoolPopOver\n        color={color}\n        currentTheme={props.currentTheme}\n        openOnHover={props.highlightOnHover}\n        onClose={() => setIsHighlighted(false)}\n        onOpen={() => setIsHighlighted(true)}\n        onOpenExternalLink={props.onOpenExternalLink}\n        openWithDelay={props.highlightWithDelay}\n        onSelect={props.onSelect}\n        stakePool={stakePool}\n        containerClassName={props.containerClassName}\n        numberOfRankedStakePools={numberOfRankedStakePools}\n        showWithSelectButton={props.showWithSelectButton}\n      >\n        <div\n          className={contentClassnames}\n          onClick={() =>\n            props.selectOnClick && props.onSelect && props.onSelect(id)\n          }\n        >\n          {content}\n        </div>\n      </PoolPopOver>\n    </div>\n  );\n}\n", "hunk": "@@ -40,7 +40,7 @@ export function ThumbPool(props: {\n     styles.content,\n     isDisabled ? styles.disabled : null,\n     isHighlighted ? styles.isHighlighted : null,\n-    props.highlightOnHover ? styles.isOnHover : null,\n+    props.highlightOnHover ? styles.shouldHighlightOnHover : null,\n   ]);\n \n   const content = isSelected ? (\n", "comment": "Maybe `isHovered` or just `hovered` would be better than `isOnHover`", "ids": ["41891", "011caefe6b14c0b4cda43af709f24824caddb76d", "be410607f1ca024bf7966bb098f7c4839cecd3aa"], "repo": "input-output-hk/daedalus", "ghid": 2373, "old": "     styles.content,\n     isDisabled ? styles.disabled : null,\n     isHighlighted ? styles.isHighlighted : null,\n-    props.highlightOnHover ? styles.isOnHover : null,\n   ]);\n   const content = isSelected ? (", "new": "     styles.content,\n     isDisabled ? styles.disabled : null,\n     isHighlighted ? styles.isHighlighted : null,\n+    props.highlightOnHover ? styles.shouldHighlightOnHover : null,\n   ]);\n   const content = isSelected ? (", "lang": "js", "norm_lang": "javascript"}
{"old_hunk": "@@ -36,13 +36,17 @@\n \n     @Override\n     public String toString() {\n-        return \"UploadResult{\" +\n-                \"errorCode='\" + errorCode + '\\'' +\n-                \", resultStatus='\" + resultStatus + '\\'' +\n-                \", dateUploaded='\" + dateUploaded.toString() + '\\'' +\n-                \", imageUrl='\" + imageUrl + '\\'' +\n-                \", canonicalFilename='\" + canonicalFilename + '\\'' +\n-                '}';\n+        if (dateUploaded != null) {", "oldf": "package fr.free.nrw.commons.mwapi;\n\nimport java.util.Date;\n\npublic class UploadResult {\n    private String errorCode;\n    private String resultStatus;\n    private Date dateUploaded;\n    private String imageUrl;\n    private String canonicalFilename;\n\n    /**\n     * Minimal constructor\n     *\n     * @param resultStatus Upload result status\n     * @param errorCode    Upload error code\n     */\n    UploadResult(String resultStatus, String errorCode) {\n        this.resultStatus = resultStatus;\n        this.errorCode = errorCode;\n    }\n\n    /**\n     * Full-fledged constructor\n     * @param resultStatus Upload result status\n     * @param dateUploaded Uploaded date\n     * @param canonicalFilename Uploaded file name\n     * @param imageUrl Uploaded image file name\n     */\n    UploadResult(String resultStatus, Date dateUploaded, String canonicalFilename, String imageUrl) {\n        this.resultStatus = resultStatus;\n        this.dateUploaded = dateUploaded;\n        this.canonicalFilename = canonicalFilename;\n        this.imageUrl = imageUrl;\n    }\n\n    @Override\n    public String toString() {\n        if (dateUploaded != null) {\n            return \"UploadResult{\" +\n                    \"errorCode='\" + errorCode + '\\'' +\n                    \", resultStatus='\" + resultStatus + '\\'' +\n                    \", dateUploaded='\" + dateUploaded.toString() + '\\'' +\n                    \", imageUrl='\" + imageUrl + '\\'' +\n                    \", canonicalFilename='\" + canonicalFilename + '\\'' +\n                    '}';\n        }\n\n        return \"Server is in read only mode\";\n    }\n\n    /**\n     * Gets uploaded date\n     * @return Upload date\n     */\n    public Date getDateUploaded() {\n        return dateUploaded;\n    }\n\n    /**\n     * Gets image url\n     * @return Uploaded image url\n     */\n    public String getImageUrl() {\n        return imageUrl;\n    }\n\n    /**\n     * Gets canonical file name\n     * @return Uploaded file name\n     */\n    public String getCanonicalFilename() {\n        return canonicalFilename;\n    }\n\n    /**\n     * Gets upload error code\n     * @return Error code\n     */\n    public String getErrorCode() {\n        return errorCode;\n    }\n\n    /**\n     * Gets upload result status\n     * @return Upload result status\n     */\n    public String getResultStatus() {\n        return resultStatus;\n    }\n}\n", "hunk": "@@ -36,7 +36,9 @@ public class UploadResult {\n \n     @Override\n     public String toString() {\n-        if (dateUploaded != null) {\n+        if (dateUploaded == null) {\n+            return \"Server is in read only mode\";\n+        }\n             return \"UploadResult{\" +\n                     \"errorCode='\" + errorCode + '\\'' +\n                     \", resultStatus='\" + resultStatus + '\\'' +\n", "comment": "Do not add the check here. Add the check just for `dateuploaded`.", "ids": ["21647", "77c23a504dbf74f6fe04318ec910a2b30bdee420", "96dcf33f3470d2860ef371994f410364295bfeee"], "repo": "commons-app/apps-android-commons", "ghid": 2460, "old": "     @Override\n     public String toString() {\n-        if (dateUploaded != null) {\n             return \"UploadResult{\" +\n                     \"errorCode='\" + errorCode + '\\'' +\n                     \", resultStatus='\" + resultStatus + '\\'' +", "new": "     @Override\n     public String toString() {\n+        if (dateUploaded == null) {\n+            return \"Server is in read only mode\";\n+        }\n             return \"UploadResult{\" +\n                     \"errorCode='\" + errorCode + '\\'' +\n                     \", resultStatus='\" + resultStatus + '\\'' +", "lang": "java", "norm_lang": "java"}
{"old_hunk": "@@ -1,65 +1,162 @@\n import json\n import os\n from pokemongo_bot.base_task import BaseTask\n+from pokemongo_bot.worker_result import WorkerResult\n from pokemongo_bot.tree_config_builder import ConfigException\n \n class RecycleItems(BaseTask):\n     SUPPORTED_TASK_API_VERSION = 1\n \n+    \"\"\"\n+    Recycle undesired items if there is less than five space in inventory.\n+    You can use either item's name or id. For the full list of items see ../../data/items.json\n+\n+    It's highly recommended to put this task before the move_to_fort task in the config file so you'll most likely be able to loot.\n+\n+    Example config :\n+    {\n+      \"type\": \"RecycleItems\",\n+      \"config\": {\n+        \"item_filter\": {\n+          \"Pokeball\": {\"keep\": 20},\n+          \"Greatball\": {\"keep\": 50},\n+          \"Ultraball\": {\"keep\": 100},\n+          \"Potion\": {\"keep\": 0},\n+          \"Super Potion\": {\"keep\": 0},\n+          \"Hyper Potion\": {\"keep\": 20},\n+          \"Max Potion\": {\"keep\": 50},\n+          \"Revive\": {\"keep\": 0},\n+          \"Max Revive\": {\"keep\": 20},\n+          \"Razz Berry\": {\"keep\": 20}\n+        }\n+      }\n+    }\n+    \"\"\"\n+\n     def initialize(self):\n-        self.item_filter = self.config.get('item_filter', {})\n+        self.items_filter = self.config.get('item_filter', {})\n         self._validate_item_filter()\n \n     def _validate_item_filter(self):\n+        \"\"\"\n+        Validate user's item filter config\n+        :return: Nothing.\n+        :rtype: None\n+        :raise: ConfigException: When an item doesn't exist in ../../data/items.json\n+        \"\"\"\n         item_list = json.load(open(os.path.join('data', 'items.json')))\n-        for config_item_name, bag_count in self.item_filter.iteritems():\n+        for config_item_name, bag_count in self.items_filter.iteritems():\n             if config_item_name not in item_list.viewvalues():\n                 if config_item_name not in item_list:\n                     raise ConfigException(\"item {} does not exist, spelling mistake? (check for valid item names in data/items.json)\".format(config_item_name))\n \n     def work(self):\n-        self.bot.latest_inventory = None\n-        item_count_dict = self.bot.item_inventory_count('all')\n-\n-        for item_id, bag_count in item_count_dict.iteritems():\n-            item_name = self.bot.item_list[str(item_id)]\n-            id_filter = self.item_filter.get(item_name, 0)\n-            if id_filter is not 0:\n-                id_filter_keep = id_filter.get('keep', 20)\n+        \"\"\"\n+        Discard items if necessary.\n+        :return: Always returns WorkerResult.SUCCESS.\n+        :rtype: WorkerResult\n+        \"\"\"\n+        if not self.bot.has_space_for_loot():\n+            # Updating user's inventory\n+            self.bot.latest_inventory = None\n+            # Getting every item in user's inventory\n+            item_counts_in_bag_dict = self.bot.item_inventory_count('all')\n+\n+            # For each user's item in inventory recycle the item if needed\n+            for item_id, item_count_in_bag in item_counts_in_bag_dict.iteritems():\n+                item = RecycleItems._Item(item_id, self.items_filter, self)\n+\n+                if item.should_be_recycled():\n+                    item.request_recycle()\n+                    item.emit_recycle_result()\n+\n+        return WorkerResult.SUCCESS\n+\n+    class _Item:", "oldf": "import json\nimport os\nfrom pokemongo_bot.base_task import BaseTask\nfrom pokemongo_bot.worker_result import WorkerResult\nfrom pokemongo_bot.tree_config_builder import ConfigException\n\nclass RecycleItems(BaseTask):\n    SUPPORTED_TASK_API_VERSION = 1\n\n    \"\"\"\n    Recycle undesired items if there is less than five space in inventory.\n    You can use either item's name or id. For the full list of items see ../../data/items.json\n\n    It's highly recommended to put this task before the move_to_fort task in the config file so you'll most likely be able to loot.\n\n    Example config :\n    {\n      \"type\": \"RecycleItems\",\n      \"config\": {\n        \"item_filter\": {\n          \"Pokeball\": {\"keep\": 20},\n          \"Greatball\": {\"keep\": 50},\n          \"Ultraball\": {\"keep\": 100},\n          \"Potion\": {\"keep\": 0},\n          \"Super Potion\": {\"keep\": 0},\n          \"Hyper Potion\": {\"keep\": 20},\n          \"Max Potion\": {\"keep\": 50},\n          \"Revive\": {\"keep\": 0},\n          \"Max Revive\": {\"keep\": 20},\n          \"Razz Berry\": {\"keep\": 20}\n        }\n      }\n    }\n    \"\"\"\n\n    def initialize(self):\n        self.items_filter = self.config.get('item_filter', {})\n        self._validate_item_filter()\n\n    def _validate_item_filter(self):\n        \"\"\"\n        Validate user's item filter config\n        :return: Nothing.\n        :rtype: None\n        :raise: ConfigException: When an item doesn't exist in ../../data/items.json\n        \"\"\"\n        item_list = json.load(open(os.path.join('data', 'items.json')))\n        for config_item_name, bag_count in self.items_filter.iteritems():\n            if config_item_name not in item_list.viewvalues():\n                if config_item_name not in item_list:\n                    raise ConfigException(\"item {} does not exist, spelling mistake? (check for valid item names in data/items.json)\".format(config_item_name))\n\n    def work(self):\n        \"\"\"\n        Discard items if necessary.\n        :return: Always returns WorkerResult.SUCCESS.\n        :rtype: WorkerResult\n        \"\"\"\n        if not self.bot.has_space_for_loot():\n            # Updating user's inventory\n            self.bot.latest_inventory = None\n            # Getting every item in user's inventory\n            item_counts_in_bag_dict = self.bot.item_inventory_count('all')\n\n            # For each user's item in inventory recycle the item if needed\n            for item_id, item_count_in_bag in item_counts_in_bag_dict.iteritems():\n                item = RecycleItems._Item(item_id, self.items_filter, self)\n\n                if item.should_be_recycled():\n                    item.request_recycle()\n                    item.emit_recycle_result()\n\n        return WorkerResult.SUCCESS\n\n    class _Item:\n        \"\"\"\n        An item found in user's inventory.\n\n        This class contains details of recycling process.\n        \"\"\"\n        def __init__(self, item_id, items_filter, recycle_items):\n            \"\"\"\n            Initializes an item\n            :param item_id: Item's id.\n            :param items_filter: List of items and their maximum amount to keep.\n            :param recycle_items: The recycle_items instance.\n            \"\"\"\n            self.recycle_items = recycle_items\n            self.bot = recycle_items.bot\n            self.id = item_id\n            self.name = recycle_items.bot.item_list[str(item_id)]\n            self.items_filter = items_filter\n            self.amount_to_keep = self._get_amount_to_keep()\n            self.amount_in_inventory = recycle_items.bot.item_inventory_count(self.id)\n            self.amount_to_recycle = 0 if self.amount_to_keep is None else self.amount_in_inventory - self.amount_to_keep\n            self.recycle_item_request_result = None\n\n        def _get_amount_to_keep(self):\n            \"\"\"\n            Determine item's amount to keep in inventory.\n            :return: Item's amount to keep in inventory.\n            :rtype: int\n            \"\"\"\n            item_filter_config = self.items_filter.get(self.name, 0)\n            if item_filter_config is not 0:\n                return item_filter_config.get('keep', 20)\n            else:\n                item_filter_config = self.items_filter.get(str(self.id), 0)\n                if item_filter_config is not 0:\n                    return item_filter_config.get('keep', 20)\n\n        def should_be_recycled(self):\n            \"\"\"\n            Returns a value indicating whether the item should be recycled.\n            :return: True if the title should be recycled; otherwise, False.\n            :rtype: bool\n            \"\"\"\n            return (self.name in self.items_filter or str(self.id) in self.items_filter) and self.amount_to_recycle > 0\n\n        def request_recycle(self):\n            \"\"\"\n            Request recycling of the item and store api call response's result.\n            :return: Nothing.\n            :rtype: None\n            \"\"\"\n            response = self.bot.api.recycle_inventory_item(item_id=self.id, count=self.amount_to_recycle)\n            # Example of good request response\n            # {'responses': {'RECYCLE_INVENTORY_ITEM': {'result': 1, 'new_count': 46}}, 'status_code': 1, 'auth_ticket': {'expire_timestamp_ms': 1469306228058L, 'start': '/HycFyfrT4t2yB2Ij+yoi+on778aymMgxY6RQgvrGAfQlNzRuIjpcnDd5dAxmfoTqDQrbz1m2dGqAIhJ+eFapg==', 'end': 'f5NOZ95a843tgzprJo4W7Q=='}, 'request_id': 8145806132888207460L}\n            self.recycle_item_request_result = response.get('responses', {}).get('RECYCLE_INVENTORY_ITEM', {}).get('result', 0)\n\n        def _is_recycling_success(self):\n            \"\"\"\n            Returns a value indicating whether the item has been successfully recycled.\n            :return: True if the item has been successfully recycled; otherwise, False.\n            :rtype: bool\n            \"\"\"\n            return self.recycle_item_request_result == 1\n\n        def emit_recycle_result(self):\n            \"\"\"\n            Emits recycle result in logs\n            :return: Nothing.\n            :rtype: None\n            \"\"\"\n            if self._is_recycling_success():\n                self.recycle_items.emit_event(\n                    'item_discarded',\n                    formatted='Discarded {amount}x {item} (maximum {maximum}).',\n                    data={\n                        'amount': str(self.amount_to_recycle),\n                        'item': self.name,\n                        'maximum': str(self.amount_to_keep)\n                    }\n                )\n            else:\n                self.recycle_items.emit_event(\n                    'item_discard_fail',\n                    formatted=\"Failed to discard {item}\",\n                    data={\n                        'item': self.name\n                    }\n                )\n", "hunk": "@@ -1,5 +1,6 @@\n import json\n import os\n+from pokemongo_bot import inventory\n from pokemongo_bot.base_task import BaseTask\n from pokemongo_bot.worker_result import WorkerResult\n from pokemongo_bot.tree_config_builder import ConfigException\n", "comment": "Excuse my python noobishness, but what is the difference between `class _Item:` and `class _Item(object):`?", "ids": ["24053", "d398e45ad4e201aed02e0f215c4609c71abf9c24", "bf0995f6804a5e3b2b828d4075ab64b35f361beb"], "repo": "PokemonGoF/PokemonGo-Bot", "ghid": 2482, "old": " import json\n import os\n from pokemongo_bot.base_task import BaseTask\n from pokemongo_bot.worker_result import WorkerResult\n from pokemongo_bot.tree_config_builder import ConfigException", "new": " import json\n import os\n+from pokemongo_bot import inventory\n from pokemongo_bot.base_task import BaseTask\n from pokemongo_bot.worker_result import WorkerResult\n from pokemongo_bot.tree_config_builder import ConfigException", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -0,0 +1,166 @@\n+# -*- coding: utf-8 -*-\n+import re\n+\n+from streamlink.plugin import Plugin\n+from streamlink.plugin.api import http, validate\n+from streamlink.stream import RTMPStream\n+\n+_url_re = re.compile(r'''^https?://\n+        (?:\\w*.)?\n+        showroom-live.com/\n+        (?:\n+            (?P<room_title>[\\w-]+$)\n+            |\n+            room/profile\\?room_id=(?P<room_id>\\d+)$\n+        )\n+''', re.VERBOSE)\n+\n+_room_id_re = re.compile(r'\"roomId\":(?P<room_id>\\d+),')\n+_room_id_alt_re = re.compile(r'content=\"showroom:///room\\?room_id=(?P<room_id>\\d+)\"')\n+_room_id_lookup_failure_log = 'Failed to find room_id for {0} using {1} regex'\n+\n+_api_status_url = 'https://www.showroom-live.com/room/is_live?room_id={room_id}'\n+_api_data_url = 'https://www.showroom-live.com/room/get_live_data?room_id={room_id}'\n+\n+_api_data_schema = validate.Schema(\n+    {\n+        \"streaming_url_list_rtmp\": validate.all([\n+            {\n+                \"url\": validate.text,\n+                \"stream_name\": validate.text,\n+                \"id\": int,\n+                \"label\": validate.text,\n+                \"is_default\": int\n+            }\n+        ]),\n+        \"is_live\": int,\n+        \"room\": {\n+            \"room_url_key\": validate.text\n+        },\n+        \"telop\": validate.any(None, validate.text)\n+    }\n+)\n+_rtmp_quality_lookup = {\n+    \"\u30aa\u30ea\u30b8\u30ca\u30eb\u753b\u8cea\": \"original\",\n+    \"original spec\": \"original\",\n+    \"\u4f4e\u753b\u8cea\": \"low\",\n+    \"low spec\": \"low\"\n+}\n+# changes here must also be updated in test_plugin_showroom\n+_quality_weights = {\n+    \"original\": 720,", "oldf": "# -*- coding: utf-8 -*-\nimport re\n\nfrom streamlink.plugin import Plugin\nfrom streamlink.plugin.api import http, validate\nfrom streamlink.stream import RTMPStream\n\n_url_re = re.compile(r'''^https?://\n        (?:\\w*.)?\n        showroom-live.com/\n        (?:\n            (?P<room_title>[\\w-]+$)\n            |\n            room/profile\\?room_id=(?P<room_id>\\d+)$\n        )\n''', re.VERBOSE)\n\n_room_id_re = re.compile(r'\"roomId\":(?P<room_id>\\d+),')\n_room_id_alt_re = re.compile(r'content=\"showroom:///room\\?room_id=(?P<room_id>\\d+)\"')\n_room_id_lookup_failure_log = 'Failed to find room_id for {0} using {1} regex'\n\n_api_status_url = 'https://www.showroom-live.com/room/is_live?room_id={room_id}'\n_api_data_url = 'https://www.showroom-live.com/room/get_live_data?room_id={room_id}'\n\n_api_data_schema = validate.Schema(\n    {\n        \"streaming_url_list_rtmp\": validate.all([\n            {\n                \"url\": validate.text,\n                \"stream_name\": validate.text,\n                \"id\": int,\n                \"label\": validate.text,\n                \"is_default\": int\n            }\n        ]),\n        \"is_live\": int,\n        \"room\": {\n            \"room_url_key\": validate.text\n        },\n        \"telop\": validate.any(None, validate.text)\n    }\n)\n_rtmp_quality_lookup = {\n    \"\u30aa\u30ea\u30b8\u30ca\u30eb\u753b\u8cea\": \"original\",\n    \"original spec\": \"original\",\n    \"\u4f4e\u753b\u8cea\": \"low\",\n    \"low spec\": \"low\"\n}\n# changes here must also be updated in test_plugin_showroom\n_quality_weights = {\n    \"original\": 720,\n    \"other\": 360,\n    \"low\": 160\n}\n# pages that definitely aren't rooms\n_info_pages = set((\n    \"onlive\",\n    \"campaign\",\n    \"timetable\",\n    \"event\",\n    \"news\",\n    \"article\",\n    \"ranking\",\n    \"follow\",\n    \"search\",\n    \"mypage\",\n    \"payment\",\n    \"user\",\n    \"notice\",\n    \"s\",\n    \"organizer_registration\",\n    \"lottery\"\n))\n\n\nclass Showroom(Plugin):\n    @staticmethod\n    def _get_stream_info(room_id):\n        res = http.get(_api_data_url.format(room_id=room_id))\n        return http.json(res, schema=_api_data_schema)\n\n    @classmethod\n    def can_handle_url(cls, url):\n        match = _url_re.match(url)\n        if not match or match.group(\"room_title\") in _info_pages:\n            return False\n        return True\n\n    @classmethod\n    def stream_weight(cls, stream):\n        if stream in _quality_weights:\n            return _quality_weights.get(stream), \"quality\"\n\n        return Plugin.stream_weight(stream)\n\n    def __init__(self, url):\n        Plugin.__init__(self, url)\n        self._room_id = None\n        self._info = None\n        self._title = None\n\n    @property\n    def telop(self):\n        if self._info:\n            return self._info['telop']\n        else:\n            return \"\"\n\n    @property\n    def room_id(self):\n        if self._room_id is None:\n            self._room_id = self._get_room_id()\n        return self._room_id\n\n    def _get_room_id(self):\n        \"\"\"\n        Locates unique identifier (\"room_id\") for the room.\n\n        Returns the room_id as a string, or None if no room_id was found\n        \"\"\"\n        match_dict = _url_re.match(self.url).groupdict()\n\n        if match_dict['room_id'] is not None:\n            return match_dict['room_id']\n        else:\n            res = http.get(self.url)\n            match = _room_id_re.search(res.text)\n            if not match:\n                title = self.url.rsplit('/', 1)[-1]\n                self.logger.debug(_room_id_lookup_failure_log.format(title, 'primary'))\n                match = _room_id_alt_re.search(res.text)\n                if not match:\n                    self.logger.debug(_room_id_lookup_failure_log.format(title, 'secondary'))\n                    return  # Raise exception?\n            return match.group('room_id')\n\n    def _get_title(self):\n        if self._title is None:\n            if 'profile?room_id=' not in self.url:\n                self._title = self.url.rsplit('/', 1)[-1]\n            else:\n                if self._info is None:\n                    # TODO: avoid this\n                    self._info = self._get_stream_info(self.room_id)\n                self._title = self._info.get('room').get('room_url_key')\n        return self._title\n\n    def _get_rtmp_stream(self, stream_info):\n        rtmp_url = '/'.join((stream_info['url'], stream_info['stream_name']))\n        quality = _rtmp_quality_lookup.get(stream_info['label'], \"other\")\n\n        params = dict(rtmp=rtmp_url, live=True)\n        return quality, RTMPStream(self.session, params=params)\n\n    def _get_streams(self):\n        self._info = self._get_stream_info(self.room_id)\n        if not self._info or not self._info['is_live']:\n            return\n\n        self.logger.debug(\"Getting streams for {0}\".format(self._get_title()))\n\n        for stream_info in self._info.get(\"streaming_url_list_rtmp\", []):\n            yield self._get_rtmp_stream(stream_info)\n\n\n__plugin__ = Showroom\n", "hunk": "@@ -41,14 +41,14 @@ _api_data_schema = validate.Schema(\n     }\n )\n _rtmp_quality_lookup = {\n-    \"\u30aa\u30ea\u30b8\u30ca\u30eb\u753b\u8cea\": \"original\",\n-    \"original spec\": \"original\",\n+    \"\u30aa\u30ea\u30b8\u30ca\u30eb\u753b\u8cea\": \"high\",\n+    \"original spec\": \"high\",\n     \"\u4f4e\u753b\u8cea\": \"low\",\n     \"low spec\": \"low\"\n }\n # changes here must also be updated in test_plugin_showroom\n _quality_weights = {\n-    \"original\": 720,\n+    \"high\": 720,\n     \"other\": 360,\n     \"low\": 160\n }\n", "comment": "This is the only thing I can see where I imagine @beardypig will chime in and talk about the naming of the weights.", "ids": ["7785", "971840aac5c6d2fd797dad7156eee045cb2c329f", "6971a9928d6ffb7d49e08d34fb5923740aab3208"], "repo": "streamlink/streamlink", "ghid": 633, "old": "     }\n )\n _rtmp_quality_lookup = {\n-    \"\u30aa\u30ea\u30b8\u30ca\u30eb\u753b\u8cea\": \"original\",\n-    \"original spec\": \"original\",\n     \"\u4f4e\u753b\u8cea\": \"low\",\n     \"low spec\": \"low\"\n }\n # changes here must also be updated in test_plugin_showroom\n _quality_weights = {\n-    \"original\": 720,\n     \"other\": 360,\n     \"low\": 160\n }", "new": "     }\n )\n _rtmp_quality_lookup = {\n+    \"\u30aa\u30ea\u30b8\u30ca\u30eb\u753b\u8cea\": \"high\",\n+    \"original spec\": \"high\",\n     \"\u4f4e\u753b\u8cea\": \"low\",\n     \"low spec\": \"low\"\n }\n # changes here must also be updated in test_plugin_showroom\n _quality_weights = {\n+    \"high\": 720,\n     \"other\": 360,\n     \"low\": 160\n }", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -155,16 +155,15 @@ TEST(printable) {\n   line.clear();\n   CHECK(printers::json<policy::oneline>(line, json{o}));\n   CHECK_EQUAL(line, \"{\\\"foo\\\": 42, \\\"bar\\\": null}\");\n-  o = {{\"baz\", 4.2}};\n+  o = {{\"baz\", json{4.2}}};\n   line.clear();\n   CHECK(printers::json<policy::oneline>(line, json{o}));\n   CHECK_EQUAL(line, \"{\\\"baz\\\": 4.2}\");\n   MESSAGE(\"tree policy\");\n-  o = {\n-    {\"baz\", 4.2},\n-    {\"x\", a},\n-    {\"inner\", json::object{{\"a\", false}, {\"c\", a}, {\"b\", 42}}}\n-  };\n+  o = {{\"baz\", json{4.2}},\n+       {\"x\", json{a}},\n+       {\"inner\", json{json::object{\n+                   {\"a\", json{false}}, {\"c\", json{a}}, {\"b\", json{42}}}}}};", "oldf": "/******************************************************************************\n *                    _   _____   __________                                  *\n *                   | | / / _ | / __/_  __/     Visibility                   *\n *                   | |/ / __ |_\\ \\  / /          Across                     *\n *                   |___/_/ |_/___/ /_/       Space and Time                 *\n *                                                                            *\n * This file is part of VAST. It is subject to the license terms in the       *\n * LICENSE file found in the top-level directory of this distribution and at  *\n * http://vast.io/license. No part of VAST, including this file, may be       *\n * copied, modified, propagated, or distributed except according to the terms *\n * contained in the LICENSE file.                                             *\n ******************************************************************************/\n\n#include \"vast/json.hpp\"\n#include \"vast/concept/parseable/vast/json.hpp\"\n#include \"vast/concept/printable/numeric.hpp\"\n#include \"vast/concept/printable/to_string.hpp\"\n#include \"vast/concept/printable/vast/json.hpp\"\n#include \"vast/concept/convertible/to.hpp\"\n\n#define SUITE json\n#include \"test.hpp\"\n\nusing namespace vast;\nusing namespace std::string_literals;\n\nTEST(construction) {\n  CHECK(caf::holds_alternative<json::null>(json{}));\n  CHECK(caf::holds_alternative<json::null>(json{caf::none}));\n  CHECK(caf::holds_alternative<json::boolean>(json{true}));\n  CHECK(caf::holds_alternative<json::number>(json{42}));\n  CHECK(caf::holds_alternative<json::number>(json{4.2}));\n  CHECK(caf::holds_alternative<json::string>(json{\"foo\"}));\n  CHECK(caf::holds_alternative<json::string>(json{\"foo\"s}));\n  CHECK(caf::holds_alternative<json::array>(json{json::make_array(1, 2, 3)}));\n  CHECK(caf::holds_alternative<json::object>(json{json::object{{\"foo\", json{42}}}}));\n}\n\nTEST(assignment) {\n  json j;\n  j = caf::none;\n  CHECK(caf::holds_alternative<json::null>(j));\n  j = true;\n  CHECK(caf::holds_alternative<json::boolean>(j));\n  j = 42;\n  CHECK(caf::holds_alternative<json::number>(j));\n  j = \"foo\";\n  CHECK(caf::holds_alternative<std::string>(j));\n  j = json::make_array(true, false);\n  CHECK(caf::holds_alternative<json::array>(j));\n  j = json::object{{\"x\", json{true}}, {\"y\", json{false}}};\n  CHECK(caf::holds_alternative<json::object>(j));\n}\n\nTEST(total order) {\n  auto j0 = json{true};\n  auto j1 = json{false};\n  CHECK(j1 < j0);\n  CHECK(j0 != j1);\n  j0 = \"bar\";\n  j1 = \"foo\";\n  CHECK(j0 != j1);\n  CHECK(j0 < j1);\n  j1 = 42;\n  CHECK(j0 != j1);\n  CHECK(!(j0 < j1));\n  CHECK(!(j0 <= j1));\n  CHECK(j0 > j1);\n  CHECK(j0 >= j1);\n}\n\nTEST(parseable) {\n  json j;\n  std::string str;\n  MESSAGE(\"bool\");\n  str = \"true\";\n  CHECK(parsers::json(str, j));\n  CHECK(j == true);\n  str = \"false\";\n  CHECK(parsers::json(str, j));\n  CHECK(j == false);\n  MESSAGE(\"null\");\n  str = \"null\";\n  CHECK(parsers::json(str, j));\n  CHECK(j == caf::none);\n  MESSAGE(\"number\");\n  str = \"42\";\n  CHECK(parsers::json(str, j));\n  CHECK(j == json::number{42});\n  str = \"-1337\";\n  CHECK(parsers::json(str, j));\n  CHECK(j == json::number{-1337});\n  str = \"4.2\";\n  CHECK(parsers::json(str, j));\n  CHECK(j == json::number{4.2});\n  MESSAGE(\"string\");\n  str = \"\\\"foo\\\"\";\n  CHECK(parsers::json(str, j));\n  CHECK(j == \"foo\");\n  MESSAGE(\"array\");\n  str = \"[]\";\n  CHECK(parsers::json(str, j));\n  CHECK(j == json::array{});\n  str = \"[    ]\";\n  CHECK(parsers::json(str, j));\n  CHECK(j == json::array{});\n  str = R\"([ 42,-1337 , \"foo\", null ,true ])\";\n  CHECK(parsers::json(str, j));\n  CHECK(j == json::make_array(42, -1337, \"foo\", caf::none, true));\n  MESSAGE(\"object\");\n  str = \"{}\";\n  CHECK(parsers::json(str, j));\n  CHECK(j == json::object{});\n  str = \"{    }\";\n  CHECK(parsers::json(str, j));\n  CHECK(j == json::object{});\n  str = R\"json({ \"baz\": 4.2, \"inner\": null })json\";\n  CHECK(parsers::json(str, j));\n  CHECK(j == json::object{{\"baz\", json{4.2}}, {\"inner\", json{caf::none}}});\n  str = R\"json({\n  \"baz\": 4.2,\n  \"inner\": null,\n  \"x\": [\n    42,\n    -1337,\n    \"foo\",\n    true\n  ]\n})json\";\n  CHECK(parsers::json(str, j));\n  auto o = json::object{\n    {\"baz\", json{4.2}},\n    {\"inner\", json{caf::none}},\n    {\"x\", json{json::make_array(42, -1337, \"foo\", true)}}\n  };\n  CHECK(j == o);\n}\n\nTEST(printable) {\n  CHECK_EQUAL(to_string(json{}), \"null\");\n  CHECK_EQUAL(to_string(json{true}), \"true\");\n  CHECK_EQUAL(to_string(json{false}), \"false\");\n  CHECK_EQUAL(to_string(json{42}), \"42\");\n  CHECK_EQUAL(to_string(json{42.0}), \"42\");\n  CHECK_EQUAL(to_string(json{4.2}), \"4.2\");\n  CHECK_EQUAL(to_string(json{\"foo\"}), \"\\\"foo\\\"\");\n  MESSAGE(\"one line policy\");\n  std::string line;\n  auto a = json::make_array(42, -1337, \"foo\", caf::none, true);\n  CHECK(printers::json<policy::oneline>(line, json{a}));\n  CHECK_EQUAL(line, \"[42, -1337, \\\"foo\\\", null, true]\");\n  json::object o;\n  o[\"foo\"] = 42;\n  o[\"bar\"] = caf::none;\n  line.clear();\n  CHECK(printers::json<policy::oneline>(line, json{o}));\n  CHECK_EQUAL(line, \"{\\\"foo\\\": 42, \\\"bar\\\": null}\");\n  o = {{\"baz\", json{4.2}}};\n  line.clear();\n  CHECK(printers::json<policy::oneline>(line, json{o}));\n  CHECK_EQUAL(line, \"{\\\"baz\\\": 4.2}\");\n  MESSAGE(\"tree policy\");\n  o = {{\"baz\", json{4.2}},\n       {\"x\", json{a}},\n       {\"inner\", json{json::object{\n                   {\"a\", json{false}}, {\"c\", json{a}}, {\"b\", json{42}}}}}};\n  auto json_tree = R\"json({\n  \"baz\": 4.2,\n  \"x\": [\n    42,\n    -1337,\n    \"foo\",\n    null,\n    true\n  ],\n  \"inner\": {\n    \"a\": false,\n    \"c\": [\n      42,\n      -1337,\n      \"foo\",\n      null,\n      true\n    ],\n    \"b\": 42\n  }\n})json\";\n  std::string str;\n  CHECK(printers::json<policy::tree>(str, json{o}));\n  CHECK_EQUAL(str, json_tree);\n}\n\nTEST(conversion) {\n  MESSAGE(\"bool\");\n  auto t = to<json>(true);\n  REQUIRE(t);\n  CHECK(*t == json{true});\n  MESSAGE(\"number\");\n  t = to<json>(4.2);\n  REQUIRE(t);\n  CHECK(*t == json{4.2});\n  MESSAGE(\"strings\");\n  t = to<json>(\"foo\");\n  REQUIRE(t);\n  CHECK(*t == json{\"foo\"});\n  MESSAGE(\"std::vector\");\n  t = to<json>(std::vector<int>{1, 2, 3});\n  REQUIRE(t);\n  CHECK(*t == json::make_array(1, 2, 3));\n  MESSAGE(\"std::map\");\n  t = to<json>(std::map<unsigned, bool>{{1, true}, {2, false}});\n  REQUIRE(t);\n  CHECK(*t == json::object{{\"1\", json{true}}, {\"2\", json{false}}});\n}\n", "hunk": "@@ -160,10 +160,13 @@ TEST(printable) {\n   CHECK(printers::json<policy::oneline>(line, json{o}));\n   CHECK_EQUAL(line, \"{\\\"baz\\\": 4.2}\");\n   MESSAGE(\"tree policy\");\n-  o = {{\"baz\", json{4.2}},\n-       {\"x\", json{a}},\n-       {\"inner\", json{json::object{\n-                   {\"a\", json{false}}, {\"c\", json{a}}, {\"b\", json{42}}}}}};\n+  o = {\n+    {\"baz\", json{4.2}},\n+    {\"x\", json{a}},\n+    {\"inner\", json{json::object{\n+     {\"a\", json{false}},\n+     {\"c\", json{a}},\n+     {\"b\", json{42}}}}}};\n   auto json_tree = R\"json({\n   \"baz\": 4.2,\n   \"x\": [\n", "comment": "Formatting it like \"actual\" JSON looks a bit nicer: ``` o = { {\"baz\", json{4.2}}, {\"x\", json{a}}, {\"inner\", json{json::object{ {\"a\", json{false}}, {\"c\", json{a}}, {\"b\", json{42}} ... ```", "ids": ["9654", "aa0ae904d13e5015bee4b129fa220dc5d5d321f9", "761cfc0844e470578924d9881162759fd2c7ba09"], "repo": "tenzir/vast", "ghid": 204, "old": "   CHECK(printers::json<policy::oneline>(line, json{o}));\n   CHECK_EQUAL(line, \"{\\\"baz\\\": 4.2}\");\n   MESSAGE(\"tree policy\");\n-  o = {{\"baz\", json{4.2}},\n-       {\"x\", json{a}},\n-       {\"inner\", json{json::object{\n-                   {\"a\", json{false}}, {\"c\", json{a}}, {\"b\", json{42}}}}}};\n   auto json_tree = R\"json({\n   \"baz\": 4.2,\n   \"x\": [", "new": "   CHECK(printers::json<policy::oneline>(line, json{o}));\n   CHECK_EQUAL(line, \"{\\\"baz\\\": 4.2}\");\n   MESSAGE(\"tree policy\");\n+  o = {\n+    {\"baz\", json{4.2}},\n+    {\"x\", json{a}},\n+    {\"inner\", json{json::object{\n+     {\"a\", json{false}},\n+     {\"c\", json{a}},\n+     {\"b\", json{42}}}}}};\n   auto json_tree = R\"json({\n   \"baz\": 4.2,\n   \"x\": [", "lang": "cpp", "norm_lang": "cpp"}
{"old_hunk": "@@ -166,13 +166,17 @@ private void setVariableDataType(Variable variable, Map<String, ItemDefinition>\n         // retrieve type from item definition\n \n         String itemSubjectRef = (String) variable.getMetaData(\"ItemSubjectRef\");\n+        Object defaultValue = variable.getMetaData(\"defaultValue\");\n         if (UndefinedDataType.getInstance().equals(variable.getType()) && itemDefinitions != null && itemSubjectRef != null) {\n             DataType dataType = DataTypeResolver.defaultDataType;\n             ItemDefinition itemDefinition = itemDefinitions.get(itemSubjectRef);\n             if (itemDefinition != null) {\n                 dataType = DataTypeResolver.fromType(itemDefinition.getStructureRef(), cl);\n             }\n             variable.setType(dataType);\n+            if(defaultValue != null) {               ", "oldf": "/*\n * Copyright 2021 Red Hat, Inc. and/or its affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *       http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.jbpm.bpmn2.xml;\n\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\n\nimport org.drools.core.xml.BaseAbstractHandler;\nimport org.drools.core.xml.ExtensibleXmlParser;\nimport org.drools.core.xml.Handler;\nimport org.jbpm.bpmn2.core.Definitions;\nimport org.jbpm.bpmn2.core.Interface;\nimport org.jbpm.bpmn2.core.Interface.Operation;\nimport org.jbpm.bpmn2.core.ItemDefinition;\nimport org.jbpm.compiler.xml.ProcessBuildData;\nimport org.jbpm.process.core.ContextContainer;\nimport org.jbpm.process.core.context.variable.Variable;\nimport org.jbpm.process.core.context.variable.VariableScope;\nimport org.jbpm.process.core.datatype.DataType;\nimport org.jbpm.process.core.datatype.DataTypeResolver;\nimport org.jbpm.process.core.datatype.impl.type.UndefinedDataType;\nimport org.jbpm.ruleflow.core.RuleFlowProcess;\nimport org.jbpm.workflow.core.NodeContainer;\nimport org.jbpm.workflow.core.node.ForEachNode;\nimport org.jbpm.workflow.core.node.WorkItemNode;\nimport org.kie.api.definition.process.Node;\nimport org.kie.api.definition.process.Process;\nimport org.w3c.dom.Element;\nimport org.xml.sax.Attributes;\nimport org.xml.sax.SAXException;\n\npublic class DefinitionsHandler extends BaseAbstractHandler implements Handler {\n\n    @SuppressWarnings(\"unchecked\")\n    public DefinitionsHandler() {\n        if ((this.validParents == null) && (this.validPeers == null)) {\n            this.validParents = new HashSet();\n            this.validParents.add(null);\n\n            this.validPeers = new HashSet();\n            this.validPeers.add(null);\n\n            this.allowNesting = false;\n        }\n    }\n\n    @Override\n    public Object start(final String uri, final String localName,\n            final Attributes attrs, final ExtensibleXmlParser parser)\n            throws SAXException {\n        parser.startElementBuilder(localName, attrs);\n        return new Definitions();\n    }\n\n    @Override\n    public Object end(final String uri, final String localName,\n            final ExtensibleXmlParser parser) throws SAXException {\n        final Element element = parser.endElementBuilder();\n        Definitions definitions = (Definitions) parser.getCurrent();\n        String namespace = element.getAttribute(\"targetNamespace\");\n        List<Process> processes = ((ProcessBuildData) parser.getData()).getProcesses();\n        Map<String, ItemDefinition> itemDefinitions = (Map<String, ItemDefinition>) ((ProcessBuildData) parser.getData()).getMetaData(\"ItemDefinitions\");\n\n        List<Interface> interfaces = (List<Interface>) ((ProcessBuildData) parser.getData()).getMetaData(\"Interfaces\");\n\n        for (Process process : processes) {\n            RuleFlowProcess ruleFlowProcess = (RuleFlowProcess) process;\n            ruleFlowProcess.setMetaData(\"TargetNamespace\", namespace);\n            postProcessItemDefinitions(ruleFlowProcess, itemDefinitions, parser.getClassLoader());\n            postProcessInterfaces(ruleFlowProcess, interfaces);\n        }\n        definitions.setTargetNamespace(namespace);\n        return definitions;\n    }\n\n    @Override\n    public Class<?> generateNodeFor() {\n        return Definitions.class;\n    }\n\n    private void postProcessInterfaces(NodeContainer nodeContainer, List<Interface> interfaces) {\n\n        for (Node node : nodeContainer.getNodes()) {\n            if (node instanceof NodeContainer) {\n                postProcessInterfaces((NodeContainer) node, interfaces);\n            }\n            if (node instanceof WorkItemNode && \"Service Task\".equals(((WorkItemNode) node).getMetaData(\"Type\"))) {\n                WorkItemNode workItemNode = (WorkItemNode) node;\n                if (interfaces == null) {\n                    throw new ProcessParsingValidationException(\"No interfaces found\");\n                }\n                String operationRef = (String) workItemNode.getMetaData(\"OperationRef\");\n                String implementation = (String) workItemNode.getMetaData(\"Implementation\");\n                Operation operation = null;\n                for (Interface i : interfaces) {\n                    operation = i.getOperation(operationRef);\n                    if (operation != null) {\n                        break;\n                    }\n                }\n                if (operation == null) {\n                    throw new ProcessParsingValidationException(\"Could not find operation \" + operationRef);\n                }\n                // avoid overriding parameters set by data input associations\n                if (workItemNode.getWork().getParameter(\"Interface\") == null) {\n                    workItemNode.getWork().setParameter(\"Interface\", operation.getInterface().getName());\n                }\n                if (workItemNode.getWork().getParameter(\"Operation\") == null) {\n                    workItemNode.getWork().setParameter(\"Operation\", operation.getName());\n                }\n                if (workItemNode.getWork().getParameter(\"ParameterType\") == null && operation.getMessage() != null) {\n                    workItemNode.getWork().setParameter(\"ParameterType\", operation.getMessage().getType());\n                }\n                // parameters to support web service invocation \n                if (implementation != null) {\n                    workItemNode.getWork().setParameter(\"interfaceImplementationRef\", operation.getInterface().getImplementationRef());\n                    workItemNode.getWork().setParameter(\"operationImplementationRef\", operation.getImplementationRef());\n                    workItemNode.getWork().setParameter(\"implementation\", implementation);\n                }\n            }\n        }\n    }\n\n    private void postProcessItemDefinitions(NodeContainer nodeContainer, Map<String, ItemDefinition> itemDefinitions, ClassLoader cl) {\n        if (nodeContainer instanceof ContextContainer) {\n            setVariablesDataType((ContextContainer) nodeContainer, itemDefinitions, cl);\n        }\n        // process composite context node of for each to enhance its variables with types\n        if (nodeContainer instanceof ForEachNode) {\n            setVariablesDataType(((ForEachNode) nodeContainer).getCompositeNode(), itemDefinitions, cl);\n        }\n        for (Node node : nodeContainer.getNodes()) {\n            if (node instanceof NodeContainer) {\n                postProcessItemDefinitions((NodeContainer) node, itemDefinitions, cl);\n            }\n            if (node instanceof ContextContainer) {\n                setVariablesDataType((ContextContainer) node, itemDefinitions, cl);\n            }\n        }\n    }\n\n    private void setVariablesDataType(ContextContainer container, Map<String, ItemDefinition> itemDefinitions, ClassLoader cl) {\n        VariableScope variableScope = (VariableScope) container.getDefaultContext(VariableScope.VARIABLE_SCOPE);\n        if (variableScope != null) {\n            for (Variable variable : variableScope.getVariables()) {\n                setVariableDataType(variable, itemDefinitions, cl);\n            }\n        }\n    }\n\n    private void setVariableDataType(Variable variable, Map<String, ItemDefinition> itemDefinitions, ClassLoader cl) {\n        // retrieve type from item definition\n\n        String itemSubjectRef = (String) variable.getMetaData(\"ItemSubjectRef\");\n        Object defaultValue = variable.getMetaData(\"defaultValue\");\n        if (UndefinedDataType.getInstance().equals(variable.getType()) && itemDefinitions != null && itemSubjectRef != null) {\n            DataType dataType = DataTypeResolver.defaultDataType;\n            ItemDefinition itemDefinition = itemDefinitions.get(itemSubjectRef);\n            if (itemDefinition != null) {\n                dataType = DataTypeResolver.fromType(itemDefinition.getStructureRef(), cl);\n            }\n            variable.setType(dataType);\n            if(defaultValue != null) {               \n                variable.setValue(dataType.verifyDataType(defaultValue) ? defaultValue : dataType.readValue((String)defaultValue));                \n            }\n        }\n    }\n\n}\n", "hunk": "@@ -174,9 +174,7 @@ public class DefinitionsHandler extends BaseAbstractHandler implements Handler {\n                 dataType = DataTypeResolver.fromType(itemDefinition.getStructureRef(), cl);\n             }\n             variable.setType(dataType);\n-            if(defaultValue != null) {               \n-                variable.setValue(dataType.verifyDataType(defaultValue) ? defaultValue : dataType.readValue((String)defaultValue));                \n-            }\n+            variable.setValue(dataType.verifyDataType(defaultValue) ? defaultValue : dataType.readValue((String) defaultValue));\n         }\n     }\n \n", "comment": "the null check is not needed", "ids": ["29536", "175f285d80a88e2da7f565381b4ea9121b6aa518", "d6e5d7d4d32836364667a6d3ad1a6b8b34cca1fd"], "repo": "kiegroup/kogito-runtimes", "ghid": 1809, "old": "                 dataType = DataTypeResolver.fromType(itemDefinition.getStructureRef(), cl);\n             }\n             variable.setType(dataType);\n-            if(defaultValue != null) {               \n-                variable.setValue(dataType.verifyDataType(defaultValue) ? defaultValue : dataType.readValue((String)defaultValue));                \n-            }\n         }\n     }", "new": "                 dataType = DataTypeResolver.fromType(itemDefinition.getStructureRef(), cl);\n             }\n             variable.setType(dataType);\n+            variable.setValue(dataType.verifyDataType(defaultValue) ? defaultValue : dataType.readValue((String) defaultValue));\n         }\n     }", "lang": "java", "norm_lang": "java"}
{"old_hunk": "@@ -77,7 +77,6 @@ def get_graph(name, format):\n \n \n def get_ogb_graph(name):\n-    os.symlink('/tmp/dataset/', os.path.join(os.getcwd(), 'dataset'))", "oldf": "from timeit import default_timer\nimport json\nimport os\nimport pickle\nimport shutil\nimport zipfile\nimport requests\nimport inspect\nimport numpy as np\nimport pandas\nimport dgl\nimport torch\nimport time\nfrom ogb.nodeproppred import DglNodePropPredDataset\n\nfrom functools import partial, reduce, wraps\n\n\ndef _download(url, path, filename):\n    fn = os.path.join(path, filename)\n    if os.path.exists(fn):\n        return\n\n    os.makedirs(path, exist_ok=True)\n    f_remote = requests.get(url, stream=True)\n    sz = f_remote.headers.get('content-length')\n    assert f_remote.status_code == 200, 'fail to open {}'.format(url)\n    with open(fn, 'wb') as writer:\n        for chunk in f_remote.iter_content(chunk_size=1024*1024):\n            writer.write(chunk)\n    print('Download finished.')\n\n\n# GRAPH_CACHE = {}\n\n\ndef get_graph(name, format):\n    # global GRAPH_CACHE\n    # if name in GRAPH_CACHE:\n    #     return GRAPH_CACHE[name].to(format)\n    g = None\n    if name == 'cora':\n        g = dgl.data.CoraGraphDataset(verbose=False)[0]\n    elif name == 'pubmed':\n        g = dgl.data.PubmedGraphDataset(verbose=False)[0]\n    elif name == 'livejournal':\n        bin_path = \"/tmp/dataset/livejournal/livejournal_{}.bin\".format(format)\n        if os.path.exists(bin_path):\n            g_list, _ = dgl.load_graphs(bin_path)\n            g = g_list[0]\n        else:\n            g = get_livejournal().formats([format])\n            dgl.save_graphs(bin_path, [g])\n    elif name == \"friendster\":\n        bin_path = \"/tmp/dataset/friendster/friendster_{}.bin\".format(format)\n        if os.path.exists(bin_path):\n            g_list, _ = dgl.load_graphs(bin_path)\n            g = g_list[0]\n        else:\n            g = get_friendster().formats([format])\n            dgl.save_graphs(bin_path, [g])\n    elif name == \"reddit\":\n        bin_path = \"/tmp/dataset/reddit/reddit_{}.bin\".format(format)\n        if os.path.exists(bin_path):\n            g_list, _ = dgl.load_graphs(bin_path)\n            g = g_list[0]\n        else:\n            g = dgl.data.RedditDataset(self_loop=True)[0].formats([format])\n            dgl.save_graphs(bin_path, [g])\n    elif name.startswith(\"ogb\"):\n        g = get_ogb_graph(name)\n    else:\n        raise Exception(\"Unknown dataset\")\n    # GRAPH_CACHE[name] = g\n    g = g.formats([format])\n    return g\n\n\ndef get_ogb_graph(name):\n    data = DglNodePropPredDataset(name=name)\n    return data[0][0]\n\n\ndef get_livejournal():\n    # Same as https://snap.stanford.edu/data/soc-LiveJournal1.txt.gz\n    _download('https://dgl-asv-data.s3-us-west-2.amazonaws.com/dataset/livejournal/soc-LiveJournal1.txt.gz',\n              '/tmp/dataset/livejournal', 'soc-LiveJournal1.txt.gz')\n    df = pandas.read_csv('/tmp/dataset/livejournal/soc-LiveJournal1.txt.gz', sep='\\t', skiprows=4, header=None,\n                         names=['src', 'dst'], compression='gzip')\n    src = df['src'].values\n    dst = df['dst'].values\n    print('construct the graph')\n    return dgl.graph((src, dst))\n\n\ndef get_friendster():\n    # Same as https://snap.stanford.edu/data/bigdata/communities/com-friendster.ungraph.txt.gz\n    _download('https://dgl-asv-data.s3-us-west-2.amazonaws.com/dataset/friendster/com-friendster.ungraph.txt.gz',\n              '/tmp/dataset/friendster', 'com-friendster.ungraph.txt.gz')\n    df = pandas.read_csv('/tmp/dataset/friendster/com-friendster.ungraph.txt.gz', sep='\\t', skiprows=4, header=None,\n                         names=['src', 'dst'], compression='gzip')\n    src = df['src'].values\n    dst = df['dst'].values\n    print('construct the graph')\n    return dgl.graph((src, dst))\n\n\nclass OGBDataset(object):\n    def __init__(self, g, num_labels, predict_category=None):\n        self._g = g\n        self._num_labels = num_labels\n        self._predict_category = predict_category\n\n    @property\n    def num_labels(self):\n        return self._num_labels\n\n    @property\n    def num_classes(self):\n        return self._num_labels\n\n    @property\n    def predict_category(self):\n        return self._predict_category\n\n    def __getitem__(self, idx):\n        return self._g\n\n\ndef load_ogb_product():\n    name = 'ogbn-products'\n    os.symlink('/tmp/dataset/', os.path.join(os.getcwd(), 'dataset'))\n\n    print('load', name)\n    data = DglNodePropPredDataset(name=name)\n    print('finish loading', name)\n    splitted_idx = data.get_idx_split()\n    graph, labels = data[0]\n    labels = labels[:, 0]\n\n    graph.ndata['label'] = labels\n    in_feats = graph.ndata['feat'].shape[1]\n    num_labels = len(torch.unique(\n        labels[torch.logical_not(torch.isnan(labels))]))\n\n    # Find the node IDs in the training, validation, and test set.\n    train_nid, val_nid, test_nid = splitted_idx['train'], splitted_idx['valid'], splitted_idx['test']\n    train_mask = torch.zeros((graph.number_of_nodes(),), dtype=torch.bool)\n    train_mask[train_nid] = True\n    val_mask = torch.zeros((graph.number_of_nodes(),), dtype=torch.bool)\n    val_mask[val_nid] = True\n    test_mask = torch.zeros((graph.number_of_nodes(),), dtype=torch.bool)\n    test_mask[test_nid] = True\n    graph.ndata['train_mask'] = train_mask\n    graph.ndata['val_mask'] = val_mask\n    graph.ndata['test_mask'] = test_mask\n\n    return OGBDataset(graph, num_labels)\n\n\ndef load_ogb_mag():\n    name = 'ogbn-mag'\n    os.symlink('/tmp/dataset/', os.path.join(os.getcwd(), 'dataset'))\n\n    print('load', name)\n    dataset = DglNodePropPredDataset(name=name)\n    print('finish loading', name)\n    split_idx = dataset.get_idx_split()\n    train_idx = split_idx[\"train\"]['paper']\n    val_idx = split_idx[\"valid\"]['paper']\n    test_idx = split_idx[\"test\"]['paper']\n    hg_orig, labels = dataset[0]\n    subgs = {}\n    for etype in hg_orig.canonical_etypes:\n        u, v = hg_orig.all_edges(etype=etype)\n        subgs[etype] = (u, v)\n        subgs[(etype[2], 'rev-'+etype[1], etype[0])] = (v, u)\n    hg = dgl.heterograph(subgs)\n    hg.nodes['paper'].data['feat'] = hg_orig.nodes['paper'].data['feat']\n    hg.nodes['paper'].data['labels'] = labels['paper'].squeeze()\n    train_mask = torch.zeros((hg.number_of_nodes('paper'),), dtype=torch.bool)\n    train_mask[train_idx] = True\n    val_mask = torch.zeros((hg.number_of_nodes('paper'),), dtype=torch.bool)\n    val_mask[val_idx] = True\n    test_mask = torch.zeros((hg.number_of_nodes('paper'),), dtype=torch.bool)\n    test_mask[test_idx] = True\n    hg.nodes['paper'].data['train_mask'] = train_mask\n    hg.nodes['paper'].data['val_mask'] = val_mask\n    hg.nodes['paper'].data['test_mask'] = test_mask\n\n    num_classes = dataset.num_classes\n    return OGBDataset(hg, num_classes, 'paper')\n\n\nclass PinsageDataset:\n    def __init__(self, g, user_ntype, item_ntype, textset):\n        self._g = g\n        self._user_ntype = user_ntype\n        self._item_ntype = item_ntype\n        self._textset = textset\n\n    @property\n    def user_ntype(self):\n        return self._user_ntype\n\n    @property\n    def item_ntype(self):\n        return self._item_ntype\n\n    @property\n    def textset(self):\n        return self._textset\n\n    def __getitem__(self, idx):\n        return self._g\n\n\ndef load_nowplaying_rs():\n    import torchtext\n    # follow examples/pytorch/pinsage/README to create nowplaying_rs.pkl\n    name = 'nowplaying_rs.pkl'\n    dataset_dir = os.path.join(os.getcwd(), 'dataset')\n    os.symlink('/tmp/dataset/', dataset_dir)\n\n    dataset_path = os.path.join(dataset_dir, \"nowplaying_rs\", name)\n    # Load dataset\n    with open(dataset_path, 'rb') as f:\n        dataset = pickle.load(f)\n\n    g = dataset['train-graph']\n    val_matrix = dataset['val-matrix'].tocsr()\n    test_matrix = dataset['test-matrix'].tocsr()\n    item_texts = dataset['item-texts']\n    user_ntype = dataset['user-type']\n    item_ntype = dataset['item-type']\n    user_to_item_etype = dataset['user-to-item-type']\n    timestamp = dataset['timestamp-edge-column']\n\n    # Assign user and movie IDs and use them as features (to learn an individual trainable\n    # embedding for each entity)\n    g.nodes[user_ntype].data['id'] = torch.arange(\n        g.number_of_nodes(user_ntype))\n    g.nodes[item_ntype].data['id'] = torch.arange(\n        g.number_of_nodes(item_ntype))\n\n    # Prepare torchtext dataset and vocabulary\n    fields = {}\n    examples = []\n    for key, texts in item_texts.items():\n        fields[key] = torchtext.data.Field(\n            include_lengths=True, lower=True, batch_first=True)\n    for i in range(g.number_of_nodes(item_ntype)):\n        example = torchtext.data.Example.fromlist(\n            [item_texts[key][i] for key in item_texts.keys()],\n            [(key, fields[key]) for key in item_texts.keys()])\n        examples.append(example)\n    textset = torchtext.data.Dataset(examples, fields)\n    for key, field in fields.items():\n        field.build_vocab(getattr(textset, key))\n\n    return PinsageDataset(g, user_ntype, item_ntype, textset)\n\n\ndef process_data(name):\n    if name == 'cora':\n        return dgl.data.CoraGraphDataset()\n    elif name == 'pubmed':\n        return dgl.data.PubmedGraphDataset()\n    elif name == 'aifb':\n        return dgl.data.AIFBDataset()\n    elif name == 'mutag':\n        return dgl.data.MUTAGDataset()\n    elif name == 'bgs':\n        return dgl.data.BGSDataset()\n    elif name == 'am':\n        return dgl.data.AMDataset()\n    elif name == 'reddit':\n        return dgl.data.RedditDataset(self_loop=True)\n    elif name == 'ogbn-products':\n        return load_ogb_product()\n    elif name == 'ogbn-mag':\n        return load_ogb_mag()\n    elif name == 'nowplaying_rs':\n        return load_nowplaying_rs()\n    else:\n        raise ValueError('Invalid dataset name:', name)\n\n\ndef get_bench_device():\n    device = os.environ.get('DGL_BENCH_DEVICE', 'cpu')\n    if device.lower() == \"gpu\":\n        return \"cuda:0\"\n    else:\n        return device\n\n\ndef setup_track_time(*args, **kwargs):\n    # fix random seed\n    np.random.seed(42)\n    torch.random.manual_seed(42)\n\n\ndef setup_track_acc(*args, **kwargs):\n    # fix random seed\n    np.random.seed(42)\n    torch.random.manual_seed(42)\n\n\ndef setup_track_flops(*args, **kwargs):\n    # fix random seed\n    np.random.seed(42)\n    torch.random.manual_seed(42)\n\n\nTRACK_UNITS = {\n    'time': 's',\n    'acc': '%',\n    'flops': 'GFLOPS',\n}\n\nTRACK_SETUP = {\n    'time': setup_track_time,\n    'acc': setup_track_acc,\n    'flops': setup_track_flops,\n}\n\n\ndef parametrize(param_name, params):\n    \"\"\"Decorator for benchmarking over a set of parameters.\n\n    Parameters\n    ----------\n    param_name : str\n        Parameter name. Must be one of the arguments of the decorated function.\n    params : list[any]\n        List of values to benchmark for the given parameter name. Recommend\n        to use Python's native object type (e.g., int, str, list[int]) because\n        ASV will display them on the plot.\n\n    Examples\n    --------\n\n    Benchmark function `foo` when argument `x` is equal to 10 or 20.\n\n    .. code::\n        @benchmark('time')\n        @parametrize('x', [10, 20]):\n        def foo(x):\n            pass\n\n    Benchmark function with multiple parametrizations. It will run the function\n    with all possible combinations. The example below generates 6 benchmarks.\n\n    .. code::\n        @benchmark('time')\n        @parametrize('x', [10, 20]):\n        @parametrize('y', [-1, -2, -3]):\n        def foo(x, y):\n            pass\n\n    When using multiple parametrizations, it can have arbitrary order. The example\n    below is the same as the above one.\n\n    .. code::\n        @benchmark('time')\n        @parametrize('y', [-1, -2, -3]):\n        @parametrize('x', [10, 20]):\n        def foo(x, y):\n            pass\n    \"\"\"\n    def _wrapper(func):\n        sig_params = inspect.signature(func).parameters.keys()\n        num_params = len(sig_params)\n        if getattr(func, 'params', None) is None:\n            func.params = [None] * num_params\n        if getattr(func, 'param_names', None) is None:\n            func.param_names = [None] * num_params\n        found_param = False\n        for i, sig_param in enumerate(sig_params):\n            if sig_param == param_name:\n                func.params[i] = params\n                func.param_names[i] = param_name\n                found_param = True\n                break\n        if not found_param:\n            raise ValueError('Invalid parameter name:', param_name)\n        return func\n    return _wrapper\n\n\ndef noop_decorator(param_name, params):\n    \"\"\"noop decorator\n    \"\"\"\n    def _wrapper(func):\n        return func\n    return _wrapper\n\n\nclass TestFilter:\n    def __init__(self):\n        self.conf = None\n        if \"DGL_REG_CONF\" in os.environ:\n            current_dir = os.path.dirname(os.path.abspath(__file__))\n            path = os.path.join(current_dir, \"../../\",\n                                os.environ[\"DGL_REG_CONF\"])\n            with open(path, \"r\") as f:\n                self.conf = json.load(f)\n            if \"INSTANCE_TYPE\" in os.environ:\n                instance_type = os.environ[\"INSTANCE_TYPE\"]\n            else:\n                raise Exception(\n                    \"Must set both DGL_REG_CONF and INSTANCE_TYPE as env\")\n            self.enabled_tests = self.conf[instance_type][\"tests\"]\n        else:\n            import logging\n            logging.warning(\"No regression test conf file specified\")\n\n    def check(self, func):\n        funcfullname = inspect.getmodule(func).__name__ + \".\" + func.__name__\n        if self.conf is None:\n            return True\n        else:\n            for enabled_testname in self.enabled_tests:\n                if enabled_testname in funcfullname:\n                    return True\n            return False\n\n\nfilter = TestFilter()\n\n\ndevice = os.environ.get('DGL_BENCH_DEVICE', 'cpu')\n\nif device == \"cpu\":\n    parametrize_cpu = parametrize\n    parametrize_gpu = noop_decorator\nelif device == \"gpu\":\n    parametrize_cpu = noop_decorator\n    parametrize_gpu = parametrize\nelse:\n    raise Exception(\n        \"Unknown device. Must be one of ['cpu', 'gpu'], but got {}\".format(device))\n\n\ndef skip_if_gpu():\n    \"\"\"skip if DGL_BENCH_DEVICE is gpu\n    \"\"\"\n    device = os.environ.get('DGL_BENCH_DEVICE', 'cpu')\n\n    def _wrapper(func):\n        if device == \"gpu\":\n            # skip if not enabled\n            func.benchmark_name = \"skip_\" + func.__name__\n        return func\n    return _wrapper\n\n\ndef benchmark(track_type, timeout=60):\n    \"\"\"Decorator for indicating the benchmark type.\n\n    Parameters\n    ----------\n    track_type : str\n        Type. Must be either:\n\n            - 'time' : For timing. Unit: second.\n            - 'acc' : For accuracy. Unit: percentage, value between 0 and 100.\n            - 'flops' : Unit: GFlops, number of floating point operations per second.\n    timeout : int\n        Timeout threshold in second.\n\n    Examples\n    --------\n\n    .. code::\n        @benchmark('time')\n        def foo():\n            pass\n    \"\"\"\n    assert track_type in ['time', 'acc', 'flops']\n\n    def _wrapper(func):\n        func.unit = TRACK_UNITS[track_type]\n        func.setup = TRACK_SETUP[track_type]\n        func.timeout = timeout\n        if not filter.check(func):\n            # skip if not enabled\n            func.benchmark_name = \"skip_\" + func.__name__\n        return func\n    return _wrapper\n\n#####################################\n# Timer\n#####################################\n\n\nclass Timer:\n    def __init__(self, device=None):\n        self.timer = default_timer\n        if device is None:\n            self.device = get_bench_device()\n        else:\n            self.device = device\n\n    def __enter__(self):\n        if self.device == 'cuda:0':\n            self.start_event = torch.cuda.Event(enable_timing=True)\n            self.end_event = torch.cuda.Event(enable_timing=True)\n            self.start_event.record()\n        else:\n            self.tic = self.timer()\n        return self\n\n    def __exit__(self, type, value, traceback):\n        if self.device == 'cuda:0':\n            self.end_event.record()\n            torch.cuda.synchronize()  # Wait for the events to be recorded!\n            self.elapsed_secs = self.start_event.elapsed_time(\n                self.end_event) / 1e3\n        else:\n            self.elapsed_secs = self.timer() - self.tic\n", "hunk": "@@ -77,6 +77,7 @@ def get_graph(name, format):\n \n \n def get_ogb_graph(name):\n+    os.symlink('/tmp/dataset/', os.path.join(os.getcwd(), 'dataset'))\n     data = DglNodePropPredDataset(name=name)\n     return data[0][0]\n \n", "comment": "@VoVAllen Is this still needed?", "ids": ["43353", "75f9c859c33eaa289541d5e28d6048d4beea5acf", "dbe4b6b94aebc6d16c6718445e477c651724dcf5"], "repo": "dmlc/dgl", "ghid": 2992, "old": " def get_ogb_graph(name):\n     data = DglNodePropPredDataset(name=name)\n     return data[0][0]", "new": " def get_ogb_graph(name):\n+    os.symlink('/tmp/dataset/', os.path.join(os.getcwd(), 'dataset'))\n     data = DglNodePropPredDataset(name=name)\n     return data[0][0]", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -454,7 +454,8 @@ export default class WalletSendForm extends Component<Props, State> {\n         this.isLatestTransactionFeeRequest(\n           this.state.feeCalculationRequestQue,\n           prevFeeCalculationRequestQue\n-        )\n+        ) &&\n+        !this.selectedAssetsAmounts.includes('0')", "oldf": "// @flow\nimport React, { Component, Fragment } from 'react';\nimport type { Node } from 'react';\nimport type { Field } from 'mobx-react-form';\nimport { observer } from 'mobx-react';\nimport { intlShape, FormattedHTMLMessage } from 'react-intl';\nimport { filter, get, indexOf, omit, map, without } from 'lodash';\nimport BigNumber from 'bignumber.js';\nimport classNames from 'classnames';\nimport SVGInline from 'react-svg-inline';\nimport vjf from 'mobx-react-form/lib/validators/VJF';\nimport { Button } from 'react-polymorph/lib/components/Button';\nimport { Input } from 'react-polymorph/lib/components/Input';\nimport { NumericInput } from 'react-polymorph/lib/components/NumericInput';\nimport { PopOver } from 'react-polymorph/lib/components/PopOver';\nimport BorderedBox from '../widgets/BorderedBox';\nimport LoadingSpinner from '../widgets/LoadingSpinner';\nimport ReadOnlyInput from '../widgets/forms/ReadOnlyInput';\nimport { FormattedHTMLMessageWithLink } from '../widgets/FormattedHTMLMessageWithLink';\nimport questionMarkIcon from '../../assets/images/question-mark.inline.svg';\nimport closeIcon from '../../assets/images/close-cross.inline.svg';\nimport globalMessages from '../../i18n/global-messages';\nimport messages from './send-form/messages';\n/* eslint-disable consistent-return */\nimport { messages as apiErrorMessages } from '../../api/errors';\nimport ReactToolboxMobxForm from '../../utils/ReactToolboxMobxForm';\nimport { submitOnEnter } from '../../utils/form';\nimport {\n  formattedAmountToNaturalUnits,\n  formattedAmountToLovelace,\n  formattedWalletAmount,\n} from '../../utils/formatters';\nimport { FORM_VALIDATION_DEBOUNCE_WAIT } from '../../config/timingConfig';\nimport { TRANSACTION_MIN_ADA_VALUE } from '../../config/walletsConfig';\nimport { NUMBER_FORMATS } from '../../../../common/types/number.types';\nimport AssetInput from './send-form/AssetInput';\nimport WalletSendAssetsConfirmationDialog from './send-form/WalletSendAssetsConfirmationDialog';\nimport WalletSendConfirmationDialogContainer from '../../containers/wallet/dialogs/WalletSendConfirmationDialogContainer';\nimport styles from './WalletSendForm.scss';\nimport Asset from '../../domains/Asset';\nimport type { HwDeviceStatus } from '../../domains/Wallet';\nimport type { AssetItems, WalletSummaryAsset } from '../../api/assets/types';\n\nmessages.fieldIsRequired = globalMessages.fieldIsRequired;\n\ntype Props = {\n  currencyUnit: string,\n  currencyMaxIntegerDigits: number,\n  currencyMaxFractionalDigits: number,\n  currentNumberFormat: string,\n  calculateTransactionFee: (\n    address: string,\n    amount: number,\n    assets: AssetItems\n  ) => Promise<BigNumber>,\n  walletAmount: BigNumber,\n  validateAmount: (amountInNaturalUnits: string) => Promise<boolean>,\n  addressValidator: Function,\n  assets: Array<WalletSummaryAsset>,\n  hasAssets: boolean,\n  selectedAsset: ?Asset,\n  isLoadingAssets: boolean,\n  isDialogOpen: Function,\n  isRestoreActive: boolean,\n  isHardwareWallet: boolean,\n  hwDeviceStatus: HwDeviceStatus,\n  onOpenDialogAction: Function,\n  onUnsetActiveAssetFingerprint: Function,\n  onExternalLinkClick: Function,\n};\n\ntype State = {\n  formFields: {\n    receiver: {\n      receiver: Field,\n      adaAmount: Field,\n      assetFields: {\n        [fingerprint: string]: Field,\n      },\n      assetsDropdown: {\n        [fingerprint: string]: Field,\n      },\n    },\n  },\n  minimumAda: BigNumber,\n  feeCalculationRequestQue: number,\n  transactionFee: BigNumber,\n  transactionFeeError: ?string | ?Node,\n  showRemoveAssetButton: { [fingerprint: string]: boolean },\n  selectedAssetFingerprints: Array<string>,\n  isResetButtonDisabled: boolean,\n  isReceiverAddressValid: boolean,\n  isTransactionFeeCalculated: boolean,\n};\n\n@observer\nexport default class WalletSendForm extends Component<Props, State> {\n  static contextTypes = {\n    intl: intlShape.isRequired,\n  };\n\n  state = {\n    formFields: {},\n    minimumAda: new BigNumber(0),\n    feeCalculationRequestQue: 0,\n    transactionFee: new BigNumber(0),\n    transactionFeeError: null,\n    showRemoveAssetButton: {},\n    selectedAssetFingerprints: [],\n    isResetButtonDisabled: true,\n    isReceiverAddressValid: false,\n    isTransactionFeeCalculated: false,\n  };\n\n  // We need to track the fee calculation state in order to disable\n  // the \"Submit\" button as soon as either receiver or amount field changes.\n  // This is required as we are using debounced validation and we need to\n  // disable the \"Submit\" button as soon as the value changes and then wait for\n  // the validation to end in order to see if the button should be enabled or not.\n  _isCalculatingTransactionFee = false;\n\n  // We need to track the mounted state in order to avoid calling\n  // setState promise handling code after the component was already unmounted:\n  // Read more: https://facebook.github.io/react/blog/2015/12/16/ismounted-antipattern.html\n  _isMounted = false;\n\n  componentDidMount() {\n    this._isMounted = true;\n    this.updateFormFields(true);\n    const { selectedAsset } = this.props;\n    if (selectedAsset) {\n      setTimeout(() => {\n        if (this._isMounted) {\n          this.addAssetRow(selectedAsset.fingerprint);\n        }\n      });\n    }\n  }\n\n  componentWillUnmount() {\n    this._isMounted = false;\n    this.props.onUnsetActiveAssetFingerprint();\n  }\n\n  getCurrentNumberFormat() {\n    return NUMBER_FORMATS[this.props.currentNumberFormat];\n  }\n\n  get selectedAssets(): Array<WalletSummaryAsset> {\n    const { selectedAssetFingerprints } = this.state;\n    const { assets: allAssets } = this.props;\n    return map(selectedAssetFingerprints, (fingerprint) =>\n      allAssets.find((asset) => asset.fingerprint === fingerprint)\n    );\n  }\n\n  get selectedAssetsAmounts(): Array<string> {\n    const { selectedAssetFingerprints, formFields } = this.state;\n    const assetFields = get(formFields, 'receiver.assetFields');\n    return map(selectedAssetFingerprints, (fingerprint) =>\n      formattedAmountToNaturalUnits(assetFields[fingerprint].value)\n    );\n  }\n\n  get availableAssets(): Array<WalletSummaryAsset> {\n    const { assets: allAssets } = this.props;\n    const { selectedAssetFingerprints } = this.state;\n    return filter(\n      allAssets,\n      ({ fingerprint }) => !selectedAssetFingerprints.includes(fingerprint)\n    );\n  }\n\n  get hasAvailableAssets(): boolean {\n    return this.availableAssets.length > 0;\n  }\n\n  getAssetByFingerprint = (fingerprint: string): ?WalletSummaryAsset => {\n    const { assets: allAssets } = this.props;\n    return allAssets.find((asset) => asset.fingerprint === fingerprint);\n  };\n\n  focusableFields: {\n    [fingerprint: string]: Field,\n  } = {};\n\n  addFocusableField = (field: ?Field) => {\n    if (field) {\n      const { name: fieldName } = field.props;\n      this.focusableFields[fieldName] = field;\n    }\n  };\n\n  focusField = (field: Field) => {\n    const { name: fieldName } = field;\n    const focusableField = this.focusableFields[fieldName];\n    if (focusableField) {\n      focusableField.focus();\n    }\n  };\n\n  handleSubmitOnEnter = submitOnEnter.bind(this, this.handleOnSubmit);\n\n  handleOnSubmit = () => {\n    if (this.isDisabled()) {\n      return false;\n    }\n    this.props.onOpenDialogAction({\n      dialog: WalletSendAssetsConfirmationDialog,\n    });\n  };\n\n  handleOnReset = () => {\n    // Cancel all debounced field validations\n    this.form.each((field) => {\n      field.debouncedValidation.cancel();\n    });\n    this.form.reset();\n    this.form.showErrors(false);\n\n    this.clearReceiverFieldValue();\n    this.clearAdaAmountFieldValue();\n    this.updateFormFields(true);\n\n    this.setState({\n      minimumAda: new BigNumber(0),\n      showRemoveAssetButton: {},\n      isResetButtonDisabled: true,\n    });\n  };\n\n  clearReceiverFieldValue = () => {\n    const receiverField = this.form.$('receiver');\n    if (receiverField) {\n      receiverField.clear();\n      this.setReceiverValidity(false);\n      this.focusField(receiverField);\n    }\n  };\n\n  clearAdaAmountFieldValue = () => {\n    const adaAmountField = this.form.$('adaAmount');\n    if (adaAmountField) {\n      adaAmountField.clear();\n    }\n  };\n\n  clearAssetFieldValue = (assetField: Field) => {\n    if (assetField) {\n      assetField.clear();\n      this.focusField(assetField);\n    }\n    this.resetTransactionFee();\n  };\n\n  updateFormFields = (resetFormFields: boolean, fingerprint?: string) => {\n    const formFields = this.form.fields;\n    const receiverField = formFields.get('receiver');\n    const adaAmountField = formFields.get('adaAmount');\n    if (resetFormFields) {\n      this.setState({\n        selectedAssetFingerprints: [],\n        formFields: {\n          receiver: {\n            receiver: receiverField,\n            adaAmount: adaAmountField,\n            assetFields: {},\n            assetsDropdown: {},\n          },\n        },\n      });\n    } else if (fingerprint) {\n      const { assetFields, assetsDropdown } = this.state.formFields.receiver;\n      const assetField = formFields.get(`asset_${fingerprint}`);\n      if (assetField) {\n        assetFields[fingerprint] = assetField;\n      }\n      const assetsDropdownField = formFields.get(\n        `assetsDropdown_${fingerprint}`\n      );\n      if (assetsDropdownField) {\n        assetsDropdown[fingerprint] = assetsDropdownField;\n      }\n      this.setState((prevState) => ({\n        formFields: {\n          ...prevState.formFields,\n          receiver: {\n            ...prevState.formFields.receiver,\n            assetFields,\n            assetsDropdown,\n          },\n        },\n      }));\n    }\n  };\n\n  hasReceiverValue = () => {\n    const receiverField = this.form.$('receiver');\n    return receiverField.value.length > 0;\n  };\n\n  hasAssetValue = (asset: Field) => {\n    return get(asset, 'value', false);\n  };\n\n  isDisabled = () =>\n    this._isCalculatingTransactionFee ||\n    !this.state.isTransactionFeeCalculated ||\n    !this.form.isValid;\n\n  form = new ReactToolboxMobxForm(\n    {\n      fields: {\n        receiver: {\n          label: this.context.intl.formatMessage(messages.receiverLabel),\n          placeholder: this.context.intl.formatMessage(messages.receiverHint),\n          value: '',\n          validators: [\n            async ({ field, form }) => {\n              const { value } = field;\n              if (value === null || value === '') {\n                this.resetTransactionFee();\n                this.setReceiverValidity(false);\n                return [\n                  false,\n                  this.context.intl.formatMessage(messages.fieldIsRequired),\n                ];\n              }\n              const isValid = await this.props.addressValidator(value);\n              this.setReceiverValidity(isValid);\n              const adaAmountField = form.$('adaAmount');\n              const isAdaAmountValid = adaAmountField.isValid;\n              if (isValid && isAdaAmountValid) {\n                this.calculateTransactionFee();\n              } else {\n                this.resetTransactionFee();\n              }\n              return [\n                isValid,\n                this.context.intl.formatMessage(\n                  apiErrorMessages.invalidAddress\n                ),\n              ];\n            },\n          ],\n        },\n        adaAmount: {\n          label: this.context.intl.formatMessage(messages.adaAmountLabel),\n          placeholder: `0${\n            this.getCurrentNumberFormat().decimalSeparator\n          }${'0'.repeat(this.props.currencyMaxFractionalDigits)}`,\n          value: '',\n          validators: [\n            async ({ field }) => {\n              const { value } = field;\n              if (value === null || value === '') {\n                this.resetTransactionFee();\n                return [\n                  false,\n                  this.context.intl.formatMessage(messages.fieldIsRequired),\n                ];\n              }\n              const amountValue = value.toString();\n              const isValid = await this.props.validateAmount(\n                formattedAmountToNaturalUnits(amountValue)\n              );\n              if (isValid) {\n                this.calculateTransactionFee();\n              } else {\n                this.resetTransactionFee();\n              }\n              return [\n                isValid,\n                this.context.intl.formatMessage(messages.invalidAmount),\n              ];\n            },\n          ],\n        },\n        estimatedFee: {\n          label: this.context.intl.formatMessage(messages.estimatedFeeLabel),\n          placeholder: `0${\n            this.getCurrentNumberFormat().decimalSeparator\n          }${'0'.repeat(this.props.currencyMaxFractionalDigits)}`,\n          value: null,\n        },\n      },\n    },\n    {\n      plugins: { vjf: vjf() },\n      options: {\n        validateOnBlur: false,\n        validateOnChange: true,\n        validationDebounceWait: FORM_VALIDATION_DEBOUNCE_WAIT,\n      },\n    }\n  );\n\n  setReceiverValidity(isValid: boolean) {\n    if (this._isMounted) {\n      this.setState({\n        isReceiverAddressValid: isValid,\n      });\n    }\n  }\n\n  isLatestTransactionFeeRequest = (\n    currentFeeCalculationRequestQue: number,\n    prevFeeCalculationRequestQue: number\n  ) => currentFeeCalculationRequestQue - prevFeeCalculationRequestQue === 1;\n\n  calculateTransactionFee = async () => {\n    const { form } = this;\n    const hasEmptyAssetFields = this.selectedAssetsAmounts.includes('0');\n    if (!form.isValid || hasEmptyAssetFields) {\n      form.showErrors(true);\n      return false;\n    }\n\n    const receiverField = form.$('receiver');\n    const receiver = receiverField.value;\n    const adaAmountField = form.$('adaAmount');\n    const adaAmount = formattedAmountToLovelace(adaAmountField.value);\n    const assets = filter(\n      this.selectedAssets.map(({ policyId, assetName }, index) => {\n        const quantity = new BigNumber(this.selectedAssetsAmounts[index]);\n        return {\n          policy_id: policyId,\n          asset_name: assetName,\n          quantity: quantity.toNumber(),\n        };\n      }),\n      'quantity'\n    );\n\n    const {\n      selectedAssetFingerprints,\n      feeCalculationRequestQue: prevFeeCalculationRequestQue,\n    } = this.state;\n    this.setState((prevState) => ({\n      feeCalculationRequestQue: prevState.feeCalculationRequestQue + 1,\n      isTransactionFeeCalculated: false,\n      transactionFee: new BigNumber(0),\n      transactionFeeError: null,\n    }));\n    try {\n      this._isCalculatingTransactionFee = true;\n      const { fee, minimumAda } = await this.props.calculateTransactionFee(\n        receiver,\n        adaAmount,\n        assets\n      );\n      if (\n        this._isMounted &&\n        this.isLatestTransactionFeeRequest(\n          this.state.feeCalculationRequestQue,\n          prevFeeCalculationRequestQue\n        ) &&\n        !this.selectedAssetsAmounts.includes('0')\n      ) {\n        this._isCalculatingTransactionFee = false;\n        this.setState({\n          isTransactionFeeCalculated: true,\n          minimumAda: minimumAda || new BigNumber(0),\n          transactionFee: fee,\n          transactionFeeError: null,\n        });\n      }\n    } catch (error) {\n      if (\n        this._isMounted &&\n        this.isLatestTransactionFeeRequest(\n          this.state.feeCalculationRequestQue,\n          prevFeeCalculationRequestQue\n        )\n      ) {\n        const errorHasLink = !!get(error, ['values', 'linkLabel']);\n        let transactionFeeError;\n        let localizableError = error;\n        let values;\n\n        if (error.id === 'api.errors.utxoTooSmall') {\n          const minimumAda = get(error, 'values.minimumAda');\n          if (minimumAda && !Number.isNaN(Number(minimumAda))) {\n            localizableError = selectedAssetFingerprints.length\n              ? messages.minAdaRequiredWithAssetTooltip\n              : messages.minAdaRequiredWithNoAssetTooltip;\n            values = { minimumAda };\n            this.setState({ minimumAda: new BigNumber(minimumAda) });\n          }\n        }\n\n        if (errorHasLink) {\n          transactionFeeError = (\n            <FormattedHTMLMessageWithLink\n              message={localizableError}\n              onExternalLinkClick={this.props.onExternalLinkClick}\n            />\n          );\n        } else {\n          transactionFeeError = (\n            <FormattedHTMLMessage {...localizableError} values={values} />\n          );\n        }\n\n        this._isCalculatingTransactionFee = false;\n        this.setState({\n          isTransactionFeeCalculated: false,\n          transactionFee: new BigNumber(0),\n          transactionFeeError,\n        });\n      }\n    }\n  };\n\n  resetTransactionFee() {\n    if (this._isMounted) {\n      this._isCalculatingTransactionFee = false;\n      this.setState({\n        isTransactionFeeCalculated: false,\n        transactionFee: new BigNumber(0),\n        transactionFeeError: null,\n      });\n    }\n  }\n\n  showRemoveAssetButton = (fingerprint: string) => {\n    const { showRemoveAssetButton } = this.state;\n    showRemoveAssetButton[fingerprint] = true;\n    this.setState({\n      showRemoveAssetButton,\n    });\n  };\n\n  hideRemoveAssetButton = (fingerprint: string) => {\n    const { showRemoveAssetButton } = this.state;\n    showRemoveAssetButton[fingerprint] = false;\n    this.setState({\n      showRemoveAssetButton,\n    });\n  };\n\n  addAssetRow = (fingerprint: string) => {\n    this.addAssetFields(fingerprint);\n    this.updateFormFields(false, fingerprint);\n    const { selectedAssetFingerprints } = this.state;\n    selectedAssetFingerprints.push(fingerprint);\n    this.setState({\n      selectedAssetFingerprints,\n    });\n    this.resetTransactionFee();\n  };\n\n  removeAssetRow = (fingerprint: string) => {\n    const { formFields, selectedAssetFingerprints } = this.state;\n    const { receiver } = formFields;\n    const assetFields = omit(receiver.assetFields, fingerprint);\n    const assetsDropdown = omit(receiver.assetsDropdown, fingerprint);\n    this.setState({\n      selectedAssetFingerprints: without(\n        selectedAssetFingerprints,\n        fingerprint\n      ),\n      formFields: {\n        ...formFields,\n        receiver: {\n          ...receiver,\n          assetFields,\n          assetsDropdown,\n        },\n      },\n    });\n    this.removeAssetFields(fingerprint);\n    setTimeout(() => {\n      this.calculateTransactionFee();\n    });\n  };\n\n  addAssetFields = (fingerprint: string) => {\n    const newAsset = `asset_${fingerprint}`;\n    this.form.add({ name: newAsset, value: null, key: newAsset });\n    this.form\n      .$(newAsset)\n      .set('label', this.context.intl.formatMessage(messages.assetLabel));\n    this.form\n      .$(newAsset)\n      .set(\n        'placeholder',\n        `0${this.getCurrentNumberFormat().decimalSeparator}${'0'.repeat(\n          this.props.currencyMaxFractionalDigits\n        )}`\n      );\n    this.form.$(newAsset).set('validators', [\n      async ({ field }) => {\n        const { value } = field;\n        if (value === null || value === '') {\n          this.resetTransactionFee();\n          return [\n            false,\n            this.context.intl.formatMessage(messages.fieldIsRequired),\n          ];\n        }\n        const amountValue = value.toString();\n        const isValidAmount = await this.props.validateAmount(\n          formattedAmountToNaturalUnits(amountValue)\n        );\n        const asset = this.getAssetByFingerprint(fingerprint);\n        if (!asset) {\n          return false;\n        }\n        const assetValue = new BigNumber(\n          formattedAmountToNaturalUnits(field.value)\n        );\n        const isValidRange =\n          assetValue.isGreaterThan(0) &&\n          assetValue.isLessThanOrEqualTo(asset.quantity);\n        const isValid = isValidAmount && isValidRange;\n        if (isValid) {\n          this.calculateTransactionFee();\n        } else {\n          this.resetTransactionFee();\n        }\n        return [\n          isValid,\n          this.context.intl.formatMessage(messages.invalidAmount),\n        ];\n      },\n    ]);\n    this.form.$(newAsset).focus();\n\n    const assetsDropdown = `assetsDropdown_${fingerprint}`;\n    this.form.add({\n      name: assetsDropdown,\n      value: null,\n      key: assetsDropdown,\n    });\n    this.form.$(assetsDropdown).set('type', 'select');\n  };\n\n  removeAssetFields = (fingerprint: string) => {\n    const assetFieldToDelete = `asset_${fingerprint}`;\n    this.form.del(assetFieldToDelete);\n    const assetsDropdownFieldToDelete = `assetsDropdown_${fingerprint}`;\n    this.form.del(assetsDropdownFieldToDelete);\n  };\n\n  onChangeAsset = (oldFingerprint: string, newFingerprint: string) => {\n    this.addAssetFields(newFingerprint);\n    this.updateFormFields(false, newFingerprint);\n    let { selectedAssetFingerprints } = this.state;\n    const index = indexOf(selectedAssetFingerprints, oldFingerprint);\n    if (index > -1) {\n      selectedAssetFingerprints = selectedAssetFingerprints.splice(\n        index,\n        1,\n        newFingerprint\n      );\n    } else {\n      selectedAssetFingerprints.push(newFingerprint);\n    }\n    this.setState({\n      selectedAssetFingerprints,\n    });\n    this.removeAssetRow(oldFingerprint);\n    this.resetTransactionFee();\n  };\n\n  renderReceiverRow = (): Node => {\n    const { intl } = this.context;\n    const {\n      formFields,\n      minimumAda,\n      transactionFeeError,\n      selectedAssetFingerprints,\n      isReceiverAddressValid,\n    } = this.state;\n    const { currencyMaxFractionalDigits, walletAmount } = this.props;\n\n    const {\n      receiver: receiverField,\n      adaAmount: adaAmountField,\n      assetFields,\n      assetsDropdown,\n    } = formFields.receiver;\n\n    const assetsSeparatorBasicHeight = 140;\n    const assetsSeparatorCalculatedHeight = selectedAssetFingerprints.length\n      ? assetsSeparatorBasicHeight * (selectedAssetFingerprints.length + 1) -\n        40 * selectedAssetFingerprints.length\n      : assetsSeparatorBasicHeight;\n\n    const minimumAdaValue = minimumAda.isZero()\n      ? TRANSACTION_MIN_ADA_VALUE\n      : minimumAda.toFormat();\n\n    const addAssetButtonClasses = classNames([\n      styles.addAssetButton,\n      !this.hasAvailableAssets ? styles.disabled : null,\n      'primary',\n    ]);\n    const minAdaRequiredTooltip = selectedAssetFingerprints.length\n      ? messages.minAdaRequiredWithAssetTooltip\n      : messages.minAdaRequiredWithNoAssetTooltip;\n\n    return (\n      <div className={styles.fieldsContainer}>\n        <div className={styles.receiverInput}>\n          <Input\n            {...receiverField.bind()}\n            ref={(field) => {\n              this.addFocusableField(field);\n            }}\n            className=\"receiver\"\n            error={receiverField.error}\n            onChange={(value) => {\n              receiverField.onChange(value || '');\n              this.setState({\n                isResetButtonDisabled: false,\n              });\n            }}\n            onKeyPress={this.handleSubmitOnEnter}\n          />\n          {this.hasReceiverValue() && (\n            <div className={styles.clearReceiverContainer}>\n              <PopOver\n                content={intl.formatMessage(messages.clearLabel)}\n                placement=\"top\"\n              >\n                <button\n                  onClick={() => this.handleOnReset()}\n                  className={styles.clearReceiverButton}\n                >\n                  <SVGInline\n                    svg={closeIcon}\n                    className={styles.clearReceiverIcon}\n                  />\n                </button>\n              </PopOver>\n            </div>\n          )}\n        </div>\n        {this.hasReceiverValue() && isReceiverAddressValid && (\n          <>\n            <div\n              className={styles.fieldsLine}\n              style={{\n                height: `${assetsSeparatorCalculatedHeight}px`,\n                top: `${assetsSeparatorCalculatedHeight - 10}px`,\n                marginTop: `-${assetsSeparatorCalculatedHeight}px`,\n              }}\n            />\n            <div className={styles.assetInput}>\n              <Fragment>\n                {walletAmount && (\n                  <div className={styles.amountTokenTotal}>\n                    {intl.formatMessage(messages.ofLabel)}&nbsp;\n                    {formattedWalletAmount(walletAmount)}\n                  </div>\n                )}\n                <div className={styles.adaAmountLabel}>\n                  {intl.formatMessage(globalMessages.unitAda)}\n                </div>\n                <NumericInput\n                  {...adaAmountField.bind()}\n                  ref={(field) => {\n                    this.addFocusableField(field);\n                  }}\n                  className=\"adaAmount\"\n                  value={adaAmountField.value}\n                  bigNumberFormat={this.getCurrentNumberFormat()}\n                  decimalPlaces={currencyMaxFractionalDigits}\n                  numberLocaleOptions={{\n                    minimumFractionDigits: currencyMaxFractionalDigits,\n                  }}\n                  onChange={(value) => {\n                    adaAmountField.onChange(value);\n                  }}\n                  currency={globalMessages.unitAda}\n                  error={adaAmountField.error || transactionFeeError}\n                  onKeyPress={this.handleSubmitOnEnter}\n                  allowSigns={false}\n                  autoFocus\n                />\n                <div className={styles.minAdaRequired}>\n                  <span>\n                    {intl.formatMessage(messages.minAdaRequired, {\n                      minimumAda: minimumAdaValue,\n                    })}\n                  </span>\n                  <PopOver\n                    content={intl.formatMessage(minAdaRequiredTooltip, {\n                      minimumAda: minimumAdaValue,\n                    })}\n                    contentClassName={styles.minAdaTooltipContent}\n                    key=\"tooltip\"\n                  >\n                    <SVGInline\n                      svg={questionMarkIcon}\n                      className={styles.infoIcon}\n                    />\n                  </PopOver>\n                </div>\n              </Fragment>\n              <Fragment>\n                {selectedAssetFingerprints.map(\n                  (fingerprint: string, index: number) => (\n                    <AssetInput\n                      key={fingerprint}\n                      fingerprint={fingerprint}\n                      index={index}\n                      getAssetByFingerprint={this.getAssetByFingerprint}\n                      availableAssets={this.availableAssets}\n                      assetFields={assetFields}\n                      assetsDropdown={assetsDropdown}\n                      addFocusableField={this.addFocusableField}\n                      removeAssetButtonVisible={\n                        this.state.showRemoveAssetButton\n                      }\n                      showRemoveAssetButton={this.showRemoveAssetButton}\n                      hideRemoveAssetButton={this.hideRemoveAssetButton}\n                      currentNumberFormat={this.getCurrentNumberFormat()}\n                      removeAssetRow={this.removeAssetRow}\n                      handleSubmitOnEnter={this.handleSubmitOnEnter}\n                      clearAssetFieldValue={this.clearAssetFieldValue}\n                      onChangeAsset={this.onChangeAsset}\n                    />\n                  )\n                )}\n              </Fragment>\n              <Button\n                className={addAssetButtonClasses}\n                label={intl.formatMessage(messages.addAssetButtonLabel)}\n                disabled={!this.hasAvailableAssets}\n                onClick={() => {\n                  this.addAssetRow(this.availableAssets[0].fingerprint);\n                }}\n              />\n            </div>\n          </>\n        )}\n      </div>\n    );\n  };\n\n  render() {\n    const { form } = this;\n    const { intl } = this.context;\n    const {\n      formFields,\n      transactionFee,\n      transactionFeeError,\n      isResetButtonDisabled,\n      isTransactionFeeCalculated,\n    } = this.state;\n    const {\n      currencyUnit,\n      currencyMaxFractionalDigits,\n      hwDeviceStatus,\n      isHardwareWallet,\n      isDialogOpen,\n      isRestoreActive,\n      onExternalLinkClick,\n    } = this.props;\n\n    const receiverField = form.$('receiver');\n    const receiver = receiverField.value;\n\n    const adaAmountField = form.$('adaAmount');\n    const adaAmount = new BigNumber(adaAmountField.value || 0);\n\n    let fees = null;\n    let total = null;\n    if (isTransactionFeeCalculated) {\n      fees = transactionFee.toFormat(currencyMaxFractionalDigits);\n      total = adaAmount\n        .plus(transactionFee)\n        .toFormat(currencyMaxFractionalDigits);\n    }\n\n    const calculatingFeesSpinnerButtonClasses = classNames([\n      styles.calculatingFeesSpinnerButton,\n      styles.spinning,\n    ]);\n\n    return (\n      <div className={styles.component}>\n        {isRestoreActive ? (\n          <div className={styles.syncingTransactionsWrapper}>\n            <LoadingSpinner big />\n            <p className={styles.syncingTransactionsText}>\n              {intl.formatMessage(messages.syncingTransactionsMessage)}\n            </p>\n          </div>\n        ) : (\n          <BorderedBox>\n            <div className={styles.walletSendForm}>\n              {formFields.receiver && this.renderReceiverRow()}\n              <div className={styles.estimatedFeeInput}>\n                <ReadOnlyInput\n                  label={intl.formatMessage(messages.estimatedFeeLabel)}\n                  value={\n                    fees && !transactionFeeError\n                      ? `${fees} ${intl.formatMessage(globalMessages.unitAda)}`\n                      : `0${\n                          this.getCurrentNumberFormat().decimalSeparator\n                        }${'0'.repeat(\n                          this.props.currencyMaxFractionalDigits\n                        )} ${intl.formatMessage(globalMessages.unitAda)}`\n                  }\n                  isSet\n                />\n                {this._isCalculatingTransactionFee && (\n                  <div className={styles.calculatingFeesContainer}>\n                    <PopOver\n                      content={intl.formatMessage(\n                        messages.calculatingFeesLabel\n                      )}\n                    >\n                      <button className={calculatingFeesSpinnerButtonClasses} />\n                    </PopOver>\n                  </div>\n                )}\n              </div>\n              <div className={styles.buttonsContainer}>\n                <Button\n                  className=\"flat\"\n                  label={intl.formatMessage(messages.resetButtonLabel)}\n                  disabled={isResetButtonDisabled}\n                  onClick={this.handleOnReset}\n                />\n                <Button\n                  className=\"primary\"\n                  label={intl.formatMessage(messages.sendButtonLabel)}\n                  disabled={this.isDisabled()}\n                  onClick={this.handleOnSubmit}\n                />\n              </div>\n            </div>\n          </BorderedBox>\n        )}\n\n        {isDialogOpen(WalletSendAssetsConfirmationDialog) ? (\n          <WalletSendConfirmationDialogContainer\n            currencyUnit={currencyUnit}\n            receiver={receiver}\n            assets={this.selectedAssets}\n            assetsAmounts={this.selectedAssetsAmounts}\n            amount={adaAmount.toFormat(currencyMaxFractionalDigits)}\n            amountToNaturalUnits={formattedAmountToNaturalUnits}\n            totalAmount={total}\n            transactionFee={fees}\n            hwDeviceStatus={hwDeviceStatus}\n            isHardwareWallet={isHardwareWallet}\n            onExternalLinkClick={onExternalLinkClick}\n          />\n        ) : null}\n      </div>\n    );\n  }\n}\n", "hunk": "@@ -455,7 +455,7 @@ export default class WalletSendForm extends Component<Props, State> {\n           this.state.feeCalculationRequestQue,\n           prevFeeCalculationRequestQue\n         ) &&\n-        !this.selectedAssetsAmounts.includes('0')\n+        !this.selectedAssetsAmounts.includes(emptyAssetFieldValue)\n       ) {\n         this._isCalculatingTransactionFee = false;\n         this.setState({\n", "comment": "@yakovkaravelov I'm not sure that this is correct. What will be if the asset amount is `10` (10 also includes 0) ? Maybe you should check if the asset is added and not empty or greater than 0.", "ids": ["42418", "af5672767c98b52cee2adacf7131b2110c5cc3b1", "d7e078aa7d3d4fcbfdb947597ec0b2249b4b9006"], "repo": "input-output-hk/daedalus", "ghid": 2501, "old": "           this.state.feeCalculationRequestQue,\n           prevFeeCalculationRequestQue\n         ) &&\n-        !this.selectedAssetsAmounts.includes('0')\n       ) {\n         this._isCalculatingTransactionFee = false;\n         this.setState({", "new": "           this.state.feeCalculationRequestQue,\n           prevFeeCalculationRequestQue\n         ) &&\n+        !this.selectedAssetsAmounts.includes(emptyAssetFieldValue)\n       ) {\n         this._isCalculatingTransactionFee = false;\n         this.setState({", "lang": "js", "norm_lang": "javascript"}
{"old_hunk": "@@ -241,4 +256,11 @@ public synchronized UniqueNameAllocator getUniqueNameAllocator() {\n     }\n     return nameAllocator;\n   }\n+\n+  public synchronized CryptoService getCryptoService() {", "oldf": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.accumulo.server;\n\nimport static com.google.common.base.Preconditions.checkArgument;\n\nimport java.io.IOException;\nimport java.util.Objects;\n\nimport org.apache.accumulo.core.Constants;\nimport org.apache.accumulo.core.client.AccumuloException;\nimport org.apache.accumulo.core.client.AccumuloSecurityException;\nimport org.apache.accumulo.core.client.ClientInfo;\nimport org.apache.accumulo.core.client.Connector;\nimport org.apache.accumulo.core.client.impl.ClientContext;\nimport org.apache.accumulo.core.client.impl.ConnectorImpl;\nimport org.apache.accumulo.core.client.security.tokens.AuthenticationToken;\nimport org.apache.accumulo.core.conf.AccumuloConfiguration;\nimport org.apache.accumulo.core.conf.Property;\nimport org.apache.accumulo.core.conf.SiteConfiguration;\nimport org.apache.accumulo.core.rpc.SslConnectionParams;\nimport org.apache.accumulo.core.security.crypto.CryptoServiceFactory;\nimport org.apache.accumulo.core.spi.crypto.CryptoService;\nimport org.apache.accumulo.core.trace.DistributedTrace;\nimport org.apache.accumulo.server.conf.ServerConfigurationFactory;\nimport org.apache.accumulo.server.fs.VolumeManager;\nimport org.apache.accumulo.server.metrics.MetricsSystemHelper;\nimport org.apache.accumulo.server.rpc.SaslServerConnectionParams;\nimport org.apache.accumulo.server.rpc.ThriftServerType;\nimport org.apache.accumulo.server.security.SecurityUtil;\nimport org.apache.accumulo.server.security.delegation.AuthenticationTokenSecretManager;\nimport org.apache.accumulo.server.tables.TableManager;\nimport org.apache.accumulo.server.tablets.UniqueNameAllocator;\nimport org.apache.hadoop.security.UserGroupInformation;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * Provides a server context for Accumulo server components that operate with the system credentials\n * and have access to the system files and configuration.\n */\npublic class ServerContext extends ClientContext {\n\n  private static final Logger log = LoggerFactory.getLogger(ServerContext.class);\n\n  private final ServerInfo info;\n  private TableManager tableManager;\n  private UniqueNameAllocator nameAllocator;\n  private ServerConfigurationFactory serverConfFactory = null;\n  private String applicationName = null;\n  private String applicationClassName = null;\n  private String hostname = null;\n  private AuthenticationTokenSecretManager secretManager;\n  private CryptoService cryptoService = null;\n\n  public ServerContext(SiteConfiguration siteConfig) {\n    this(new ServerInfo(siteConfig));\n  }\n\n  public ServerContext(SiteConfiguration siteConfig, String instanceName, String zooKeepers,\n      int zooKeepersSessionTimeOut) {\n    this(new ServerInfo(siteConfig, instanceName, zooKeepers, zooKeepersSessionTimeOut));\n  }\n\n  public ServerContext(SiteConfiguration siteConfig, ClientInfo info) {\n    this(new ServerInfo(siteConfig, info.getInstanceName(), info.getZooKeepers(),\n        info.getZooKeepersSessionTimeOut()));\n  }\n\n  private ServerContext(ServerInfo info) {\n    super(info, info.getSiteConfiguration());\n    this.info = info;\n  }\n\n  public void setupServer(String appName, String appClassName, String hostname) {\n    applicationName = appName;\n    applicationClassName = appClassName;\n    this.hostname = hostname;\n    SecurityUtil.serverLogin(info.getSiteConfiguration());\n    log.info(\"Version \" + Constants.VERSION);\n    log.info(\"Instance \" + info.getInstanceID());\n    try {\n      Accumulo.init(getVolumeManager(), getInstanceID(), getServerConfFactory(), applicationName);\n    } catch (IOException e) {\n      throw new IllegalStateException(e);\n    }\n    MetricsSystemHelper.configure(applicationClassName);\n    DistributedTrace.enable(hostname, applicationName,\n        getServerConfFactory().getSystemConfiguration());\n    if (null != getSaslParams()) {\n      // Server-side \"client\" check to make sure we're logged in as a user we expect to be\n      enforceKerberosLogin();\n    }\n  }\n\n  /**\n   * Should only be called by the Tablet server\n   */\n  public synchronized void setupCrypto() throws CryptoService.CryptoException {\n    if (cryptoService != null)\n      throw new CryptoService.CryptoException(\"Crypto Service \" + cryptoService.getClass().getName()\n          + \" already exists and cannot be setup again\");\n\n    AccumuloConfiguration acuConf = getConfiguration();\n    cryptoService = CryptoServiceFactory.newInstance(acuConf);\n  }\n\n  public void teardownServer() {\n    DistributedTrace.disable();\n  }\n\n  public String getApplicationName() {\n    Objects.requireNonNull(applicationName);\n    return applicationName;\n  }\n\n  public String getApplicationClassName() {\n    Objects.requireNonNull(applicationClassName);\n    return applicationName;\n  }\n\n  public String getHostname() {\n    Objects.requireNonNull(hostname);\n    return hostname;\n  }\n\n  public synchronized ServerConfigurationFactory getServerConfFactory() {\n    if (serverConfFactory == null) {\n      serverConfFactory = new ServerConfigurationFactory(this, info.getSiteConfiguration());\n    }\n    return serverConfFactory;\n  }\n\n  @Override\n  public AccumuloConfiguration getConfiguration() {\n    return getServerConfFactory().getSystemConfiguration();\n  }\n\n  /**\n   * A \"client-side\" assertion for servers to validate that they are logged in as the expected user,\n   * per the configuration, before performing any RPC\n   */\n  // Should be private, but package-protected so EasyMock will work\n  void enforceKerberosLogin() {\n    final AccumuloConfiguration conf = getServerConfFactory().getSiteConfiguration();\n    // Unwrap _HOST into the FQDN to make the kerberos principal we'll compare against\n    final String kerberosPrincipal = SecurityUtil\n        .getServerPrincipal(conf.get(Property.GENERAL_KERBEROS_PRINCIPAL));\n    UserGroupInformation loginUser;\n    try {\n      // The system user should be logged in via keytab when the process is started, not the\n      // currentUser() like KerberosToken\n      loginUser = UserGroupInformation.getLoginUser();\n    } catch (IOException e) {\n      throw new RuntimeException(\"Could not get login user\", e);\n    }\n\n    checkArgument(loginUser.hasKerberosCredentials(), \"Server does not have Kerberos credentials\");\n    checkArgument(kerberosPrincipal.equals(loginUser.getUserName()),\n        \"Expected login user to be \" + kerberosPrincipal + \" but was \" + loginUser.getUserName());\n  }\n\n  public VolumeManager getVolumeManager() {\n    return info.getVolumeManager();\n  }\n\n  /**\n   * Retrieve the SSL/TLS configuration for starting up a listening service\n   */\n  public SslConnectionParams getServerSslParams() {\n    return SslConnectionParams.forServer(getConfiguration());\n  }\n\n  @Override\n  public SaslServerConnectionParams getSaslParams() {\n    AccumuloConfiguration conf = getServerConfFactory().getSiteConfiguration();\n    if (!conf.getBoolean(Property.INSTANCE_RPC_SASL_ENABLED)) {\n      return null;\n    }\n    return new SaslServerConnectionParams(conf, getCredentials().getToken(), secretManager);\n  }\n\n  /**\n   * Determine the type of Thrift server to instantiate given the server's configuration.\n   *\n   * @return A {@link ThriftServerType} value to denote the type of Thrift server to construct\n   */\n  public ThriftServerType getThriftServerType() {\n    AccumuloConfiguration conf = getConfiguration();\n    if (conf.getBoolean(Property.INSTANCE_RPC_SSL_ENABLED)) {\n      if (conf.getBoolean(Property.INSTANCE_RPC_SASL_ENABLED)) {\n        throw new IllegalStateException(\n            \"Cannot create a Thrift server capable of both SASL and SSL\");\n      }\n\n      return ThriftServerType.SSL;\n    } else if (conf.getBoolean(Property.INSTANCE_RPC_SASL_ENABLED)) {\n      if (conf.getBoolean(Property.INSTANCE_RPC_SSL_ENABLED)) {\n        throw new IllegalStateException(\n            \"Cannot create a Thrift server capable of both SASL and SSL\");\n      }\n\n      return ThriftServerType.SASL;\n    } else {\n      // Lets us control the type of Thrift server created, primarily for benchmarking purposes\n      String serverTypeName = conf.get(Property.GENERAL_RPC_SERVER_TYPE);\n      return ThriftServerType.get(serverTypeName);\n    }\n  }\n\n  public void setSecretManager(AuthenticationTokenSecretManager secretManager) {\n    this.secretManager = secretManager;\n  }\n\n  public AuthenticationTokenSecretManager getSecretManager() {\n    return secretManager;\n  }\n\n  @Override\n  public synchronized Connector getConnector() throws AccumuloException, AccumuloSecurityException {\n    if (conn == null) {\n      conn = new ConnectorImpl(this);\n    }\n    return conn;\n  }\n\n  public Connector getConnector(String principal, AuthenticationToken token)\n      throws AccumuloSecurityException, AccumuloException {\n    return Connector.builder().usingClientInfo(info).usingToken(principal, token).build();\n  }\n\n  public synchronized TableManager getTableManager() {\n    if (tableManager == null) {\n      tableManager = new TableManager(this);\n    }\n    return tableManager;\n  }\n\n  public synchronized UniqueNameAllocator getUniqueNameAllocator() {\n    if (nameAllocator == null) {\n      nameAllocator = new UniqueNameAllocator(this);\n    }\n    return nameAllocator;\n  }\n\n  public synchronized CryptoService getCryptoService() {\n    if (cryptoService == null) {\n      throw new CryptoService.CryptoException(\"Crypto service not initialized.\");\n    }\n    return cryptoService;\n  }\n}\n", "hunk": "@@ -257,7 +257,7 @@ public class ServerContext extends ClientContext {\n     return nameAllocator;\n   }\n \n-  public synchronized CryptoService getCryptoService() {\n+  public CryptoService getCryptoService() {\n     if (cryptoService == null) {\n       throw new CryptoService.CryptoException(\"Crypto service not initialized.\");\n     }\n", "comment": "not sure if this needs to be synchronized", "ids": ["13238", "b09678db0a7e6de8b3c7c53d4c13da6f67acdfe4", "143ec5decb7db6b2a0424a8844e97571e596fa64"], "repo": "apache/accumulo", "ghid": 609, "old": "     return nameAllocator;\n   }\n-  public synchronized CryptoService getCryptoService() {\n     if (cryptoService == null) {\n       throw new CryptoService.CryptoException(\"Crypto service not initialized.\");\n     }", "new": "     return nameAllocator;\n   }\n+  public CryptoService getCryptoService() {\n     if (cryptoService == null) {\n       throw new CryptoService.CryptoException(\"Crypto service not initialized.\");\n     }", "lang": "java", "norm_lang": "java"}
{"old_hunk": "@@ -521,11 +521,7 @@ define([\"loading\", \"appRouter\", \"layoutManager\", \"connectionManager\", \"cardBuild\n         renderDetails(page, item, apiClient, context);\n         renderTrackSelections(page, instance, item);\n \n-        if (dom.getWindowSize().innerWidth >= 1000) {\n-            backdrop.setBackdrops([item]);\n-        } else {\n-            backdrop.clear();\n-        }\n+        backdrop.clear();", "oldf": "define([\"loading\", \"appRouter\", \"layoutManager\", \"connectionManager\", \"cardBuilder\", \"datetime\", \"mediaInfo\", \"backdrop\", \"listView\", \"itemContextMenu\", \"itemHelper\", \"dom\", \"indicators\", \"apphost\", \"imageLoader\", \"libraryMenu\", \"globalize\", \"browser\", \"events\", \"scrollHelper\", \"playbackManager\", \"libraryBrowser\", \"scrollStyles\", \"emby-itemscontainer\", \"emby-checkbox\", \"emby-button\", \"emby-playstatebutton\", \"emby-ratingbutton\", \"emby-scroller\", \"emby-select\"], function (loading, appRouter, layoutManager, connectionManager, cardBuilder, datetime, mediaInfo, backdrop, listView, itemContextMenu, itemHelper, dom, indicators, appHost, imageLoader, libraryMenu, globalize, browser, events, scrollHelper, playbackManager, libraryBrowser) {\n    \"use strict\";\n\n    function getPromise(apiClient, params) {\n        var id = params.id;\n\n        if (id) {\n            return apiClient.getItem(apiClient.getCurrentUserId(), id);\n        }\n\n        if (params.seriesTimerId) {\n            return apiClient.getLiveTvSeriesTimer(params.seriesTimerId);\n        }\n\n        if (params.genre) {\n            return apiClient.getGenre(params.genre, apiClient.getCurrentUserId());\n        }\n\n        if (params.musicgenre) {\n            return apiClient.getMusicGenre(params.musicgenre, apiClient.getCurrentUserId());\n        }\n\n        if (params.musicartist) {\n            return apiClient.getArtist(params.musicartist, apiClient.getCurrentUserId());\n        }\n\n        throw new Error(\"Invalid request\");\n    }\n\n    function hideAll(page, className, show) {\n        var i;\n        var length;\n        var elems = page.querySelectorAll(\".\" + className);\n\n        for (i = 0, length = elems.length; i < length; i++) {\n            if (show) {\n                elems[i].classList.remove(\"hide\");\n            } else {\n                elems[i].classList.add(\"hide\");\n            }\n        }\n    }\n\n    function getContextMenuOptions(item, user, button) {\n        var options = {\n            item: item,\n            open: false,\n            play: false,\n            playAllFromHere: false,\n            queueAllFromHere: false,\n            positionTo: button,\n            cancelTimer: false,\n            record: false,\n            deleteItem: true === item.IsFolder,\n            shuffle: false,\n            instantMix: false,\n            user: user,\n            share: true\n        };\n        return options;\n    }\n\n    function getProgramScheduleHtml(items, options) {\n        options = options || {};\n        var html = \"\";\n        html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer vertical-list\" data-contextmenu=\"false\">';\n        html += listView.getListViewHtml({\n            items: items,\n            enableUserDataButtons: false,\n            image: true,\n            imageSource: \"channel\",\n            showProgramDateTime: true,\n            showChannel: false,\n            mediaInfo: false,\n            action: \"none\",\n            moreButton: false,\n            recordButton: false\n        });\n        return html += \"</div>\";\n    }\n\n    function renderSeriesTimerSchedule(page, apiClient, seriesTimerId) {\n        apiClient.getLiveTvTimers({\n            UserId: apiClient.getCurrentUserId(),\n            ImageTypeLimit: 1,\n            EnableImageTypes: \"Primary,Backdrop,Thumb\",\n            SortBy: \"StartDate\",\n            EnableTotalRecordCount: false,\n            EnableUserData: false,\n            SeriesTimerId: seriesTimerId,\n            Fields: \"ChannelInfo,ChannelImage\"\n        }).then(function (result) {\n            if (result.Items.length && result.Items[0].SeriesTimerId != seriesTimerId) {\n                result.Items = [];\n            }\n\n            var html = getProgramScheduleHtml(result.Items);\n            var scheduleTab = page.querySelector(\".seriesTimerSchedule\");\n            scheduleTab.innerHTML = html;\n            imageLoader.lazyChildren(scheduleTab);\n        });\n    }\n\n    function renderTimerEditor(page, item, apiClient, user) {\n        if (\"Recording\" !== item.Type || !user.Policy.EnableLiveTvManagement || !item.TimerId || \"InProgress\" !== item.Status) {\n            return void hideAll(page, \"btnCancelTimer\");\n        }\n\n        hideAll(page, \"btnCancelTimer\", true);\n    }\n\n    function renderSeriesTimerEditor(page, item, apiClient, user) {\n        if (\"SeriesTimer\" !== item.Type) {\n            return void hideAll(page, \"btnCancelSeriesTimer\");\n        }\n\n        if (user.Policy.EnableLiveTvManagement) {\n            require([\"seriesRecordingEditor\"], function (seriesRecordingEditor) {\n                seriesRecordingEditor.embed(item, apiClient.serverId(), {\n                    context: page.querySelector(\".seriesRecordingEditor\")\n                });\n            });\n\n            page.querySelector(\".seriesTimerScheduleSection\").classList.remove(\"hide\");\n            hideAll(page, \"btnCancelSeriesTimer\", true);\n            return void renderSeriesTimerSchedule(page, apiClient, item.Id);\n        }\n\n        page.querySelector(\".seriesTimerScheduleSection\").classList.add(\"hide\");\n        return void hideAll(page, \"btnCancelSeriesTimer\");\n    }\n\n    function renderTrackSelections(page, instance, item, forceReload) {\n        var select = page.querySelector(\".selectSource\");\n\n        if (!item.MediaSources || !itemHelper.supportsMediaSourceSelection(item) || -1 === playbackManager.getSupportedCommands().indexOf(\"PlayMediaSource\") || !playbackManager.canPlay(item)) {\n            page.querySelector(\".trackSelections\").classList.add(\"hide\");\n            select.innerHTML = \"\";\n            page.querySelector(\".selectVideo\").innerHTML = \"\";\n            page.querySelector(\".selectAudio\").innerHTML = \"\";\n            page.querySelector(\".selectSubtitles\").innerHTML = \"\";\n            return;\n        }\n\n        playbackManager.getPlaybackMediaSources(item).then(function (mediaSources) {\n            instance._currentPlaybackMediaSources = mediaSources;\n            page.querySelector(\".trackSelections\").classList.remove(\"hide\");\n            select.setLabel(globalize.translate(\"LabelVersion\"));\n            var currentValue = select.value;\n            var selectedId = mediaSources[0].Id;\n            select.innerHTML = mediaSources.map(function (v) {\n                var selected = v.Id === selectedId ? \" selected\" : \"\";\n                return '<option value=\"' + v.Id + '\"' + selected + \">\" + v.Name + \"</option>\";\n            }).join(\"\");\n\n            if (mediaSources.length > 1) {\n                page.querySelector(\".selectSourceContainer\").classList.remove(\"hide\");\n            } else {\n                page.querySelector(\".selectSourceContainer\").classList.add(\"hide\");\n            }\n\n            if (select.value !== currentValue || forceReload) {\n                renderVideoSelections(page, mediaSources);\n                renderAudioSelections(page, mediaSources);\n                renderSubtitleSelections(page, mediaSources);\n            }\n        });\n    }\n\n    function renderVideoSelections(page, mediaSources) {\n        var mediaSourceId = page.querySelector(\".selectSource\").value;\n        var mediaSource = mediaSources.filter(function (m) {\n            return m.Id === mediaSourceId;\n        })[0];\n        var tracks = mediaSource.MediaStreams.filter(function (m) {\n            return \"Video\" === m.Type;\n        });\n        var select = page.querySelector(\".selectVideo\");\n        select.setLabel(globalize.translate(\"LabelVideo\"));\n        var selectedId = tracks.length ? tracks[0].Index : -1;\n        select.innerHTML = tracks.map(function (v) {\n            var selected = v.Index === selectedId ? \" selected\" : \"\";\n            var titleParts = [];\n            var resolutionText = mediaInfo.getResolutionText(v);\n\n            if (resolutionText) {\n                titleParts.push(resolutionText);\n            }\n\n            if (v.Codec) {\n                titleParts.push(v.Codec.toUpperCase());\n            }\n\n            return '<option value=\"' + v.Index + '\" ' + selected + \">\" + (v.DisplayTitle || titleParts.join(\" \")) + \"</option>\";\n        }).join(\"\");\n        select.setAttribute(\"disabled\", \"disabled\");\n\n        if (tracks.length) {\n            page.querySelector(\".selectVideoContainer\").classList.remove(\"hide\");\n        } else {\n            page.querySelector(\".selectVideoContainer\").classList.add(\"hide\");\n        }\n    }\n\n    function renderAudioSelections(page, mediaSources) {\n        var mediaSourceId = page.querySelector(\".selectSource\").value;\n        var mediaSource = mediaSources.filter(function (m) {\n            return m.Id === mediaSourceId;\n        })[0];\n        var tracks = mediaSource.MediaStreams.filter(function (m) {\n            return \"Audio\" === m.Type;\n        });\n        var select = page.querySelector(\".selectAudio\");\n        select.setLabel(globalize.translate(\"LabelAudio\"));\n        var selectedId = mediaSource.DefaultAudioStreamIndex;\n        select.innerHTML = tracks.map(function (v) {\n            var selected = v.Index === selectedId ? \" selected\" : \"\";\n            return '<option value=\"' + v.Index + '\" ' + selected + \">\" + v.DisplayTitle + \"</option>\";\n        }).join(\"\");\n\n        if (tracks.length > 1) {\n            select.removeAttribute(\"disabled\");\n        } else {\n            select.setAttribute(\"disabled\", \"disabled\");\n        }\n\n        if (tracks.length) {\n            page.querySelector(\".selectAudioContainer\").classList.remove(\"hide\");\n        } else {\n            page.querySelector(\".selectAudioContainer\").classList.add(\"hide\");\n        }\n    }\n\n    function renderSubtitleSelections(page, mediaSources) {\n        var mediaSourceId = page.querySelector(\".selectSource\").value;\n        var mediaSource = mediaSources.filter(function (m) {\n            return m.Id === mediaSourceId;\n        })[0];\n        var tracks = mediaSource.MediaStreams.filter(function (m) {\n            return \"Subtitle\" === m.Type;\n        });\n        var select = page.querySelector(\".selectSubtitles\");\n        select.setLabel(globalize.translate(\"LabelSubtitles\"));\n        var selectedId = null == mediaSource.DefaultSubtitleStreamIndex ? -1 : mediaSource.DefaultSubtitleStreamIndex;\n\n        if (tracks.length) {\n            var selected = -1 === selectedId ? \" selected\" : \"\";\n            select.innerHTML = '<option value=\"-1\">' + globalize.translate(\"Off\") + \"</option>\" + tracks.map(function (v) {\n                selected = v.Index === selectedId ? \" selected\" : \"\";\n                return '<option value=\"' + v.Index + '\" ' + selected + \">\" + v.DisplayTitle + \"</option>\";\n            }).join(\"\");\n            page.querySelector(\".selectSubtitlesContainer\").classList.remove(\"hide\");\n        } else {\n            select.innerHTML = \"\";\n            page.querySelector(\".selectSubtitlesContainer\").classList.add(\"hide\");\n        }\n    }\n\n    function reloadPlayButtons(page, item) {\n        var canPlay = false;\n\n        if (\"Program\" == item.Type) {\n            var now = new Date();\n\n            if (now >= datetime.parseISO8601Date(item.StartDate, true) && now < datetime.parseISO8601Date(item.EndDate, true)) {\n                hideAll(page, \"btnPlay\", true);\n                canPlay = true;\n            } else {\n                hideAll(page, \"btnPlay\");\n            }\n\n            hideAll(page, \"btnResume\");\n            hideAll(page, \"btnInstantMix\");\n            hideAll(page, \"btnShuffle\");\n        } else if (playbackManager.canPlay(item)) {\n            hideAll(page, \"btnPlay\", true);\n            var enableInstantMix = -1 !== [\"Audio\", \"MusicAlbum\", \"MusicGenre\", \"MusicArtist\"].indexOf(item.Type);\n            hideAll(page, \"btnInstantMix\", enableInstantMix);\n            var enableShuffle = item.IsFolder || -1 !== [\"MusicAlbum\", \"MusicGenre\", \"MusicArtist\"].indexOf(item.Type);\n            hideAll(page, \"btnShuffle\", enableShuffle);\n            canPlay = true;\n            hideAll(page, \"btnResume\", item.UserData && item.UserData.PlaybackPositionTicks > 0);\n        } else {\n            hideAll(page, \"btnPlay\");\n            hideAll(page, \"btnResume\");\n            hideAll(page, \"btnInstantMix\");\n            hideAll(page, \"btnShuffle\");\n        }\n\n        return canPlay;\n    }\n\n    function reloadUserDataButtons(page, item) {\n        var i;\n        var length;\n        var btnPlaystates = page.querySelectorAll(\".btnPlaystate\");\n\n        for (i = 0, length = btnPlaystates.length; i < length; i++) {\n            var btnPlaystate = btnPlaystates[i];\n\n            if (itemHelper.canMarkPlayed(item)) {\n                btnPlaystate.classList.remove(\"hide\");\n                btnPlaystate.setItem(item);\n            } else {\n                btnPlaystate.classList.add(\"hide\");\n                btnPlaystate.setItem(null);\n            }\n        }\n\n        var btnUserRatings = page.querySelectorAll(\".btnUserRating\");\n\n        for (i = 0, length = btnUserRatings.length; i < length; i++) {\n            var btnUserRating = btnUserRatings[i];\n\n            if (itemHelper.canRate(item)) {\n                btnUserRating.classList.remove(\"hide\");\n                btnUserRating.setItem(item);\n            } else {\n                btnUserRating.classList.add(\"hide\");\n                btnUserRating.setItem(null);\n            }\n        }\n    }\n\n    function getArtistLinksHtml(artists, serverId, context) {\n        var html = [];\n\n        for (var i = 0, length = artists.length; i < length; i++) {\n            var artist = artists[i];\n            var href = appRouter.getRouteUrl(artist, {\n                context: context,\n                itemType: \"MusicArtist\",\n                serverId: serverId\n            });\n            html.push('<a style=\"color:inherit;\" class=\"button-link\" is=\"emby-linkbutton\" href=\"' + href + '\">' + artist.Name + \"</a>\");\n        }\n\n        return html = html.join(\" / \");\n    }\n\n    function renderName(item, container, isStatic, context) {\n        var parentRoute;\n        var parentNameHtml = [];\n        var parentNameLast = false;\n\n        if (item.AlbumArtists) {\n            parentNameHtml.push(getArtistLinksHtml(item.AlbumArtists, item.ServerId, context));\n            parentNameLast = true;\n        } else if (item.ArtistItems && item.ArtistItems.length && \"MusicVideo\" === item.Type) {\n            parentNameHtml.push(getArtistLinksHtml(item.ArtistItems, item.ServerId, context));\n            parentNameLast = true;\n        } else if (item.SeriesName && \"Episode\" === item.Type) {\n            parentRoute = appRouter.getRouteUrl({\n                Id: item.SeriesId,\n                Name: item.SeriesName,\n                Type: \"Series\",\n                IsFolder: true,\n                ServerId: item.ServerId\n            }, {\n                context: context\n            });\n            parentNameHtml.push('<a style=\"color:inherit;\" class=\"button-link\" is=\"emby-linkbutton\" href=\"' + parentRoute + '\">' + item.SeriesName + \"</a>\");\n        } else if (item.IsSeries || item.EpisodeTitle) {\n            parentNameHtml.push(item.Name);\n        }\n\n        if (item.SeriesName && \"Season\" === item.Type) {\n            parentRoute = appRouter.getRouteUrl({\n                Id: item.SeriesId,\n                Name: item.SeriesName,\n                Type: \"Series\",\n                IsFolder: true,\n                ServerId: item.ServerId\n            }, {\n                context: context\n            });\n            parentNameHtml.push('<a style=\"color:inherit;\" class=\"button-link\" is=\"emby-linkbutton\" href=\"' + parentRoute + '\">' + item.SeriesName + \"</a>\");\n        } else if (null != item.ParentIndexNumber && \"Episode\" === item.Type) {\n            parentRoute = appRouter.getRouteUrl({\n                Id: item.SeasonId,\n                Name: item.SeasonName,\n                Type: \"Season\",\n                IsFolder: true,\n                ServerId: item.ServerId\n            }, {\n                context: context\n            });\n            parentNameHtml.push('<a style=\"color:inherit;\" class=\"button-link\" is=\"emby-linkbutton\" href=\"' + parentRoute + '\">' + item.SeasonName + \"</a>\");\n        } else if (null != item.ParentIndexNumber && item.IsSeries) {\n            parentNameHtml.push(item.SeasonName || \"S\" + item.ParentIndexNumber);\n        } else if (item.Album && item.AlbumId && (\"MusicVideo\" === item.Type || \"Audio\" === item.Type)) {\n            parentRoute = appRouter.getRouteUrl({\n                Id: item.AlbumId,\n                Name: item.Album,\n                Type: \"MusicAlbum\",\n                IsFolder: true,\n                ServerId: item.ServerId\n            }, {\n                context: context\n            });\n            parentNameHtml.push('<a style=\"color:inherit;\" class=\"button-link\" is=\"emby-linkbutton\" href=\"' + parentRoute + '\">' + item.Album + \"</a>\");\n        } else if (item.Album) {\n            parentNameHtml.push(item.Album);\n        }\n\n        var html = \"\";\n\n        if (parentNameHtml.length) {\n            if (parentNameLast) {\n                html = '<h3 class=\"parentName\" style=\"margin: .25em 0;\">' + parentNameHtml.join(\" - \") + \"</h3>\";\n            } else {\n                html = '<h1 class=\"parentName\" style=\"margin: .1em 0 .25em;\">' + parentNameHtml.join(\" - \") + \"</h1>\";\n            }\n        }\n\n        var name = itemHelper.getDisplayName(item, {\n            includeParentInfo: false\n        });\n        var offset = parentNameLast ? \".25em\" : \".5em\";\n\n        if (html && !parentNameLast) {\n            html += '<h3 class=\"itemName\" style=\"margin: .25em 0 .5em;\">' + name + '</h3>';\n        } else {\n            html = '<h1 class=\"itemName\" style=\"margin: .1em 0 ' + offset + ';\">' + name + \"</h1>\" + html;\n        }\n\n        if (item.OriginalTitle && item.OriginalTitle != item.Name) {\n            html += '<h4 class=\"itemName\" style=\"margin: -' + offset + ' 0 0\">' + item.OriginalTitle + '</h4>';\n        }\n\n        container.innerHTML = html;\n\n        if (html.length) {\n            container.classList.remove(\"hide\");\n        } else {\n            container.classList.add(\"hide\");\n        }\n    }\n\n    function setTrailerButtonVisibility(page, item) {\n        if ((item.LocalTrailerCount || item.RemoteTrailers && item.RemoteTrailers.length) && -1 !== playbackManager.getSupportedCommands().indexOf(\"PlayTrailers\")) {\n            hideAll(page, \"btnPlayTrailer\", true);\n        } else {\n            hideAll(page, \"btnPlayTrailer\");\n        }\n    }\n\n    function renderDetailPageBackdrop(page, item, apiClient) {\n        var imgUrl;\n        var screenWidth = screen.availWidth;\n        var hasbackdrop = false;\n        var itemBackdropElement = page.querySelector(\"#itemBackdrop\");\n        var usePrimaryImage = item.MediaType === \"Video\" && item.Type !== \"Movie\" && item.Type !== \"Trailer\" ||\n            item.MediaType && item.MediaType !== \"Video\" ||\n            item.Type === \"MusicAlbum\" ||\n            item.Type === \"MusicArtist\";\n\n        if (\"Program\" === item.Type && item.ImageTags && item.ImageTags.Thumb) {\n            imgUrl = apiClient.getScaledImageUrl(item.Id, {\n                type: \"Thumb\",\n                index: 0,\n                tag: item.ImageTags.Thumb\n            });\n            itemBackdropElement.classList.remove(\"noBackdrop\");\n            imageLoader.lazyImage(itemBackdropElement, imgUrl, false);\n            hasbackdrop = true;\n        } else if (usePrimaryImage && item.ImageTags && item.ImageTags.Primary) {\n            imgUrl = apiClient.getScaledImageUrl(item.Id, {\n                type: \"Primary\",\n                index: 0,\n                tag: item.ImageTags.Primary\n            });\n            itemBackdropElement.classList.remove(\"noBackdrop\");\n            imageLoader.lazyImage(itemBackdropElement, imgUrl, false);\n            hasbackdrop = true;\n        } else if (item.BackdropImageTags && item.BackdropImageTags.length) {\n            imgUrl = apiClient.getScaledImageUrl(item.Id, {\n                type: \"Backdrop\",\n                index: 0,\n                tag: item.BackdropImageTags[0]\n            });\n            itemBackdropElement.classList.remove(\"noBackdrop\");\n            imageLoader.lazyImage(itemBackdropElement, imgUrl, false);\n            hasbackdrop = true;\n        } else if (item.ParentBackdropItemId && item.ParentBackdropImageTags && item.ParentBackdropImageTags.length) {\n            imgUrl = apiClient.getScaledImageUrl(item.ParentBackdropItemId, {\n                type: \"Backdrop\",\n                index: 0,\n                tag: item.ParentBackdropImageTags[0]\n            });\n            itemBackdropElement.classList.remove(\"noBackdrop\");\n            imageLoader.lazyImage(itemBackdropElement, imgUrl, false);\n            hasbackdrop = true;\n        } else if (item.ImageTags && item.ImageTags.Thumb) {\n            imgUrl = apiClient.getScaledImageUrl(item.Id, {\n                type: \"Thumb\",\n                index: 0,\n                tag: item.ImageTags.Thumb\n            });\n            itemBackdropElement.classList.remove(\"noBackdrop\");\n            imageLoader.lazyImage(itemBackdropElement, imgUrl, false);\n            hasbackdrop = true;\n        } else {\n            itemBackdropElement.classList.add(\"noBackdrop\");\n            itemBackdropElement.style.backgroundImage = \"\";\n        }\n\n        return hasbackdrop;\n    }\n\n    function reloadFromItem(instance, page, params, item, user) {\n        var context = params.context;\n        renderName(item, page.querySelector(\".nameContainer\"), false, context);\n        var apiClient = connectionManager.getApiClient(item.ServerId);\n        renderSeriesTimerEditor(page, item, apiClient, user);\n        renderTimerEditor(page, item, apiClient, user);\n        renderImage(page, item, apiClient, user);\n        renderLogo(page, item, apiClient);\n        setTitle(item, apiClient);\n        setInitialCollapsibleState(page, item, apiClient, context, user);\n        renderDetails(page, item, apiClient, context);\n        renderTrackSelections(page, instance, item);\n\n        backdrop.clear();\n\n        renderDetailPageBackdrop(page, item, apiClient);\n        var canPlay = reloadPlayButtons(page, item);\n\n        if ((item.LocalTrailerCount || item.RemoteTrailers && item.RemoteTrailers.length) && -1 !== playbackManager.getSupportedCommands().indexOf(\"PlayTrailers\")) {\n            hideAll(page, \"btnPlayTrailer\", true);\n        } else {\n            hideAll(page, \"btnPlayTrailer\");\n        }\n\n        setTrailerButtonVisibility(page, item);\n\n        if (item.CanDelete && !item.IsFolder) {\n            hideAll(page, \"btnDeleteItem\", true);\n        } else {\n            hideAll(page, \"btnDeleteItem\");\n        }\n\n        if (\"Program\" !== item.Type || canPlay) {\n            hideAll(page, \"mainDetailButtons\", true);\n        } else {\n            hideAll(page, \"mainDetailButtons\");\n        }\n\n        showRecordingFields(instance, page, item, user);\n        var groupedVersions = (item.MediaSources || []).filter(function (g) {\n            return \"Grouping\" == g.Type;\n        });\n\n        if (user.Policy.IsAdministrator && groupedVersions.length) {\n            page.querySelector(\".btnSplitVersions\").classList.remove(\"hide\");\n        } else {\n            page.querySelector(\".btnSplitVersions\").classList.add(\"hide\");\n        }\n\n        if (itemContextMenu.getCommands(getContextMenuOptions(item, user)).length) {\n            hideAll(page, \"btnMoreCommands\", true);\n        } else {\n            hideAll(page, \"btnMoreCommands\");\n        }\n\n        var itemBirthday = page.querySelector(\"#itemBirthday\");\n\n        if (\"Person\" == item.Type && item.PremiereDate) {\n            try {\n                var birthday = datetime.parseISO8601Date(item.PremiereDate, true).toDateString();\n                itemBirthday.classList.remove(\"hide\");\n                itemBirthday.innerHTML = globalize.translate(\"BirthDateValue\").replace(\"{0}\", birthday);\n            } catch (err) {\n                itemBirthday.classList.add(\"hide\");\n            }\n        } else {\n            itemBirthday.classList.add(\"hide\");\n        }\n\n        var itemDeathDate = page.querySelector(\"#itemDeathDate\");\n\n        if (\"Person\" == item.Type && item.EndDate) {\n            try {\n                var deathday = datetime.parseISO8601Date(item.EndDate, true).toDateString();\n                itemDeathDate.classList.remove(\"hide\");\n                itemDeathDate.innerHTML = globalize.translate(\"DeathDateValue\").replace(\"{0}\", deathday);\n            } catch (err) {\n                itemDeathDate.classList.add(\"hide\");\n            }\n        } else {\n            itemDeathDate.classList.add(\"hide\");\n        }\n\n        var itemBirthLocation = page.querySelector(\"#itemBirthLocation\");\n\n        if (\"Person\" == item.Type && item.ProductionLocations && item.ProductionLocations.length) {\n            var gmap = '<a is=\"emby-linkbutton\" class=\"button-link textlink\" target=\"_blank\" href=\"https://maps.google.com/maps?q=' + item.ProductionLocations[0] + '\">' + item.ProductionLocations[0] + \"</a>\";\n            itemBirthLocation.classList.remove(\"hide\");\n            itemBirthLocation.innerHTML = globalize.translate(\"BirthPlaceValue\").replace(\"{0}\", gmap);\n        } else {\n            itemBirthLocation.classList.add(\"hide\");\n        }\n\n        setPeopleHeader(page, item);\n        loading.hide();\n\n        if (item.Type === \"Book\") {\n            hideAll(page, \"btnDownload\", true);\n        }\n\n        try {\n            require([\"focusManager\"], function (focusManager) {\n                [\".btnResume\", \".btnPlay\"].every(function (cls) {\n                    var elems = page.querySelectorAll(cls);\n\n                    for (var i = 0; i < elems.length; i++) {\n                        if (focusManager.isCurrentlyFocusable(elems[i])) {\n                            focusManager.focus(elems[i]);\n                            return false;\n                        }\n                    }\n\n                    return true;\n                });\n            });\n        } catch (e) {\n            console.log(e);\n        }\n    }\n\n    function logoImageUrl(item, apiClient, options) {\n        options = options || {};\n        options.type = \"Logo\";\n\n        if (item.ImageTags && item.ImageTags.Logo) {\n            options.tag = item.ImageTags.Logo;\n            return apiClient.getScaledImageUrl(item.Id, options);\n        }\n\n        if (item.ParentLogoImageTag) {\n            options.tag = item.ParentLogoImageTag;\n            return apiClient.getScaledImageUrl(item.ParentLogoItemId, options);\n        }\n\n        return null;\n    }\n\n    function setTitle(item, apiClient) {\n        var url = logoImageUrl(item, apiClient, {});\n\n        if (url = null) {\n            var pageTitle = document.querySelector(\".pageTitle\");\n            pageTitle.style.backgroundImage = \"url('\" + url + \"')\";\n            pageTitle.classList.add(\"pageTitleWithLogo\");\n            pageTitle.innerHTML = \"\";\n        } else {\n            Emby.Page.setTitle(\"\");\n        }\n    }\n\n    function renderLogo(page, item, apiClient) {\n        var url = logoImageUrl(item, apiClient, {\n            maxWidth: 400\n        });\n        var detailLogo = page.querySelector(\".detailLogo\");\n\n        if (url) {\n            detailLogo.classList.remove(\"hide\");\n            detailLogo.classList.add(\"lazy\");\n            detailLogo.setAttribute(\"data-src\", url);\n            imageLoader.lazyImage(detailLogo);\n        } else {\n            detailLogo.classList.add(\"hide\");\n        }\n    }\n\n    function showRecordingFields(instance, page, item, user) {\n        if (!instance.currentRecordingFields) {\n            var recordingFieldsElement = page.querySelector(\".recordingFields\");\n\n            if (\"Program\" == item.Type && user.Policy.EnableLiveTvManagement) {\n                require([\"recordingFields\"], function (recordingFields) {\n                    instance.currentRecordingFields = new recordingFields({\n                        parent: recordingFieldsElement,\n                        programId: item.Id,\n                        serverId: item.ServerId\n                    });\n                    recordingFieldsElement.classList.remove(\"hide\");\n                });\n            } else {\n                recordingFieldsElement.classList.add(\"hide\");\n                recordingFieldsElement.innerHTML = \"\";\n            }\n        }\n    }\n\n    function renderUserInfo(page, item) {\n        var lastPlayedElement = page.querySelector(\".itemLastPlayed\");\n\n        if (item.UserData && item.UserData.LastPlayedDate) {\n            lastPlayedElement.classList.remove(\"hide\");\n            var datePlayed = datetime.parseISO8601Date(item.UserData.LastPlayedDate);\n            lastPlayedElement.innerHTML = globalize.translate(\"DatePlayed\") + \": \" + datetime.toLocaleDateString(datePlayed) + \" \" + datetime.getDisplayTime(datePlayed);\n        } else {\n            lastPlayedElement.classList.add(\"hide\");\n        }\n    }\n\n    function renderLinks(linksElem, item) {\n        var html = [];\n\n        if (item.DateCreated && itemHelper.enableDateAddedDisplay(item)) {\n            var dateCreated = datetime.parseISO8601Date(item.DateCreated);\n            html.push(globalize.translate(\"AddedOnValue\", datetime.toLocaleDateString(dateCreated) + \" \" + datetime.getDisplayTime(dateCreated)));\n        }\n\n        var links = [];\n\n        if (!layoutManager.tv && item.HomePageUrl) {\n            links.push('<a style=\"color:inherit;\" is=\"emby-linkbutton\" class=\"button-link\" href=\"' + item.HomePageUrl + '\" target=\"_blank\">' + globalize.translate(\"ButtonWebsite\") + \"</a>\");\n        }\n        if (item.ExternalUrls) {\n            for (var i = 0, length = item.ExternalUrls.length; i < length; i++) {\n                var url = item.ExternalUrls[i];\n                links.push('<a style=\"color:inherit;\" is=\"emby-linkbutton\" class=\"button-link\" href=\"' + url.Url + '\" target=\"_blank\">' + url.Name + \"</a>\");\n            }\n        }\n\n        if (links.length) {\n            html.push(globalize.translate(\"LinksValue\", links.join(\", \")));\n        }\n\n        linksElem.innerHTML = html.join(\", \");\n\n        if (html.length) {\n            linksElem.classList.remove(\"hide\");\n        } else {\n            linksElem.classList.add(\"hide\");\n        }\n    }\n\n    function renderDetailImage(page, elem, item, apiClient, editable, imageLoader, indicators) {\n        if (\"SeriesTimer\" === item.Type || \"Program\" === item.Type) {\n            editable = false;\n        }\n\n        if (\"Person\" !== item.Type) {\n            elem.classList.add(\"detailimg-hidemobile\");\n            page.querySelector(\".detailPageContent\").classList.add(\"detailPageContent-nodetailimg\");\n        } else {\n            page.querySelector(\".detailPageContent\").classList.remove(\"detailPageContent-nodetailimg\");\n        }\n\n        var imageTags = item.ImageTags || {};\n\n        if (item.PrimaryImageTag) {\n            imageTags.Primary = item.PrimaryImageTag;\n        }\n\n        var url;\n        var html = \"\";\n        var shape = \"portrait\";\n        var detectRatio = false;\n\n        if (imageTags.Primary) {\n            url = apiClient.getScaledImageUrl(item.Id, {\n                type: \"Primary\",\n                tag: item.ImageTags.Primary\n            });\n            detectRatio = true;\n        } else if (item.BackdropImageTags && item.BackdropImageTags.length) {\n            url = apiClient.getScaledImageUrl(item.Id, {\n                type: \"Backdrop\",\n                tag: item.BackdropImageTags[0]\n            });\n            shape = \"thumb\";\n        } else if (imageTags.Thumb) {\n            url = apiClient.getScaledImageUrl(item.Id, {\n                type: \"Thumb\",\n                tag: item.ImageTags.Thumb\n            });\n            shape = \"thumb\";\n        } else if (imageTags.Disc) {\n            url = apiClient.getScaledImageUrl(item.Id, {\n                type: \"Disc\",\n                tag: item.ImageTags.Disc\n            });\n            shape = \"square\";\n        } else if (item.AlbumId && item.AlbumPrimaryImageTag) {\n            url = apiClient.getScaledImageUrl(item.AlbumId, {\n                type: \"Primary\",\n                tag: item.AlbumPrimaryImageTag\n            });\n            shape = \"square\";\n        } else if (item.SeriesId && item.SeriesPrimaryImageTag) {\n            url = apiClient.getScaledImageUrl(item.SeriesId, {\n                type: \"Primary\",\n                tag: item.SeriesPrimaryImageTag\n            });\n        } else if (item.ParentPrimaryImageItemId && item.ParentPrimaryImageTag) {\n            url = apiClient.getScaledImageUrl(item.ParentPrimaryImageItemId, {\n                type: \"Primary\",\n                tag: item.ParentPrimaryImageTag\n            });\n        }\n\n        html += '<div style=\"position:relative;\">';\n\n        if (editable) {\n            html += \"<a class='itemDetailGalleryLink' is='emby-linkbutton' style='display:block;padding:2px;margin:0;' href='#'>\";\n        }\n\n        if (detectRatio && item.PrimaryImageAspectRatio) {\n            if (item.PrimaryImageAspectRatio >= 1.48) {\n                shape = \"thumb\";\n            } else if (item.PrimaryImageAspectRatio >= 0.85 && item.PrimaryImageAspectRatio <= 1.34) {\n                shape = \"square\";\n            }\n        }\n\n        html += \"<img class='itemDetailImage lazy' src='data:image/gif;base64,R0lGODlhAQABAAD/ACwAAAAAAQABAAACADs=' />\";\n\n        if (editable) {\n            html += \"</a>\";\n        }\n\n        var progressHtml = item.IsFolder || !item.UserData ? \"\" : indicators.getProgressBarHtml(item);\n        html += '<div class=\"detailImageProgressContainer\">';\n\n        if (progressHtml) {\n            html += progressHtml;\n        }\n\n        html += \"</div>\";\n        html += \"</div>\";\n        elem.innerHTML = html;\n\n        if (\"thumb\" == shape) {\n            elem.classList.add(\"thumbDetailImageContainer\");\n            elem.classList.remove(\"portraitDetailImageContainer\");\n            elem.classList.remove(\"squareDetailImageContainer\");\n        } else if (\"square\" == shape) {\n            elem.classList.remove(\"thumbDetailImageContainer\");\n            elem.classList.remove(\"portraitDetailImageContainer\");\n            elem.classList.add(\"squareDetailImageContainer\");\n        } else {\n            elem.classList.remove(\"thumbDetailImageContainer\");\n            elem.classList.add(\"portraitDetailImageContainer\");\n            elem.classList.remove(\"squareDetailImageContainer\");\n        }\n\n        if (url) {\n            imageLoader.lazyImage(elem.querySelector(\"img\"), url);\n        }\n    }\n\n    function renderImage(page, item, apiClient, user) {\n        renderDetailImage(\n            page,\n            page.querySelector(\".detailImageContainer\"),\n            item,\n            apiClient,\n            user.Policy.IsAdministrator && \"Photo\" != item.MediaType,\n            imageLoader,\n            indicators\n        );\n    }\n\n    function refreshDetailImageUserData(elem, item) {\n        elem.querySelector(\".detailImageProgressContainer\").innerHTML = indicators.getProgressBarHtml(item);\n    }\n\n    function refreshImage(page, item, user) {\n        refreshDetailImageUserData(page.querySelector(\".detailImageContainer\"), item);\n    }\n\n    function setPeopleHeader(page, item) {\n        if (\"Audio\" == item.MediaType || \"MusicAlbum\" == item.Type || \"Book\" == item.MediaType || \"Photo\" == item.MediaType) {\n            page.querySelector(\"#peopleHeader\").innerHTML = globalize.translate(\"HeaderPeople\");\n        } else {\n            page.querySelector(\"#peopleHeader\").innerHTML = globalize.translate(\"HeaderCastAndCrew\");\n        }\n    }\n\n    function renderNextUp(page, item, user) {\n        var section = page.querySelector(\".nextUpSection\");\n\n        if (\"Series\" != item.Type) {\n            return void section.classList.add(\"hide\");\n        }\n\n        connectionManager.getApiClient(item.ServerId).getNextUpEpisodes({\n            SeriesId: item.Id,\n            UserId: user.Id\n        }).then(function (result) {\n            if (result.Items.length) {\n                section.classList.remove(\"hide\");\n            } else {\n                section.classList.add(\"hide\");\n            }\n\n            var html = cardBuilder.getCardsHtml({\n                items: result.Items,\n                shape: getThumbShape(false),\n                showTitle: true,\n                displayAsSpecial: \"Season\" == item.Type && item.IndexNumber,\n                overlayText: false,\n                centerText: true,\n                overlayPlayButton: true\n            });\n            var itemsContainer = section.querySelector(\".nextUpItems\");\n            itemsContainer.innerHTML = html;\n            imageLoader.lazyChildren(itemsContainer);\n        });\n    }\n\n    function setInitialCollapsibleState(page, item, apiClient, context, user) {\n        page.querySelector(\".collectionItems\").innerHTML = \"\";\n\n        if (\"Playlist\" == item.Type) {\n            page.querySelector(\"#childrenCollapsible\").classList.remove(\"hide\");\n            renderPlaylistItems(page, item, user);\n        } else if (\"Studio\" == item.Type || \"Person\" == item.Type || \"Genre\" == item.Type || \"MusicGenre\" == item.Type || \"MusicArtist\" == item.Type) {\n            page.querySelector(\"#childrenCollapsible\").classList.remove(\"hide\");\n            renderItemsByName(page, item, user);\n        } else if (item.IsFolder) {\n            if (\"BoxSet\" == item.Type) {\n                page.querySelector(\"#childrenCollapsible\").classList.add(\"hide\");\n            }\n\n            renderChildren(page, item);\n        } else {\n            page.querySelector(\"#childrenCollapsible\").classList.add(\"hide\");\n        }\n\n        if (\"Series\" == item.Type) {\n            renderSeriesSchedule(page, item, user);\n            renderNextUp(page, item, user);\n        } else {\n            page.querySelector(\".nextUpSection\").classList.add(\"hide\");\n        }\n\n        renderScenes(page, item);\n\n        if (item.SpecialFeatureCount && 0 != item.SpecialFeatureCount && \"Series\" != item.Type) {\n            page.querySelector(\"#specialsCollapsible\").classList.remove(\"hide\");\n            renderSpecials(page, item, user, 6);\n        } else {\n            page.querySelector(\"#specialsCollapsible\").classList.add(\"hide\");\n        }\n\n        renderCast(page, item, context, enableScrollX() ? null : 12);\n\n        if (item.PartCount && item.PartCount > 1) {\n            page.querySelector(\"#additionalPartsCollapsible\").classList.remove(\"hide\");\n            renderAdditionalParts(page, item, user);\n        } else {\n            page.querySelector(\"#additionalPartsCollapsible\").classList.add(\"hide\");\n        }\n\n        if (\"MusicAlbum\" == item.Type) {\n            renderMusicVideos(page, item, user);\n        } else {\n            page.querySelector(\"#musicVideosCollapsible\").classList.add(\"hide\");\n        }\n    }\n\n    function renderOverview(elems, item) {\n        for (var i = 0, length = elems.length; i < length; i++) {\n            var elem = elems[i];\n            var overview = item.Overview || \"\";\n\n            if (overview) {\n                elem.innerHTML = overview;\n                elem.classList.remove(\"hide\");\n                var anchors = elem.querySelectorAll(\"a\");\n\n                for (var j = 0, length2 = anchors.length; j < length2; j++) {\n                    anchors[j].setAttribute(\"target\", \"_blank\");\n                }\n            } else {\n                elem.innerHTML = \"\";\n                elem.classList.add(\"hide\");\n            }\n        }\n    }\n\n    function renderGenres(page, item, apiClient, context, isStatic) {\n        context = context || inferContext(item);\n        var type;\n        var genres = item.GenreItems || [];\n\n        switch (context) {\n            case \"music\":\n                type = \"MusicGenre\";\n                break;\n\n            default:\n                type = \"Genre\";\n        }\n\n        var html = genres.map(function (p) {\n            return '<a style=\"color:inherit;\" class=\"button-link\" is=\"emby-linkbutton\" href=\"' + appRouter.getRouteUrl({\n                Name: p.Name,\n                Type: type,\n                ServerId: item.ServerId,\n                Id: p.Id\n            }, {\n                context: context\n            }) + '\">' + p.Name + \"</a>\";\n        }).join(\", \");\n        var elem = page.querySelector(\".genres\");\n        elem.innerHTML = globalize.translate(genres.length > 1 ? \"GenresValue\" : \"GenreValue\", html);\n\n        if (genres.length) {\n            elem.classList.remove(\"hide\");\n        } else {\n            elem.classList.add(\"hide\");\n        }\n    }\n\n    function renderDirector(page, item, apiClient, context, isStatic) {\n        var directors = (item.People || []).filter(function (p) {\n            return \"Director\" === p.Type;\n        });\n        var html = directors.map(function (p) {\n            return '<a style=\"color:inherit;\" class=\"button-link\" is=\"emby-linkbutton\" href=\"' + appRouter.getRouteUrl({\n                Name: p.Name,\n                Type: \"Person\",\n                ServerId: item.ServerId,\n                Id: p.Id\n            }, {\n                context: context\n            }) + '\">' + p.Name + \"</a>\";\n        }).join(\", \");\n        var elem = page.querySelector(\".directors\");\n        elem.innerHTML = globalize.translate(directors.length > 1 ? \"DirectorsValue\" : \"DirectorValue\", html);\n\n        if (directors.length) {\n            elem.classList.remove(\"hide\");\n        } else {\n            elem.classList.add(\"hide\");\n        }\n    }\n\n    function renderDetails(page, item, apiClient, context, isStatic) {\n        renderSimilarItems(page, item, context);\n        renderMoreFromSeason(page, item, apiClient);\n        renderMoreFromArtist(page, item, apiClient);\n        renderDirector(page, item, apiClient, context, isStatic);\n        renderGenres(page, item, apiClient, context, isStatic);\n        renderChannelGuide(page, apiClient, item);\n        var taglineElement = page.querySelector(\".tagline\");\n\n        if (item.Taglines && item.Taglines.length) {\n            taglineElement.classList.remove(\"hide\");\n            taglineElement.innerHTML = item.Taglines[0];\n        } else {\n            taglineElement.classList.add(\"hide\");\n        }\n\n        var overview = page.querySelector(\".overview\");\n        var externalLinksElem = page.querySelector(\".itemExternalLinks\");\n\n        if (\"Season\" !== item.Type && \"MusicAlbum\" !== item.Type && \"MusicArtist\" !== item.Type) {\n            overview.classList.add(\"detailsHiddenOnMobile\");\n            externalLinksElem.classList.add(\"detailsHiddenOnMobile\");\n        }\n\n        renderOverview([overview], item);\n        var i;\n        var itemMiscInfo;\n        itemMiscInfo = page.querySelectorAll(\".itemMiscInfo-primary\");\n\n        for (i = 0; i < itemMiscInfo.length; i++) {\n            mediaInfo.fillPrimaryMediaInfo(itemMiscInfo[i], item, {\n                interactive: true,\n                episodeTitle: false,\n                subtitles: false\n            });\n\n            if (itemMiscInfo[i].innerHTML && \"SeriesTimer\" !== item.Type) {\n                itemMiscInfo[i].classList.remove(\"hide\");\n            } else {\n                itemMiscInfo[i].classList.add(\"hide\");\n            }\n        }\n\n        itemMiscInfo = page.querySelectorAll(\".itemMiscInfo-secondary\");\n\n        for (i = 0; i < itemMiscInfo.length; i++) {\n            mediaInfo.fillSecondaryMediaInfo(itemMiscInfo[i], item, {\n                interactive: true\n            });\n\n            if (itemMiscInfo[i].innerHTML && \"SeriesTimer\" !== item.Type) {\n                itemMiscInfo[i].classList.remove(\"hide\");\n            } else {\n                itemMiscInfo[i].classList.add(\"hide\");\n            }\n        }\n\n        reloadUserDataButtons(page, item);\n        renderLinks(externalLinksElem, item);\n        renderUserInfo(page, item);\n        renderTags(page, item);\n        renderSeriesAirTime(page, item, isStatic);\n    }\n\n    function enableScrollX() {\n        return browser.mobile && screen.availWidth <= 1000;\n    }\n\n    function getPortraitShape(scrollX) {\n        if (null == scrollX) {\n            scrollX = enableScrollX();\n        }\n\n        return scrollX ? \"overflowPortrait\" : \"portrait\";\n    }\n\n    function getSquareShape(scrollX) {\n        if (null == scrollX) {\n            scrollX = enableScrollX();\n        }\n\n        return scrollX ? \"overflowSquare\" : \"square\";\n    }\n\n    function getThumbShape(scrollX) {\n        if (null == scrollX) {\n            scrollX = enableScrollX();\n        }\n\n        return scrollX ? \"overflowBackdrop\" : \"backdrop\";\n    }\n\n    function renderMoreFromSeason(view, item, apiClient) {\n        var section = view.querySelector(\".moreFromSeasonSection\");\n\n        if (section) {\n            if (\"Episode\" !== item.Type || !item.SeasonId || !item.SeriesId) {\n                return void section.classList.add(\"hide\");\n            }\n\n            var userId = apiClient.getCurrentUserId();\n            apiClient.getEpisodes(item.SeriesId, {\n                SeasonId: item.SeasonId,\n                UserId: userId,\n                Fields: \"ItemCounts,PrimaryImageAspectRatio,BasicSyncInfo,CanDelete,MediaSourceCount\"\n            }).then(function (result) {\n                if (result.Items.length < 2) {\n                    return void section.classList.add(\"hide\");\n                }\n\n                section.classList.remove(\"hide\");\n                section.querySelector(\"h2\").innerHTML = globalize.translate(\"MoreFromValue\", item.SeasonName);\n                var itemsContainer = section.querySelector(\".itemsContainer\");\n                cardBuilder.buildCards(result.Items, {\n                    parentContainer: section,\n                    itemsContainer: itemsContainer,\n                    shape: \"autooverflow\",\n                    sectionTitleTagName: \"h2\",\n                    scalable: true,\n                    showTitle: true,\n                    overlayText: false,\n                    centerText: true,\n                    includeParentInfoInTitle: false,\n                    allowBottomPadding: false\n                });\n                var card = itemsContainer.querySelector('.card[data-id=\"' + item.Id + '\"]');\n\n                if (card) {\n                    setTimeout(function () {\n                        section.querySelector(\".emby-scroller\").toStart(card.previousSibling || card, true);\n                    }, 100);\n                }\n            });\n        }\n    }\n\n    function renderMoreFromArtist(view, item, apiClient) {\n        var section = view.querySelector(\".moreFromArtistSection\");\n\n        if (section) {\n            if (\"MusicArtist\" === item.Type) {\n                if (!apiClient.isMinServerVersion(\"3.4.1.19\")) {\n                    return void section.classList.add(\"hide\");\n                }\n            } else if (\"MusicAlbum\" !== item.Type || !item.AlbumArtists || !item.AlbumArtists.length) {\n                return void section.classList.add(\"hide\");\n            }\n\n            var query = {\n                IncludeItemTypes: \"MusicAlbum\",\n                Recursive: true,\n                ExcludeItemIds: item.Id,\n                SortBy: \"ProductionYear,SortName\",\n                SortOrder: \"Descending\"\n            };\n\n            if (\"MusicArtist\" === item.Type) {\n                query.ContributingArtistIds = item.Id;\n            } else if (apiClient.isMinServerVersion(\"3.4.1.18\")) {\n                query.AlbumArtistIds = item.AlbumArtists[0].Id;\n            } else {\n                query.ArtistIds = item.AlbumArtists[0].Id;\n            }\n\n            apiClient.getItems(apiClient.getCurrentUserId(), query).then(function (result) {\n                if (!result.Items.length) {\n                    return void section.classList.add(\"hide\");\n                }\n\n                section.classList.remove(\"hide\");\n\n                if (\"MusicArtist\" === item.Type) {\n                    section.querySelector(\"h2\").innerHTML = globalize.translate(\"HeaderAppearsOn\");\n                } else {\n                    section.querySelector(\"h2\").innerHTML = globalize.translate(\"MoreFromValue\", item.AlbumArtists[0].Name);\n                }\n\n                cardBuilder.buildCards(result.Items, {\n                    parentContainer: section,\n                    itemsContainer: section.querySelector(\".itemsContainer\"),\n                    shape: \"autooverflow\",\n                    sectionTitleTagName: \"h2\",\n                    scalable: true,\n                    coverImage: \"MusicArtist\" === item.Type || \"MusicAlbum\" === item.Type,\n                    showTitle: true,\n                    showParentTitle: false,\n                    centerText: true,\n                    overlayText: false,\n                    overlayPlayButton: true,\n                    showYear: true\n                });\n            });\n        }\n    }\n\n    function renderSimilarItems(page, item, context) {\n        var similarCollapsible = page.querySelector(\"#similarCollapsible\");\n\n        if (similarCollapsible) {\n            if (\"Movie\" != item.Type && \"Trailer\" != item.Type && \"Series\" != item.Type && \"Program\" != item.Type && \"Recording\" != item.Type && \"MusicAlbum\" != item.Type && \"MusicArtist\" != item.Type && \"Playlist\" != item.Type) {\n                return void similarCollapsible.classList.add(\"hide\");\n            }\n\n            similarCollapsible.classList.remove(\"hide\");\n            var apiClient = connectionManager.getApiClient(item.ServerId);\n            var options = {\n                userId: apiClient.getCurrentUserId(),\n                limit: 12,\n                fields: \"PrimaryImageAspectRatio,UserData,CanDelete\"\n            };\n\n            if (\"MusicAlbum\" == item.Type && item.AlbumArtists && item.AlbumArtists.length) {\n                options.ExcludeArtistIds = item.AlbumArtists[0].Id;\n            }\n\n            apiClient.getSimilarItems(item.Id, options).then(function (result) {\n                if (!result.Items.length) {\n                    return void similarCollapsible.classList.add(\"hide\");\n                }\n\n                similarCollapsible.classList.remove(\"hide\");\n                var html = \"\";\n                html += cardBuilder.getCardsHtml({\n                    items: result.Items,\n                    shape: \"autooverflow\",\n                    showParentTitle: \"MusicAlbum\" == item.Type,\n                    centerText: true,\n                    showTitle: true,\n                    context: context,\n                    lazy: true,\n                    showDetailsMenu: true,\n                    coverImage: \"MusicAlbum\" == item.Type || \"MusicArtist\" == item.Type,\n                    overlayPlayButton: true,\n                    overlayText: false,\n                    showYear: \"Movie\" === item.Type || \"Trailer\" === item.Type\n                });\n                var similarContent = similarCollapsible.querySelector(\".similarContent\");\n                similarContent.innerHTML = html;\n                imageLoader.lazyChildren(similarContent);\n            });\n        }\n    }\n\n    function renderSeriesAirTime(page, item, isStatic) {\n        var seriesAirTime = page.querySelector(\"#seriesAirTime\");\n        if (\"Series\" != item.Type) {\n            seriesAirTime.classList.add(\"hide\");\n            return;\n        }\n        var html = \"\";\n        if (item.AirDays && item.AirDays.length) {\n            if (7 == item.AirDays.length) {\n                html += \"daily\";\n            } else {\n                html += item.AirDays.map(function (a) {\n                    return a + \"s\";\n                }).join(\",\");\n            }\n        }\n        if (item.AirTime) {\n            html += \" at \" + item.AirTime;\n        }\n        if (item.Studios.length) {\n            if (isStatic) {\n                html += \" on \" + item.Studios[0].Name;\n            } else {\n                var context = inferContext(item);\n                var href = appRouter.getRouteUrl(item.Studios[0], {\n                    context: context,\n                    itemType: \"Studio\",\n                    serverId: item.ServerId\n                });\n                html += ' on <a class=\"textlink button-link\" is=\"emby-linkbutton\" href=\"' + href + '\">' + item.Studios[0].Name + \"</a>\";\n            }\n        }\n        if (html) {\n            html = (\"Ended\" == item.Status ? \"Aired \" : \"Airs \") + html;\n            seriesAirTime.innerHTML = html;\n            seriesAirTime.classList.remove(\"hide\");\n        } else {\n            seriesAirTime.classList.add(\"hide\");\n        }\n    }\n\n    function renderTags(page, item) {\n        var itemTags = page.querySelector(\".itemTags\");\n        var tagElements = [];\n        var tags = item.Tags || [];\n\n        if (\"Program\" === item.Type) {\n            tags = [];\n        }\n\n        for (var i = 0, length = tags.length; i < length; i++) {\n            tagElements.push(tags[i]);\n        }\n\n        if (tagElements.length) {\n            itemTags.innerHTML = globalize.translate(\"TagsValue\", tagElements.join(\", \"));\n            itemTags.classList.remove(\"hide\");\n        } else {\n            itemTags.innerHTML = \"\";\n            itemTags.classList.add(\"hide\");\n        }\n    }\n\n    function renderChildren(page, item) {\n        var fields = \"ItemCounts,PrimaryImageAspectRatio,BasicSyncInfo,CanDelete,MediaSourceCount\";\n        var query = {\n            ParentId: item.Id,\n            Fields: fields\n        };\n\n        if (\"BoxSet\" !== item.Type) {\n            query.SortBy = \"SortName\";\n        }\n\n        var promise;\n        var apiClient = connectionManager.getApiClient(item.ServerId);\n        var userId = apiClient.getCurrentUserId();\n\n        if (\"Series\" == item.Type) {\n            promise = apiClient.getSeasons(item.Id, {\n                userId: userId,\n                Fields: fields\n            });\n        } else if (\"Season\" == item.Type) {\n            fields += \",Overview\";\n            promise = apiClient.getEpisodes(item.SeriesId, {\n                seasonId: item.Id,\n                userId: userId,\n                Fields: fields\n            });\n        } else if (\"MusicArtist\" == item.Type) {\n            query.SortBy = \"ProductionYear,SortName\";\n        }\n\n        promise = promise || apiClient.getItems(apiClient.getCurrentUserId(), query);\n        promise.then(function (result) {\n            var html = \"\";\n            var scrollX = false;\n            var isList = false;\n            var childrenItemsContainer = page.querySelector(\".childrenItemsContainer\");\n\n            if (\"MusicAlbum\" == item.Type) {\n                html = listView.getListViewHtml({\n                    items: result.Items,\n                    smallIcon: true,\n                    showIndex: true,\n                    index: \"disc\",\n                    showIndexNumberLeft: true,\n                    playFromHere: true,\n                    action: \"playallfromhere\",\n                    image: false,\n                    artist: \"auto\",\n                    containerAlbumArtists: item.AlbumArtists,\n                    addToListButton: true\n                });\n                isList = true;\n            } else if (\"Series\" == item.Type) {\n                scrollX = enableScrollX();\n                html = cardBuilder.getCardsHtml({\n                    items: result.Items,\n                    shape: getPortraitShape(),\n                    showTitle: true,\n                    centerText: true,\n                    lazy: true,\n                    overlayPlayButton: true,\n                    allowBottomPadding: !scrollX\n                });\n            } else if (\"Season\" == item.Type || \"Episode\" == item.Type) {\n                if (\"Episode\" !== item.Type) {\n                    isList = true;\n                }\n                scrollX = \"Episode\" == item.Type;\n                if (result.Items.length < 2 && \"Episode\" === item.Type) {\n                    return;\n                }\n\n                if (\"Episode\" === item.Type) {\n                    html = cardBuilder.getCardsHtml({\n                        items: result.Items,\n                        shape: getThumbShape(scrollX),\n                        showTitle: true,\n                        displayAsSpecial: \"Season\" == item.Type && item.IndexNumber,\n                        playFromHere: true,\n                        overlayText: true,\n                        lazy: true,\n                        showDetailsMenu: true,\n                        overlayPlayButton: true,\n                        allowBottomPadding: !scrollX,\n                        includeParentInfoInTitle: false\n                    });\n                } else if (\"Season\" === item.Type) {\n                    html = listView.getListViewHtml({\n                        items: result.Items,\n                        showIndexNumber: false,\n                        enableOverview: true,\n                        imageSize: \"large\",\n                        enableSideMediaInfo: false,\n                        highlight: false,\n                        action: layoutManager.tv ? \"resume\" : \"none\",\n                        infoButton: true,\n                        imagePlayButton: true,\n                        includeParentInfoInTitle: false\n                    });\n                }\n            }\n\n            if (\"BoxSet\" !== item.Type) {\n                page.querySelector(\"#childrenCollapsible\").classList.remove(\"hide\");\n            }\n            if (scrollX) {\n                childrenItemsContainer.classList.add(\"scrollX\");\n                childrenItemsContainer.classList.add(\"hiddenScrollX\");\n                childrenItemsContainer.classList.remove(\"vertical-wrap\");\n                childrenItemsContainer.classList.remove(\"vertical-list\");\n            } else {\n                childrenItemsContainer.classList.remove(\"scrollX\");\n                childrenItemsContainer.classList.remove(\"hiddenScrollX\");\n                childrenItemsContainer.classList.remove(\"smoothScrollX\");\n                if (isList) {\n                    childrenItemsContainer.classList.add(\"vertical-list\");\n                    childrenItemsContainer.classList.remove(\"vertical-wrap\");\n                } else {\n                    childrenItemsContainer.classList.add(\"vertical-wrap\");\n                    childrenItemsContainer.classList.remove(\"vertical-list\");\n                }\n            }\n            childrenItemsContainer.innerHTML = html;\n            imageLoader.lazyChildren(childrenItemsContainer);\n            if (\"BoxSet\" == item.Type) {\n                var collectionItemTypes = [{\n                    name: globalize.translate(\"HeaderVideos\"),\n                    mediaType: \"Video\"\n                }, {\n                    name: globalize.translate(\"HeaderSeries\"),\n                    type: \"Series\"\n                }, {\n                    name: globalize.translate(\"HeaderAlbums\"),\n                    type: \"MusicAlbum\"\n                }, {\n                    name: globalize.translate(\"HeaderBooks\"),\n                    type: \"Book\"\n                }];\n                renderCollectionItems(page, item, collectionItemTypes, result.Items);\n            }\n        });\n\n        if (\"Season\" == item.Type) {\n            page.querySelector(\"#childrenTitle\").innerHTML = globalize.translate(\"HeaderEpisodes\");\n        } else if (\"Series\" == item.Type) {\n            page.querySelector(\"#childrenTitle\").innerHTML = globalize.translate(\"HeaderSeasons\");\n        } else if (\"MusicAlbum\" == item.Type) {\n            page.querySelector(\"#childrenTitle\").innerHTML = globalize.translate(\"HeaderTracks\");\n        } else {\n            page.querySelector(\"#childrenTitle\").innerHTML = globalize.translate(\"HeaderItems\");\n        }\n\n        if (\"MusicAlbum\" == item.Type || \"Season\" == item.Type) {\n            page.querySelector(\".childrenSectionHeader\").classList.add(\"hide\");\n            page.querySelector(\"#childrenCollapsible\").classList.add(\"verticalSection-extrabottompadding\");\n        } else {\n            page.querySelector(\".childrenSectionHeader\").classList.remove(\"hide\");\n        }\n    }\n\n    function renderItemsByName(page, item, user) {\n        require(\"scripts/itembynamedetailpage\".split(\",\"), function () {\n            window.ItemsByName.renderItems(page, item);\n        });\n    }\n\n    function renderPlaylistItems(page, item, user) {\n        require(\"scripts/playlistedit\".split(\",\"), function () {\n            PlaylistViewer.render(page, item);\n        });\n    }\n\n    function renderProgramsForChannel(page, result) {\n        var html = \"\";\n        var currentItems = [];\n        var currentStartDate = null;\n\n        for (var i = 0, length = result.Items.length; i < length; i++) {\n            var item = result.Items[i];\n            var itemStartDate = datetime.parseISO8601Date(item.StartDate);\n\n            if (!(currentStartDate && currentStartDate.toDateString() === itemStartDate.toDateString())) {\n                if (currentItems.length) {\n                    html += '<div class=\"verticalSection verticalDetailSection\">';\n                    html += '<h2 class=\"sectionTitle padded-left\">' + datetime.toLocaleDateString(currentStartDate, {\n                        weekday: \"long\",\n                        month: \"long\",\n                        day: \"numeric\"\n                    }) + \"</h2>\";\n                    html += '<div is=\"emby-itemscontainer\" class=\"vertical-list padded-left padded-right\">' + listView.getListViewHtml({\n                        items: currentItems,\n                        enableUserDataButtons: false,\n                        showParentTitle: true,\n                        image: false,\n                        showProgramTime: true,\n                        mediaInfo: false,\n                        parentTitleWithTitle: true\n                    }) + \"</div></div>\";\n                }\n\n                currentStartDate = itemStartDate;\n                currentItems = [];\n            }\n\n            currentItems.push(item);\n        }\n\n        if (currentItems.length) {\n            html += '<div class=\"verticalSection verticalDetailSection\">';\n            html += '<h2 class=\"sectionTitle padded-left\">' + datetime.toLocaleDateString(currentStartDate, {\n                weekday: \"long\",\n                month: \"long\",\n                day: \"numeric\"\n            }) + \"</h2>\";\n            html += '<div is=\"emby-itemscontainer\" class=\"vertical-list padded-left padded-right\">' + listView.getListViewHtml({\n                items: currentItems,\n                enableUserDataButtons: false,\n                showParentTitle: true,\n                image: false,\n                showProgramTime: true,\n                mediaInfo: false,\n                parentTitleWithTitle: true\n            }) + \"</div></div>\";\n        }\n\n        page.querySelector(\".programGuide\").innerHTML = html;\n    }\n\n    function renderChannelGuide(page, apiClient, item) {\n        if (\"TvChannel\" === item.Type) {\n            page.querySelector(\".programGuideSection\").classList.remove(\"hide\");\n            apiClient.getLiveTvPrograms({\n                ChannelIds: item.Id,\n                UserId: apiClient.getCurrentUserId(),\n                HasAired: false,\n                SortBy: \"StartDate\",\n                EnableTotalRecordCount: false,\n                EnableImages: false,\n                ImageTypeLimit: 0,\n                EnableUserData: false\n            }).then(function (result) {\n                renderProgramsForChannel(page, result);\n            });\n        }\n    }\n\n    function renderSeriesSchedule(page, item, user) {\n        var apiClient = connectionManager.getApiClient(item.ServerId);\n        apiClient.getLiveTvPrograms({\n            UserId: apiClient.getCurrentUserId(),\n            HasAired: false,\n            SortBy: \"StartDate\",\n            EnableTotalRecordCount: false,\n            EnableImages: false,\n            ImageTypeLimit: 0,\n            Limit: 50,\n            EnableUserData: false,\n            LibrarySeriesId: item.Id\n        }).then(function (result) {\n            if (result.Items.length) {\n                page.querySelector(\"#seriesScheduleSection\").classList.remove(\"hide\");\n            } else {\n                page.querySelector(\"#seriesScheduleSection\").classList.add(\"hide\");\n            }\n\n            page.querySelector(\"#seriesScheduleList\").innerHTML = listView.getListViewHtml({\n                items: result.Items,\n                enableUserDataButtons: false,\n                showParentTitle: false,\n                image: false,\n                showProgramDateTime: true,\n                mediaInfo: false,\n                showTitle: true,\n                moreButton: false,\n                action: \"programdialog\"\n            });\n            loading.hide();\n        });\n    }\n\n    function inferContext(item) {\n        if (\"Movie\" === item.Type || \"BoxSet\" === item.Type) {\n            return \"movies\";\n        }\n\n        if (\"Series\" === item.Type || \"Season\" === item.Type || \"Episode\" === item.Type) {\n            return \"tvshows\";\n        }\n\n        if (\"MusicArtist\" === item.Type || \"MusicAlbum\" === item.Type || \"Audio\" === item.Type || \"AudioBook\" === item.Type) {\n            return \"music\";\n        }\n\n        if (\"Program\" === item.Type) {\n            return \"livetv\";\n        }\n\n        return null;\n    }\n\n    function filterItemsByCollectionItemType(items, typeInfo) {\n        return items.filter(function (item) {\n            if (typeInfo.mediaType) {\n                return item.MediaType == typeInfo.mediaType;\n            }\n\n            return item.Type == typeInfo.type;\n        });\n    }\n\n    function canPlaySomeItemInCollection(items) {\n        var i = 0;\n\n        for (length = items.length; i < length; i++) {\n            if (playbackManager.canPlay(items[i])) {\n                return true;\n            }\n        }\n\n        return false;\n    }\n\n    function renderCollectionItems(page, parentItem, types, items) {\n        page.querySelector(\".collectionItems\").innerHTML = \"\";\n        var i;\n        var length;\n\n        for (i = 0, length = types.length; i < length; i++) {\n            var type = types[i];\n            var typeItems = filterItemsByCollectionItemType(items, type);\n\n            if (typeItems.length) {\n                renderCollectionItemType(page, parentItem, type, typeItems);\n            }\n        }\n\n        var otherType = {\n            name: globalize.translate(\"HeaderOtherItems\")\n        };\n        var otherTypeItems = items.filter(function (curr) {\n            return !types.filter(function (t) {\n                return filterItemsByCollectionItemType([curr], t).length > 0;\n            }).length;\n        });\n\n        if (otherTypeItems.length) {\n            renderCollectionItemType(page, parentItem, otherType, otherTypeItems);\n        }\n\n        if (!items.length) {\n            renderCollectionItemType(page, parentItem, {\n                name: globalize.translate(\"HeaderItems\")\n            }, items);\n        }\n\n        var containers = page.querySelectorAll(\".collectionItemsContainer\");\n\n        var notifyRefreshNeeded = function () {\n            renderChildren(page, parentItem);\n        };\n\n        for (i = 0, length = containers.length; i < length; i++) {\n            containers[i].notifyRefreshNeeded = notifyRefreshNeeded;\n        }\n\n        // if nothing in the collection can be played hide play and shuffle buttons\n        if (!canPlaySomeItemInCollection(items)) {\n            hideAll(page, \"btnPlay\", false);\n            hideAll(page, \"btnShuffle\", false);\n        }\n    }\n\n    function renderCollectionItemType(page, parentItem, type, items) {\n        var html = \"\";\n        html += '<div class=\"verticalSection\">';\n        html += '<div class=\"sectionTitleContainer sectionTitleContainer-cards padded-left\">';\n        html += '<h2 class=\"sectionTitle sectionTitle-cards\">';\n        html += \"<span>\" + type.name + \"</span>\";\n        html += \"</h2>\";\n        html += '<button class=\"btnAddToCollection sectionTitleButton\" type=\"button\" is=\"paper-icon-button-light\" style=\"margin-left:1em;\"><i class=\"md-icon\" icon=\"add\">&#xE145;</i></button>';\n        html += \"</div>\";\n        html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer collectionItemsContainer vertical-wrap padded-left padded-right\">';\n        var shape = \"MusicAlbum\" == type.type ? getSquareShape(false) : getPortraitShape(false);\n        html += cardBuilder.getCardsHtml({\n            items: items,\n            shape: shape,\n            showTitle: true,\n            centerText: true,\n            lazy: true,\n            showDetailsMenu: true,\n            overlayMoreButton: true,\n            showAddToCollection: false,\n            showRemoveFromCollection: true,\n            collectionId: parentItem.Id\n        });\n        html += \"</div>\";\n        html += \"</div>\";\n        var collectionItems = page.querySelector(\".collectionItems\");\n        collectionItems.insertAdjacentHTML(\"beforeend\", html);\n        imageLoader.lazyChildren(collectionItems);\n        collectionItems.querySelector(\".btnAddToCollection\").addEventListener(\"click\", function () {\n            require([\"alert\"], function (alert) {\n                alert({\n                    text: globalize.translate(\"AddItemToCollectionHelp\"),\n                    html: globalize.translate(\"AddItemToCollectionHelp\") + '<br/><br/><a is=\"emby-linkbutton\" class=\"button-link\" target=\"_blank\" href=\"https://web.archive.org/web/20181216120305/https://github.com/MediaBrowser/Wiki/wiki/Collections\">' + globalize.translate(\"ButtonLearnMore\") + \"</a>\"\n                });\n            });\n        });\n    }\n\n    function renderMusicVideos(page, item, user) {\n        connectionManager.getApiClient(item.ServerId).getItems(user.Id, {\n            SortBy: \"SortName\",\n            SortOrder: \"Ascending\",\n            IncludeItemTypes: \"MusicVideo\",\n            Recursive: true,\n            Fields: \"PrimaryImageAspectRatio,BasicSyncInfo,CanDelete,MediaSourceCount\",\n            AlbumIds: item.Id\n        }).then(function (result) {\n            if (result.Items.length) {\n                page.querySelector(\"#musicVideosCollapsible\").classList.remove(\"hide\");\n                var musicVideosContent = page.querySelector(\".musicVideosContent\");\n                musicVideosContent.innerHTML = getVideosHtml(result.Items, user);\n                imageLoader.lazyChildren(musicVideosContent);\n            } else {\n                page.querySelector(\"#musicVideosCollapsible\").classList.add(\"hide\");\n            }\n        });\n    }\n\n    function renderAdditionalParts(page, item, user) {\n        connectionManager.getApiClient(item.ServerId).getAdditionalVideoParts(user.Id, item.Id).then(function (result) {\n            if (result.Items.length) {\n                page.querySelector(\"#additionalPartsCollapsible\").classList.remove(\"hide\");\n                var additionalPartsContent = page.querySelector(\"#additionalPartsContent\");\n                additionalPartsContent.innerHTML = getVideosHtml(result.Items, user);\n                imageLoader.lazyChildren(additionalPartsContent);\n            } else {\n                page.querySelector(\"#additionalPartsCollapsible\").classList.add(\"hide\");\n            }\n        });\n    }\n\n    function renderScenes(page, item) {\n        var chapters = item.Chapters || [];\n\n        if (chapters.length && !chapters[0].ImageTag && (chapters = []), chapters.length) {\n            page.querySelector(\"#scenesCollapsible\").classList.remove(\"hide\");\n            var scenesContent = page.querySelector(\"#scenesContent\");\n\n            require([\"chaptercardbuilder\"], function (chaptercardbuilder) {\n                chaptercardbuilder.buildChapterCards(item, chapters, {\n                    itemsContainer: scenesContent,\n                    width: 400,\n                    backdropShape: \"overflowBackdrop\",\n                    squareShape: \"overflowSquare\"\n                });\n            });\n        } else {\n            page.querySelector(\"#scenesCollapsible\").classList.add(\"hide\");\n        }\n    }\n\n    function getVideosHtml(items, user, limit, moreButtonClass) {\n        var html = cardBuilder.getCardsHtml({\n            items: items,\n            shape: \"auto\",\n            showTitle: true,\n            action: \"play\",\n            overlayText: false,\n            centerText: true,\n            showRuntime: true\n        });\n\n        if (limit && items.length > limit) {\n            html += '<p style=\"margin: 0;padding-left:5px;\"><button is=\"emby-button\" type=\"button\" class=\"raised more ' + moreButtonClass + '\">' + globalize.translate(\"ButtonMore\") + \"</button></p>\";\n        }\n\n        return html;\n    }\n\n    function renderSpecials(page, item, user, limit) {\n        connectionManager.getApiClient(item.ServerId).getSpecialFeatures(user.Id, item.Id).then(function (specials) {\n            var specialsContent = page.querySelector(\"#specialsContent\");\n            specialsContent.innerHTML = getVideosHtml(specials, user, limit, \"moreSpecials\");\n            imageLoader.lazyChildren(specialsContent);\n        });\n    }\n\n    function renderCast(page, item, context, limit, isStatic) {\n        var people = (item.People || []).filter(function (p) {\n            return \"Director\" !== p.Type;\n        });\n\n        if (!people.length) {\n            return void page.querySelector(\"#castCollapsible\").classList.add(\"hide\");\n        }\n\n        page.querySelector(\"#castCollapsible\").classList.remove(\"hide\");\n        var castContent = page.querySelector(\"#castContent\");\n\n        require([\"peoplecardbuilder\"], function (peoplecardbuilder) {\n            peoplecardbuilder.buildPeopleCards(people, {\n                itemsContainer: castContent,\n                coverImage: true,\n                serverId: item.ServerId,\n                width: 160,\n                shape: getPortraitShape()\n            });\n        });\n    }\n\n    function itemDetailPage() {\n        var self = this;\n        self.setInitialCollapsibleState = setInitialCollapsibleState;\n        self.renderDetails = renderDetails;\n        self.renderCast = renderCast;\n    }\n\n    function bindAll(view, selector, eventName, fn) {\n        var i;\n        var length;\n        var elems = view.querySelectorAll(selector);\n\n        for (i = 0, length = elems.length; i < length; i++) {\n            elems[i].addEventListener(eventName, fn);\n        }\n    }\n\n    function onTrackSelectionsSubmit(e) {\n        e.preventDefault();\n        return false;\n    }\n\n    window.ItemDetailPage = new itemDetailPage();\n    return function (view, params) {\n        function reload(instance, page, params) {\n            loading.show();\n            var apiClient = params.serverId ? connectionManager.getApiClient(params.serverId) : ApiClient;\n            var promises = [getPromise(apiClient, params), apiClient.getCurrentUser()];\n            Promise.all(promises).then(function (responses) {\n                var item = responses[0];\n                var user = responses[1];\n                currentItem = item;\n                reloadFromItem(instance, page, params, item, user);\n            });\n        }\n\n        function splitVersions(instance, page, apiClient, params) {\n            require([\"confirm\"], function (confirm) {\n                confirm(\"Are you sure you wish to split the media sources into separate items?\", \"Split Media Apart\").then(function () {\n                    loading.show();\n                    apiClient.ajax({\n                        type: \"DELETE\",\n                        url: apiClient.getUrl(\"Videos/\" + params.id + \"/AlternateSources\")\n                    }).then(function () {\n                        loading.hide();\n                        reload(instance, page, params);\n                    });\n                });\n            });\n        }\n\n        function getPlayOptions(startPosition) {\n            var audioStreamIndex = view.querySelector(\".selectAudio\").value || null;\n            return {\n                startPositionTicks: startPosition,\n                mediaSourceId: view.querySelector(\".selectSource\").value,\n                audioStreamIndex: audioStreamIndex,\n                subtitleStreamIndex: view.querySelector(\".selectSubtitles\").value\n            };\n        }\n\n        function playItem(item, startPosition) {\n            var playOptions = getPlayOptions(startPosition);\n            playOptions.items = [item];\n            playbackManager.play(playOptions);\n        }\n\n        function playTrailer(page) {\n            playbackManager.playTrailers(currentItem);\n        }\n\n        function playCurrentItem(button, mode) {\n            var item = currentItem;\n\n            if (\"Program\" === item.Type) {\n                var apiClient = connectionManager.getApiClient(item.ServerId);\n                return void apiClient.getLiveTvChannel(item.ChannelId, apiClient.getCurrentUserId()).then(function (channel) {\n                    playbackManager.play({\n                        items: [channel]\n                    });\n                });\n            }\n\n            playItem(item, item.UserData && \"resume\" === mode ? item.UserData.PlaybackPositionTicks : 0);\n        }\n\n        function onPlayClick() {\n            playCurrentItem(this, this.getAttribute(\"data-mode\"));\n        }\n\n        function onInstantMixClick() {\n            playbackManager.instantMix(currentItem);\n        }\n\n        function onShuffleClick() {\n            playbackManager.shuffle(currentItem);\n        }\n\n        function onDeleteClick() {\n            require([\"deleteHelper\"], function (deleteHelper) {\n                deleteHelper.deleteItem({\n                    item: currentItem,\n                    navigate: true\n                });\n            });\n        }\n\n        function onCancelSeriesTimerClick() {\n            require([\"recordingHelper\"], function (recordingHelper) {\n                recordingHelper.cancelSeriesTimerWithConfirmation(currentItem.Id, currentItem.ServerId).then(function () {\n                    Dashboard.navigate(\"livetv.html\");\n                });\n            });\n        }\n\n        function onCancelTimerClick() {\n            require([\"recordingHelper\"], function (recordingHelper) {\n                recordingHelper.cancelTimer(connectionManager.getApiClient(currentItem.ServerId), currentItem.TimerId).then(function () {\n                    reload(self, view, params);\n                });\n            });\n        }\n\n        function onPlayTrailerClick() {\n            playTrailer(view);\n        }\n\n        function onDownloadChange() {\n            reload(self, view, params);\n        }\n\n        function onDownloadClick() {\n            require(['fileDownloader'], function (fileDownloader) {\n                var downloadHref = apiClient.getItemDownloadUrl(currentItem.Id);\n                fileDownloader.download([{\n                    url: downloadHref,\n                    itemId: currentItem.Id,\n                    serverId: currentItem.serverId\n                }]);\n            });\n        }\n\n        function onMoreCommandsClick() {\n            var button = this;\n            apiClient.getCurrentUser().then(function (user) {\n                itemContextMenu.show(getContextMenuOptions(currentItem, user, button)).then(function (result) {\n                    if (result.deleted) {\n                        appRouter.goHome();\n                    } else if (result.updated) {\n                        reload(self, view, params);\n                    }\n                });\n            });\n        }\n\n        function onPlayerChange() {\n            renderTrackSelections(view, self, currentItem);\n            setTrailerButtonVisibility(view, currentItem);\n        }\n\n        function editImages() {\n            return new Promise(function (resolve, reject) {\n                require([\"imageEditor\"], function (imageEditor) {\n                    imageEditor.show({\n                        itemId: currentItem.Id,\n                        serverId: currentItem.ServerId\n                    }).then(resolve, reject);\n                });\n            });\n        }\n\n        function onWebSocketMessage(e, data) {\n            var msg = data;\n\n            if (\"UserDataChanged\" === msg.MessageType && currentItem && msg.Data.UserId == apiClient.getCurrentUserId()) {\n                var key = currentItem.UserData.Key;\n                var userData = msg.Data.UserDataList.filter(function (u) {\n                    return u.Key == key;\n                })[0];\n\n                if (userData) {\n                    currentItem.UserData = userData;\n                    reloadPlayButtons(view, currentItem);\n                    apiClient.getCurrentUser().then(function (user) {\n                        refreshImage(view, currentItem, user);\n                    });\n                }\n            }\n        }\n\n        var currentItem;\n        var self = this;\n        var apiClient = params.serverId ? connectionManager.getApiClient(params.serverId) : ApiClient;\n        view.querySelectorAll(\".btnPlay\");\n        bindAll(view, \".btnPlay\", \"click\", onPlayClick);\n        bindAll(view, \".btnResume\", \"click\", onPlayClick);\n        bindAll(view, \".btnInstantMix\", \"click\", onInstantMixClick);\n        bindAll(view, \".btnShuffle\", \"click\", onShuffleClick);\n        bindAll(view, \".btnPlayTrailer\", \"click\", onPlayTrailerClick);\n        bindAll(view, \".btnCancelSeriesTimer\", \"click\", onCancelSeriesTimerClick);\n        bindAll(view, \".btnCancelTimer\", \"click\", onCancelTimerClick);\n        bindAll(view, \".btnDeleteItem\", \"click\", onDeleteClick);\n        bindAll(view, \".btnDownload\", \"click\", onDownloadClick);\n        view.querySelector(\".btnMoreCommands i\").innerHTML = \"&#xE5D3;\";\n        view.querySelector(\".trackSelections\").addEventListener(\"submit\", onTrackSelectionsSubmit);\n        view.querySelector(\".btnSplitVersions\").addEventListener(\"click\", function () {\n            splitVersions(self, view, apiClient, params);\n        });\n        bindAll(view, \".btnMoreCommands\", \"click\", onMoreCommandsClick);\n        view.querySelector(\".selectSource\").addEventListener(\"change\", function () {\n            renderVideoSelections(view, self._currentPlaybackMediaSources);\n            renderAudioSelections(view, self._currentPlaybackMediaSources);\n            renderSubtitleSelections(view, self._currentPlaybackMediaSources);\n        });\n        view.addEventListener(\"click\", function (e) {\n            if (dom.parentWithClass(e.target, \"moreScenes\")) {\n                apiClient.getCurrentUser().then(function (user) {\n                    renderScenes(view, currentItem);\n                });\n            } else if (dom.parentWithClass(e.target, \"morePeople\")) {\n                renderCast(view, currentItem, params.context);\n            } else if (dom.parentWithClass(e.target, \"moreSpecials\")) {\n                apiClient.getCurrentUser().then(function (user) {\n                    renderSpecials(view, currentItem, user);\n                });\n            }\n        });\n        view.querySelector(\".detailImageContainer\").addEventListener(\"click\", function (e) {\n            if (dom.parentWithClass(e.target, \"itemDetailGalleryLink\")) {\n                editImages().then(function () {\n                    reload(self, view, params);\n                });\n            }\n        });\n        view.addEventListener(\"viewshow\", function (e) {\n            var page = this;\n            libraryMenu.setTransparentMenu(true);\n\n            if (e.detail.isRestored) {\n                if (currentItem) {\n                    setTitle(currentItem, connectionManager.getApiClient(currentItem.ServerId));\n                    renderTrackSelections(page, self, currentItem, true);\n                }\n            } else {\n                reload(self, page, params);\n            }\n\n            events.on(apiClient, \"message\", onWebSocketMessage);\n            events.on(playbackManager, \"playerchange\", onPlayerChange);\n        });\n        view.addEventListener(\"viewbeforehide\", function () {\n            events.off(apiClient, \"message\", onWebSocketMessage);\n            events.off(playbackManager, \"playerchange\", onPlayerChange);\n            libraryMenu.setTransparentMenu(false);\n        });\n        view.addEventListener(\"viewdestroy\", function () {\n            currentItem = null;\n            self._currentPlaybackMediaSources = null;\n            self.currentRecordingFields = null;\n        });\n    };\n});\n", "hunk": "@@ -520,9 +520,7 @@ define([\"loading\", \"appRouter\", \"layoutManager\", \"connectionManager\", \"cardBuild\n         setInitialCollapsibleState(page, item, apiClient, context, user);\n         renderDetails(page, item, apiClient, context);\n         renderTrackSelections(page, instance, item);\n-\n-        backdrop.clear();\n-\n+        renderBackdrop(page, item, apiClient);\n         renderDetailPageBackdrop(page, item, apiClient);\n         var canPlay = reloadPlayButtons(page, item);\n \n", "comment": "Why not just check the backdrop setting here as well for the time being? We can combine the code later but at least then it would be configurable if people like the backgrounds.", "ids": ["12324", "bad156654bcd442551207d34e28f1551111d6098", "7afdc4b2b1dd9f3f5f98342a434ee9a225a76b59"], "repo": "jellyfin/jellyfin-web", "ghid": 594, "old": "         setInitialCollapsibleState(page, item, apiClient, context, user);\n         renderDetails(page, item, apiClient, context);\n         renderTrackSelections(page, instance, item);\n-\n-        backdrop.clear();\n-\n         renderDetailPageBackdrop(page, item, apiClient);\n         var canPlay = reloadPlayButtons(page, item);", "new": "         setInitialCollapsibleState(page, item, apiClient, context, user);\n         renderDetails(page, item, apiClient, context);\n         renderTrackSelections(page, instance, item);\n+        renderBackdrop(page, item, apiClient);\n         renderDetailPageBackdrop(page, item, apiClient);\n         var canPlay = reloadPlayButtons(page, item);", "lang": "js", "norm_lang": "javascript"}
{"old_hunk": "@@ -15,65 +15,41 @@ export function getVideoQualityOptions(options) {\n \n     const qualityOptions = [];\n \n+    const autoQualityOption = {\n+        name: globalize.translate('Auto'),\n+        bitrate: 0,\n+        selected: options.isAutomaticBitrateEnabled\n+    };\n+\n+    if (options.enableAuto) {\n+        qualityOptions.push(autoQualityOption);\n+    }\n+\n+    // Quality options are indexed by bitrate. If you must duplicate them, make sure each of them are unique (by making the last digit a 1)\n     if (maxAllowedWidth >= 3800) {\n         qualityOptions.push({ name: '4K - 120 Mbps', maxHeight: 2160, bitrate: 120000000 });\n-        qualityOptions.push({ name: '4K - 100 Mbps', maxHeight: 2160, bitrate: 100000000 });\n         qualityOptions.push({ name: '4K - 80 Mbps', maxHeight: 2160, bitrate: 80000000 });\n     }\n-\n     // Some 1080- videos are reported as 1912?\n     if (maxAllowedWidth >= 1900) {\n         qualityOptions.push({ name: '1080p - 60 Mbps', maxHeight: 1080, bitrate: 60000000 });\n-        qualityOptions.push({ name: '1080p - 50 Mbps', maxHeight: 1080, bitrate: 50000000 });\n         qualityOptions.push({ name: '1080p - 40 Mbps', maxHeight: 1080, bitrate: 40000000 });\n-        qualityOptions.push({ name: '1080p - 30 Mbps', maxHeight: 1080, bitrate: 30000000 });\n-        qualityOptions.push({ name: '1080p - 25 Mbps', maxHeight: 1080, bitrate: 25000000 });\n         qualityOptions.push({ name: '1080p - 20 Mbps', maxHeight: 1080, bitrate: 20000000 });\n         qualityOptions.push({ name: '1080p - 15 Mbps', maxHeight: 1080, bitrate: 15000000 });\n-        qualityOptions.push({ name: '1080p - 10 Mbps', maxHeight: 1080, bitrate: 10000001 });\n-        qualityOptions.push({ name: '1080p - 8 Mbps', maxHeight: 1080, bitrate: 8000001 });\n-        qualityOptions.push({ name: '1080p - 6 Mbps', maxHeight: 1080, bitrate: 6000001 });\n-        qualityOptions.push({ name: '1080p - 5 Mbps', maxHeight: 1080, bitrate: 5000001 });\n-        qualityOptions.push({ name: '1080p - 4 Mbps', maxHeight: 1080, bitrate: 4000002 });\n-    } else if (maxAllowedWidth >= 1260) {\n-        qualityOptions.push({ name: '720p - 10 Mbps', maxHeight: 720, bitrate: 10000000 });\n+        qualityOptions.push({ name: '1080p - 10 Mbps', maxHeight: 1080, bitrate: 10000000 });\n+    }\n+    if (maxAllowedWidth >= 1260) {\n         qualityOptions.push({ name: '720p - 8 Mbps', maxHeight: 720, bitrate: 8000000 });\n         qualityOptions.push({ name: '720p - 6 Mbps', maxHeight: 720, bitrate: 6000000 });\n-        qualityOptions.push({ name: '720p - 5 Mbps', maxHeight: 720, bitrate: 5000000 });\n-    } else if (maxAllowedWidth >= 620) {\n-        qualityOptions.push({ name: '480p - 4 Mbps', maxHeight: 480, bitrate: 4000001 });\n-        qualityOptions.push({ name: '480p - 3 Mbps', maxHeight: 480, bitrate: 3000001 });\n-        qualityOptions.push({ name: '480p - 2.5 Mbps', maxHeight: 480, bitrate: 2500000 });\n-        qualityOptions.push({ name: '480p - 2 Mbps', maxHeight: 480, bitrate: 2000001 });\n-        qualityOptions.push({ name: '480p - 1.5 Mbps', maxHeight: 480, bitrate: 1500001 });\n+        qualityOptions.push({ name: '720p - 4 Mbps', maxHeight: 720, bitrate: 3000000 });", "oldf": "import globalize from 'globalize';\n\nexport function getVideoQualityOptions(options) {\n    const maxStreamingBitrate = options.currentMaxBitrate;\n    let videoWidth = options.videoWidth;\n    const videoHeight = options.videoHeight;\n\n    // If the aspect ratio is less than 16/9 (1.77), set the width as if it were pillarboxed.\n    // 4:3 1440x1080 -> 1920x1080\n    if (videoWidth / videoHeight < 16 / 9) {\n        videoWidth = videoHeight * (16 / 9);\n    }\n\n    const maxAllowedWidth = videoWidth || 4096;\n\n    const qualityOptions = [];\n\n    const autoQualityOption = {\n        name: globalize.translate('Auto'),\n        bitrate: 0,\n        selected: options.isAutomaticBitrateEnabled\n    };\n\n    if (options.enableAuto) {\n        qualityOptions.push(autoQualityOption);\n    }\n\n    // Quality options are indexed by bitrate. If you must duplicate them, make sure each of them are unique (by making the last digit a 1)\n    if (maxAllowedWidth >= 3800) {\n        qualityOptions.push({ name: '4K - 120 Mbps', maxHeight: 2160, bitrate: 120000000 });\n        qualityOptions.push({ name: '4K - 80 Mbps', maxHeight: 2160, bitrate: 80000000 });\n    }\n    // Some 1080- videos are reported as 1912?\n    if (maxAllowedWidth >= 1900) {\n        qualityOptions.push({ name: '1080p - 60 Mbps', maxHeight: 1080, bitrate: 60000000 });\n        qualityOptions.push({ name: '1080p - 40 Mbps', maxHeight: 1080, bitrate: 40000000 });\n        qualityOptions.push({ name: '1080p - 20 Mbps', maxHeight: 1080, bitrate: 20000000 });\n        qualityOptions.push({ name: '1080p - 15 Mbps', maxHeight: 1080, bitrate: 15000000 });\n        qualityOptions.push({ name: '1080p - 10 Mbps', maxHeight: 1080, bitrate: 10000000 });\n    }\n    if (maxAllowedWidth >= 1260) {\n        qualityOptions.push({ name: '720p - 8 Mbps', maxHeight: 720, bitrate: 8000000 });\n        qualityOptions.push({ name: '720p - 6 Mbps', maxHeight: 720, bitrate: 6000000 });\n        qualityOptions.push({ name: '720p - 4 Mbps', maxHeight: 720, bitrate: 3000000 });\n    }\n    if (maxAllowedWidth >= 620) {\n        qualityOptions.push({ name: '480p - 3 Mbps', maxHeight: 480, bitrate: 3000000 });\n        qualityOptions.push({ name: '480p - 1.5 Mbps', maxHeight: 480, bitrate: 1500000 });\n        qualityOptions.push({ name: '480p - 720 kbps', maxHeight: 480, bitrate: 720000 });\n    }\n\n    qualityOptions.push({ name: '360p - 420 kbps', maxHeight: 360, bitrate: 420000 });\n\n    if (maxStreamingBitrate) {\n        let selectedIndex = -1;\n        for (let i = 0, length = qualityOptions.length; i < length; i++) {\n            const option = qualityOptions[i];\n\n            if (selectedIndex === -1 && option.bitrate <= maxStreamingBitrate) {\n                selectedIndex = i;\n            }\n        }\n\n        if (selectedIndex === -1) {\n            selectedIndex = qualityOptions.length - 1;\n        }\n\n        const currentQualityOption = qualityOptions[selectedIndex];\n\n        if (!options.isAutomaticBitrateEnabled) {\n            currentQualityOption.selected = true;\n        } else {\n            autoQualityOption.autoText = currentQualityOption.name;\n        }\n    }\n\n    return qualityOptions;\n}\n\nexport function getAudioQualityOptions(options) {\n    const maxStreamingBitrate = options.currentMaxBitrate;\n\n    const qualityOptions = [];\n\n    const autoQualityOption = {\n        name: globalize.translate('Auto'),\n        bitrate: 0,\n        selected: options.isAutomaticBitrateEnabled\n    };\n\n    if (options.enableAuto) {\n        qualityOptions.push(autoQualityOption);\n    }\n\n    qualityOptions.push({ name: '2 Mbps', bitrate: 2000000 });\n    qualityOptions.push({ name: '1.5 Mbps', bitrate: 1500000 });\n    qualityOptions.push({ name: '1 Mbps', bitrate: 1000000 });\n    qualityOptions.push({ name: '320 kbps', bitrate: 320000 });\n    qualityOptions.push({ name: '256 kbps', bitrate: 256000 });\n    qualityOptions.push({ name: '192 kbps', bitrate: 192000 });\n    qualityOptions.push({ name: '128 kbps', bitrate: 128000 });\n    qualityOptions.push({ name: '96 kbps', bitrate: 96000 });\n    qualityOptions.push({ name: '64 kbps', bitrate: 64000 });\n\n    if (maxStreamingBitrate) {\n        let selectedIndex = -1;\n        for (let i = 0, length = qualityOptions.length; i < length; i++) {\n            const option = qualityOptions[i];\n\n            if (selectedIndex === -1 && option.bitrate <= maxStreamingBitrate) {\n                selectedIndex = i;\n            }\n        }\n\n        if (selectedIndex === -1) {\n            selectedIndex = qualityOptions.length - 1;\n        }\n\n        const currentQualityOption = qualityOptions[selectedIndex];\n\n        if (!options.isAutomaticBitrateEnabled) {\n            currentQualityOption.selected = true;\n        } else {\n            autoQualityOption.autoText = currentQualityOption.name;\n        }\n    }\n\n    return qualityOptions;\n}\n\nexport default {\n    getVideoQualityOptions,\n    getAudioQualityOptions\n};\n", "hunk": "@@ -41,7 +41,7 @@ export function getVideoQualityOptions(options) {\n     if (maxAllowedWidth >= 1260) {\n         qualityOptions.push({ name: '720p - 8 Mbps', maxHeight: 720, bitrate: 8000000 });\n         qualityOptions.push({ name: '720p - 6 Mbps', maxHeight: 720, bitrate: 6000000 });\n-        qualityOptions.push({ name: '720p - 4 Mbps', maxHeight: 720, bitrate: 3000000 });\n+        qualityOptions.push({ name: '720p - 4 Mbps', maxHeight: 720, bitrate: 4000000 });\n     }\n     if (maxAllowedWidth >= 620) {\n         qualityOptions.push({ name: '480p - 3 Mbps', maxHeight: 480, bitrate: 3000000 });\n", "comment": "```suggestion qualityOptions.push({ name: '720p - 4 Mbps', maxHeight: 720, bitrate: 4000000 }); ```", "ids": ["17763", "4971a90947f2ce8283c196f56f18287cebf3ac97", "78278223892676ae205198f900ddb4da421b0f07"], "repo": "jellyfin/jellyfin-web", "ghid": 1993, "old": "     if (maxAllowedWidth >= 1260) {\n         qualityOptions.push({ name: '720p - 8 Mbps', maxHeight: 720, bitrate: 8000000 });\n         qualityOptions.push({ name: '720p - 6 Mbps', maxHeight: 720, bitrate: 6000000 });\n-        qualityOptions.push({ name: '720p - 4 Mbps', maxHeight: 720, bitrate: 3000000 });\n     }\n     if (maxAllowedWidth >= 620) {\n         qualityOptions.push({ name: '480p - 3 Mbps', maxHeight: 480, bitrate: 3000000 });", "new": "     if (maxAllowedWidth >= 1260) {\n         qualityOptions.push({ name: '720p - 8 Mbps', maxHeight: 720, bitrate: 8000000 });\n         qualityOptions.push({ name: '720p - 6 Mbps', maxHeight: 720, bitrate: 6000000 });\n+        qualityOptions.push({ name: '720p - 4 Mbps', maxHeight: 720, bitrate: 4000000 });\n     }\n     if (maxAllowedWidth >= 620) {\n         qualityOptions.push({ name: '480p - 3 Mbps', maxHeight: 480, bitrate: 3000000 });", "lang": "js", "norm_lang": "javascript"}
{"old_hunk": "@@ -230,7 +277,7 @@ def report(accumulators):\n class _KerasModel(model_lib.Model):\n   \"\"\"Internal wrapper class for tf.keras.Model objects.\"\"\"\n \n-  def __init__(self, inner_model, dummy_batch, loss_fn, metrics):\n+  def __init__(self, inner_model, dummy_batch, loss_fns, loss_weights, metrics):", "oldf": "# Lint as: python3\n# Copyright 2019, The TensorFlow Federated Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utility methods for working with Keras in TensorFlow Federated.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport itertools\nimport logging\n\nimport six\nfrom six.moves import zip\nimport tensorflow as tf\n\nfrom tensorflow_federated.python import core as tff\nfrom tensorflow_federated.python.common_libs import anonymous_tuple\nfrom tensorflow_federated.python.common_libs import py_typecheck\nfrom tensorflow_federated.python.learning import model as model_lib\nfrom tensorflow_federated.python.learning import model_utils\nfrom tensorflow_federated.python.tensorflow_libs import graph_keys\n\n\ndef assign_weights_to_keras_model(keras_model, tff_weights):\n  \"\"\"Assigns a nested structure of TFF weights to a Keras model.\n\n  This function may be used to retrieve the model parameters trained by the\n  federated averaging process for use in an existing `tf.keras.models.Model`,\n  e.g.:\n\n  ```\n  keras_model = tf.keras.models.Model(inputs=..., outputs=...)\n\n  def model_fn():\n    return tff.learning.from_keras_model(keras_model)\n\n  fed_avg = tff.learning.build_federated_averaging_process(model_fn, ...)\n  state = fed_avg.initialize()\n  state = fed_avg.next(state, ...)\n  ...\n  tff.learning.assign_weights_to_keras_model(state.model, keras_model)\n  ```\n\n  Args:\n    keras_model: A `tf.keras.models.Model` instance to assign weights to.\n    tff_weights: A TFF value representing the weights of a model.\n\n  Raises:\n    TypeError: if `tff_weights` is not a TFF value, or `keras_model` is not a\n      `tf.keras.models.Model` instance.\n  \"\"\"\n  # TODO(b/123092620): Simplify this.\n  py_typecheck.check_type(\n      tff_weights, (anonymous_tuple.AnonymousTuple, model_utils.ModelWeights))\n  py_typecheck.check_type(keras_model, tf.keras.models.Model)\n  if isinstance(tff_weights, anonymous_tuple.AnonymousTuple):\n    weights_to_assign = model_utils.ModelWeights.from_tff_value(tff_weights)\n  else:\n    weights_to_assign = tff_weights\n  weights_to_assign.assign_weights_to(keras_model)\n\n\ndef _preprocess_dummy_batch(dummy_batch):\n  dummy_tensors = tf.nest.map_structure(tf.convert_to_tensor_or_sparse_tensor,\n                                        dummy_batch)\n  if py_typecheck.is_named_tuple(dummy_tensors):\n    dummy_tensors = dummy_tensors._asdict()\n  if not isinstance(dummy_tensors, collections.OrderedDict):\n    dummy_tensors = collections.OrderedDict([\n        (k, v) for k, v in six.iteritems(dummy_tensors)\n    ])\n  return dummy_tensors\n\n\ndef from_keras_model(keras_model,\n                     dummy_batch,\n                     loss,\n                     loss_weights=None,\n                     metrics=None,\n                     optimizer=None):\n  \"\"\"Builds a `tff.learning.Model` for an example mini batch.\n\n  Args:\n    keras_model: A `tf.keras.Model` object that is not compiled.\n    dummy_batch: A nested structure of values that are convertible to *batched*\n      tensors with the same shapes and types as would be input to `keras_model`.\n      The values of the tensors are not important and can be filled with any\n      reasonable input value.\n    loss: A callable that takes two batched tensor parameters, `y_true` and\n      `y_pred`, and returns the loss. If the model has multiple outputs, you\n      can use a different loss on each output by passing a dictionary or a list\n      of losses. The loss value that will be minimized by the model will then\n      be the sum of all individual losses.\n    loss_weights: (Optional) a list or dictionary specifying scalar coefficients\n      (Python floats) to weight the loss contributions of different model\n      outputs. The loss value that will be minimized by the model will then\n      be the *weighted sum* of all individual losses, weighted by the\n      `loss_weights` coefficients. If a list, it is expected to have a 1:1\n      mapping to the model's outputs. If a tensor, it is expected to map\n      output names (strings) to scalar coefficients.\n    metrics: (Optional) a list of `tf.keras.metrics.Metric` objects.\n    optimizer: (Optional) a `tf.keras.optimizer.Optimizer`. If None, returned\n      model cannot be used for training.\n\n  Returns:\n    A `tff.learning.Model` object.\n\n  Raises:\n    TypeError: If `keras_model` is not an instance of `tf.keras.Model`.\n    ValueError: If `keras_model` was compiled.\n  \"\"\"\n  py_typecheck.check_type(keras_model, tf.keras.Model)\n  py_typecheck.check_type(loss, (tf.keras.losses.Loss,\n                                 collections.Sequence,\n                                 collections.Mapping))\n\n  if loss_weights is not None:\n    py_typecheck.check_type(loss, (collections.Sequence,\n                                   collections.Mapping))\n\n  if isinstance(loss, collections.Sequence):\n    if len(loss) != len(keras_model.outputs):\n      raise ValueError('`keras_model` must have equal number of '\n                       'outputs and losses')\n\n    if loss_weights is not None and len(loss) != len(loss_weights):\n      raise ValueError('`keras_model` must have equal number of '\n                       'losses and loss weights')\n\n  if keras_model._is_compiled:  # pylint: disable=protected-access\n    raise ValueError('`keras_model` must not be compiled. Use '\n                     'from_compiled_keras_model() instead.')\n\n  dummy_tensors = _preprocess_dummy_batch(dummy_batch)\n  if optimizer is None:\n    if isinstance(loss, collections.Mapping):\n      loss_functions = []\n      for name in keras_model.output_names:\n        if name not in loss:\n          logging.warning(\n              'Output {0} missing from loss dictionary. We assume '\n              'this was done on purpose. The fit and evaluate APIs will not be '\n              'expecting any data to be passed to {0}.'.format(name))\n        loss_functions.append(loss[name])\n\n    elif isinstance(loss, collections.Sequence):\n      loss_functions = loss\n\n    else:\n      loss_functions = [loss]\n\n    return model_utils.enhance(\n        _KerasModel(keras_model, dummy_tensors, loss_functions, loss_weights, metrics))\n\n  keras_model.compile(loss=loss, optimizer=optimizer, loss_weights=loss_weights, metrics=metrics)\n  # NOTE: A sub-classed tf.keras.Model does not produce the compiled metrics\n  # until the model has been called on input. The work-around is to call\n  # Model.test_on_batch() once before asking for metrics.\n  keras_model.test_on_batch(**dummy_tensors)\n  return model_utils.enhance(_TrainableKerasModel(keras_model, dummy_tensors))\n\n\ndef from_compiled_keras_model(keras_model, dummy_batch):\n  \"\"\"Builds a `tff.learning.Model` for an example mini batch.\n\n  Args:\n    keras_model: A `tf.keras.Model` object that was compiled.\n    dummy_batch: A nested structure of values that are convertible to *batched*\n      tensors with the same shapes and types as expected by `forward_pass()`.\n      The values of the tensors are not important and can be filled with any\n      reasonable input value.\n\n  Returns:\n    A `tff.learning.Model`.\n\n  Raises:\n    TypeError: If `keras_model` is not an instance of `tf.keras.Model`.\n    ValueError: If `keras_model` was *not* compiled.\n  \"\"\"\n  py_typecheck.check_type(keras_model, tf.keras.Model)\n  dummy_tensors = _preprocess_dummy_batch(dummy_batch)\n  # NOTE: A sub-classed tf.keras.Model does not produce the compiled metrics\n  # until the model has been called on input. The work-around is to call\n  # Model.test_on_batch() once before asking for metrics.\n  keras_model.test_on_batch(**dummy_tensors)\n  # Optimizer attribute is only set after calling tf.keras.Model.compile().\n  if not hasattr(keras_model, 'optimizer'):\n    raise ValueError('`keras_model` must be compiled. Use from_keras_model() '\n                     'instead.')\n  return model_utils.enhance(_TrainableKerasModel(keras_model, dummy_tensors))\n\n\ndef federated_aggregate_keras_metric(metric_type, metric_config,\n                                     federated_variables):\n  \"\"\"Aggregates variables a keras metric placed at CLIENTS to SERVER.\n\n  Args:\n    metric_type: a type object (type must inherit from\n      `tf.keras.metrics.Metric`).\n    metric_config: the result of calling `get_config()` on a metric object, used\n      with `metric_type.from_config()` to locally construct a new metric object.\n    federated_variables: a federated value place on clients that is the value\n      returned by `tf.keras.metrics.Metric.variables`.\n\n  Returns:\n    The result of calling `result()` on a `tf.keras.metrics.Metric` of type\n  `metric_type`, after aggregation all CLIENTS places `variables`.\n  \"\"\"\n  member_type = federated_variables.type_signature.member\n\n  @tff.tf_computation\n  def zeros_fn():\n    # `member_type` is a (potentially nested) `tff.NamedTupleType`, which is an\n    # `anonymous_tuple.AnonymousTuple`.\n    return anonymous_tuple.map_structure(\n        lambda v: tf.zeros(v.shape, dtype=v.dtype), member_type)\n\n  zeros = zeros_fn()\n\n  # TODO(b/123995628): as of 2019-02-01 all variables created in a\n  # `tf.keras.metrics.Metric` use the argument\n  # `aggregation=tf.VariableAggregation.SUM`, hence below only uses `tf.add`.\n  # This may change in the future (and the `tf.Variable.aggregation` property\n  # will be exposed in a future TF version). Need to handle non-SUM variables.\n\n  @tff.tf_computation(member_type, member_type)\n  def accumulate(accumulators, variables):\n    return tf.nest.map_structure(tf.add, accumulators, variables)\n\n  @tff.tf_computation(member_type, member_type)\n  def merge(a, b):\n    return tf.nest.map_structure(tf.add, a, b)\n\n  @tff.tf_computation(member_type)\n  def report(accumulators):\n    \"\"\"Insert `accumulators` back into the kera metric to obtain result.\"\"\"\n    # NOTE: the following call requires that `metric_type` have a no argument\n    # __init__ method, which will restrict the types of metrics that can be\n    # used. This is somewhat limiting, but the pattern to use default arguments\n    # and export the values in `get_config()` (see\n    # `tf.keras.metrics.TopKCategoricalAccuracy`) works well.\n    keras_metric = None\n    try:\n      keras_metric = metric_type.from_config(metric_config)\n    except TypeError as e:\n      # Re-raise the error with a more helpful message, but the previous stack\n      # trace.\n      raise TypeError(\n          'Caught expection trying to call `{t}.from_config()` with '\n          'config {c}. Confirm that {t}.__init__() has an argument for '\n          'each member of the config.\\nException: {e}'.format(\n              t=metric_type, c=metric_config, e=e))\n\n    assignments = []\n    for v, a in zip(keras_metric.variables, accumulators):\n      assignments.append(tf.assign(v, a))\n    with tf.control_dependencies(assignments):\n      return keras_metric.result()\n\n  return tff.federated_aggregate(federated_variables, zeros, accumulate, merge,\n                                 report)\n\n\nclass _KerasModel(model_lib.Model):\n  \"\"\"Internal wrapper class for tf.keras.Model objects.\"\"\"\n\n  def __init__(self, inner_model, dummy_batch, loss_fns, loss_weights, metrics):\n\n    # NOTE: sub-classed `tf.keras.Model`s do not have fully initialized\n    # variables until they are called on input. We forced that here.\n    inner_model(dummy_batch['x'])\n\n    def _tensor_spec_with_undefined_batch_dim(tensor):\n      # Remove the batch dimension and leave it unspecified.\n      spec = tf.TensorSpec(\n          shape=[None] + tensor.shape.dims[1:], dtype=tensor.dtype)\n      return spec\n\n    self._input_spec = tf.nest.map_structure(\n        _tensor_spec_with_undefined_batch_dim, dummy_batch)\n\n    self._keras_model = inner_model\n    self._loss_fns = loss_fns\n\n    if isinstance(loss_weights, collections.Mapping):\n      print(inner_model.output_names)\n      print(loss_weights.keys())\n      self._loss_weights = []\n      for name in inner_model.output_names:\n        if name not in loss_weights:\n          self._loss_weights.append(1.0)\n        else:\n          self._loss_weights.append(loss_weights[name])\n    else:\n      if loss_weights is None:\n        self._loss_weights = [1.0 for _ in range(len(loss_fns))]\n      else:\n        self._loss_weights = loss_weights\n\n    loss_weights = self._loss_weights\n    self._metrics = metrics if metrics is not None else []\n\n    # This is defined here so that it closes over the `loss_fn`.\n    class _WeightedMeanLossMetric(tf.keras.metrics.Mean):\n      \"\"\"A `tf.keras.metrics.Metric` wrapper for the loss function.\"\"\"\n\n      def __init__(self, name='loss', dtype=tf.float32):\n        super(_WeightedMeanLossMetric, self).__init__(name, dtype)\n        self._loss_fns = loss_fns\n        self._loss_weights = loss_weights\n\n      def update_state(self, y_true, y_pred, sample_weight=None):\n        if len(self._loss_fns) == 1:\n          batch_size = tf.cast(tf.shape(y_pred)[0], self._dtype)\n          y_true = tf.cast(y_true, self._dtype)\n          y_pred = tf.cast(y_pred, self._dtype)\n          batch_loss = self._loss_fns[0](y_true, y_pred)\n\n        else:\n          batch_loss = tf.zeros(())\n          for i in range(len(self._loss_fns)):\n            y_t = tf.cast(y_true[i], self._dtype)\n            y_p = tf.cast(y_pred[i], self._dtype)\n            batch_loss += self._loss_weights[i] * self._loss_fns[i](y_t, y_p)\n\n          batch_size = tf.cast(tf.shape(y_pred[0])[0], self._dtype)\n\n        return super(_WeightedMeanLossMetric,\n                     self).update_state(batch_loss, batch_size)\n\n    self._loss_metric = _WeightedMeanLossMetric()\n\n    metric_variable_type_dict = tf.nest.map_structure(\n        tf.TensorSpec.from_tensor, self.report_local_outputs())\n    federated_local_outputs_type = tff.FederatedType(metric_variable_type_dict,\n                                                     tff.CLIENTS)\n\n    def federated_output(local_outputs):\n      results = collections.OrderedDict()\n      for metric, variables in zip(self.get_metrics(), local_outputs):\n        results[metric.name] = federated_aggregate_keras_metric(\n            type(metric), metric.get_config(), variables)\n      return results\n\n    self._federated_output_computation = tff.federated_computation(\n        federated_output, federated_local_outputs_type)\n\n    # Keras creates variables that are not added to any collection, making it\n    # impossible for TFF to extract them and create the appropriate initializer\n    # before call a tff.Computation. Here we store them in a TFF specific\n    # collection so that they can be retrieved later.\n    # TODO(b/122081673): this likely goes away in TF2.0\n    for variable in itertools.chain(self.trainable_variables,\n                                    self.non_trainable_variables,\n                                    self.local_variables):\n      tf.add_to_collection(graph_keys.GraphKeys.VARS_FOR_TFF_TO_INITIALIZE,\n                           variable)\n\n  @property\n  def trainable_variables(self):\n    return self._keras_model.trainable_variables\n\n  @property\n  def non_trainable_variables(self):\n    return self._keras_model.non_trainable_variables\n\n  @property\n  def local_variables(self):\n    local_variables = []\n    for metric in self.get_metrics():\n      local_variables.extend(metric.variables)\n    return local_variables\n\n  def get_metrics(self):\n    if not self._keras_model._is_compiled:  # pylint: disable=protected-access\n      return self._metrics + [self._loss_metric]\n    else:\n      return self._keras_model.metrics + [self._loss_metric]\n\n  @property\n  def input_spec(self):\n    return self._input_spec\n\n  def _forward_pass(self, batch_input, training=True):\n    # forward_pass requires batch_input be a dictionary that can be passed to\n    # tf.keras.Model.__call__, namely it has keys `x`, and optionally `y`.\n    if hasattr(batch_input, '_asdict'):\n      batch_input = batch_input._asdict()\n\n    inputs = batch_input.get('x')\n    if inputs is None:\n      raise KeyError('Received a batch_input that is missing required key `x`. '\n                     'Instead have keys {}'.format(list(batch_input.keys())))\n    predictions = self._keras_model(inputs=inputs, training=training)\n    y_true = batch_input.get('y')\n    if y_true is not None:\n      if len(self._loss_fns) == 1:\n        loss_fn = self._loss_fns[0]\n        batch_loss = loss_fn(y_true=y_true, y_pred=predictions)\n\n      else:\n        batch_loss = tf.zeros(())\n        for i in range(len(self._loss_fns)):\n          loss_fn = self._loss_fns[i]\n          loss_wt = self._loss_weights[i]\n          batch_loss += loss_wt * loss_fn(y_true=y_true[i], y_pred=predictions[i])\n\n    else:\n      batch_loss = None\n\n    for metric in self.get_metrics():\n      metric.update_state(y_true=y_true, y_pred=predictions)\n\n    return model_lib.BatchOutput(loss=batch_loss, predictions=predictions)\n\n  @tf.function\n  def forward_pass(self, batch_input, training=True):\n    return self._forward_pass(batch_input, training=training)\n\n  @tf.function\n  def report_local_outputs(self):\n    \"\"\"Reports the variables of the metrics tracked during local training.\n\n    Returns:\n      A `collections.OrderedDict` of metric name keys to lists of metric\n    variables.\n    \"\"\"\n    outputs = collections.OrderedDict()\n    for metric in self.get_metrics():\n      outputs[metric.name] = [v.read_value() for v in metric.variables]\n    return outputs\n\n  @property\n  def federated_output_computation(self):\n    return self._federated_output_computation\n\n  @classmethod\n  def make_batch(cls, x, y):\n    return cls.Batch(x=x, y=y)\n\n\n\nclass _TrainableKerasModel(_KerasModel, model_lib.TrainableModel):\n  \"\"\"Wrapper class for `tf.keras.Model`s that can be trained.\"\"\"\n\n  def __init__(self, inner_model, dummy_batch):\n    super(_TrainableKerasModel,\n          self).__init__(inner_model, dummy_batch,\n                         inner_model.loss_functions,\n                         inner_model.loss_weights,\n                         inner_model.metrics)\n\n  @property\n  def local_variables(self):\n    return (super(_TrainableKerasModel, self).local_variables +\n            self._keras_model.optimizer.variables())\n\n  @tf.function\n  def train_on_batch(self, batch_input):\n    batch_output = self._forward_pass(batch_input)\n    _ = self._keras_model.optimizer.get_updates(\n        loss=batch_output.loss, params=self.trainable_variables)\n    return batch_output\n", "hunk": "@@ -277,7 +277,7 @@ def federated_aggregate_keras_metric(metric_type, metric_config,\n class _KerasModel(model_lib.Model):\n   \"\"\"Internal wrapper class for tf.keras.Model objects.\"\"\"\n \n-  def __init__(self, inner_model, dummy_batch, loss_fns, loss_weights, metrics):\n+  def __init__(self, inner_model, dummy_batch, loss_fns, loss_weights=None, metrics=None):\n \n     # NOTE: sub-classed `tf.keras.Model`s do not have fully initialized\n     # variables until they are called on input. We forced that here.\n", "comment": "```suggestion def __init__(self, inner_model, dummy_batch, loss_fns, loss_weights=None, metrics=None): ``` Lets add defaults for everything after `loss_fns`", "ids": ["8207", "098859d97ab305705779d0b3ff4807498ee84c1f", "ee5ba8d4c553ea5c0b47b48a2b1bbf986f36b36b"], "repo": "tensorflow/federated", "ghid": 484, "old": " class _KerasModel(model_lib.Model):\n   \"\"\"Internal wrapper class for tf.keras.Model objects.\"\"\"\n-  def __init__(self, inner_model, dummy_batch, loss_fns, loss_weights, metrics):\n     # NOTE: sub-classed `tf.keras.Model`s do not have fully initialized\n     # variables until they are called on input. We forced that here.", "new": " class _KerasModel(model_lib.Model):\n   \"\"\"Internal wrapper class for tf.keras.Model objects.\"\"\"\n+  def __init__(self, inner_model, dummy_batch, loss_fns, loss_weights=None, metrics=None):\n     # NOTE: sub-classed `tf.keras.Model`s do not have fully initialized\n     # variables until they are called on input. We forced that here.", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -57,6 +60,17 @@ def _apply_incubators(self):\n             for egg in self.eggs:\n                 if egg[\"used\"] or egg[\"km\"] == -1:\n                     continue\n+                \n+                if self.breakable_incubator:\n+                    if incubator.get('uses_remaining') is not None:\n+                        if egg[\"km\"] not in self.breakable_incubator:\n+                            continue\n+                    \n+                if self.infinite_incubator:\n+                    if incubator.get('uses_remaining') is None: ", "oldf": "from datetime import datetime, timedelta\n\nfrom pokemongo_bot.human_behaviour import sleep\nfrom pokemongo_bot.base_task import BaseTask\n\n\nclass IncubateEggs(BaseTask):\n    SUPPORTED_TASK_API_VERSION = 1\n\n    last_km_walked = 0\n\n    def initialize(self):\n        self.next_update = None\n        self.ready_incubators = []\n        self.used_incubators = []\n        self.eggs = []\n        self.km_walked = 0\n        self.hatching_animation_delay = 4.20\n        self.max_iv = 45.0\n\n        self._process_config()\n\n    def _process_config(self):\n        self.longer_eggs_first = self.config.get(\"longer_eggs_first\", True)\n        self.min_interval = self.config.get('min_interval', 120)\n        \n        self.breakable_incubator = self.config.get(\"breakable\", [])\n        self.infinite_incubator = self.config.get(\"infinite\", [])\n    \n    def work(self):\n        try:\n            self._check_inventory()\n        except:\n            return\n\n        if self.used_incubators and IncubateEggs.last_km_walked != self.km_walked:\n            self.used_incubators.sort(key=lambda x: x.get(\"km\"))\n            km_left = self.used_incubators[0]['km']-self.km_walked\n            if km_left <= 0:\n                self._hatch_eggs()\n            else:\n                self.bot.metrics.next_hatching_km(km_left)\n\n        if self._should_print():\n            self._print_eggs()\n            self._compute_next_update()\n\n        IncubateEggs.last_km_walked = self.km_walked\n\n        sorting = self.longer_eggs_first\n        self.eggs.sort(key=lambda x: x.get(\"km\"), reverse=sorting)\n\n        if self.ready_incubators:\n            self._apply_incubators()\n\n    def _apply_incubators(self):\n        for incubator in self.ready_incubators:\n            if incubator.get('used', False):\n                continue\n            for egg in self.eggs:\n                if egg[\"used\"] or egg[\"km\"] == -1:\n                    continue\n                \n                if self.breakable_incubator:\n                    if incubator.get('uses_remaining') is not None:\n                        if egg[\"km\"] not in self.breakable_incubator:\n                            continue\n                    \n                if self.infinite_incubator:\n                    if incubator.get('uses_remaining') is None: \n                        if egg[\"km\"] not in self.infinite_incubator:\n                            continue\n                \n                self.emit_event(\n                    'incubate_try',\n                    level='debug',\n                    formatted=\"Attempting to apply incubator {incubator_id} to egg {egg_id}\",\n                    data={\n                        'incubator_id': incubator['id'],\n                        'egg_id': egg['id']\n                    }\n                )\n                ret = self.bot.api.use_item_egg_incubator(\n                    item_id=incubator[\"id\"],\n                    pokemon_id=egg[\"id\"]\n                )\n                if ret:\n                    code = ret.get(\"responses\", {}).get(\"USE_ITEM_EGG_INCUBATOR\", {}).get(\"result\", 0)\n                    if code == 1:\n                        self.emit_event(\n                            'incubate',\n                            formatted='Incubating a {distance_in_km} egg.',\n                            data={\n                                'distance_in_km': str(egg['km'])\n                            }\n                        )\n                        egg[\"used\"] = True\n                        incubator[\"used\"] = True\n                        break\n                    elif code == 5 or code == 7:\n                        self.emit_event(\n                            'incubator_already_used',\n                            level='debug',\n                            formatted='Incubator in use.',\n                        )\n                        incubator[\"used\"] = True\n                        break\n                    elif code == 6:\n                        self.emit_event(\n                            'egg_already_incubating',\n                            level='debug',\n                            formatted='Egg already incubating',\n                        )\n                        egg[\"used\"] = True\n\n    def _check_inventory(self, lookup_ids=[]):\n        inv = {}\n        response_dict = self.bot.api.get_inventory()\n        matched_pokemon = []\n        temp_eggs = []\n        temp_used_incubators = []\n        temp_ready_incubators = []\n        inv = reduce(\n            dict.__getitem__,\n            [\"responses\", \"GET_INVENTORY\", \"inventory_delta\", \"inventory_items\"],\n            response_dict\n        )\n        for inv_data in inv:\n            inv_data = inv_data.get(\"inventory_item_data\", {})\n            if \"egg_incubators\" in inv_data:\n                temp_used_incubators = []\n                temp_ready_incubators = []\n                incubators = inv_data.get(\"egg_incubators\", {}).get(\"egg_incubator\",[])\n                if isinstance(incubators, basestring):  # checking for old response\n                    incubators = [incubators]\n                for incubator in incubators:                                           \n                    if 'pokemon_id' in incubator:\n                        start_km = incubator.get('start_km_walked', 9001)\n                        km_walked = incubator.get('target_km_walked', 9001)\n                        temp_used_incubators.append({\n                            \"id\": incubator.get('id', -1),\n                            \"km\": km_walked,\n                            \"km_needed\": (km_walked - start_km)\n                        })\n                    else:\n                        temp_ready_incubators.append({\n                            \"id\": incubator.get('id', -1)\n                        })\n                continue\n            if \"pokemon_data\" in inv_data:\n                pokemon = inv_data.get(\"pokemon_data\", {})\n                if pokemon.get(\"is_egg\", False) and \"egg_incubator_id\" not in pokemon:\n                    temp_eggs.append({\n                        \"id\": pokemon.get(\"id\", -1),\n                        \"km\": pokemon.get(\"egg_km_walked_target\", -1),\n                        \"used\": False\n                    })\n                elif 'is_egg' not in pokemon and pokemon['id'] in lookup_ids:\n                    pokemon.update({\n                        \"iv\": [\n                            pokemon.get('individual_attack', 0),\n                            pokemon.get('individual_defense', 0),\n                            pokemon.get('individual_stamina', 0)\n                        ]})\n                    matched_pokemon.append(pokemon)\n                continue\n            if \"player_stats\" in inv_data:\n                self.km_walked = inv_data.get(\"player_stats\", {}).get(\"km_walked\", 0)\n        if temp_used_incubators:\n            self.used_incubators = temp_used_incubators\n        if temp_ready_incubators:\n            self.ready_incubators = temp_ready_incubators\n        if temp_eggs:\n            self.eggs = temp_eggs\n        return matched_pokemon\n\n    def _hatch_eggs(self):\n        response_dict = self.bot.api.get_hatched_eggs()\n        log_color = 'green'\n        try:\n            result = reduce(dict.__getitem__, [\"responses\", \"GET_HATCHED_EGGS\"], response_dict)\n        except KeyError:\n            return\n        pokemon_ids = []\n        if 'pokemon_id' in result:\n            pokemon_ids = [id for id in result['pokemon_id']]\n        stardust = result.get('stardust_awarded', \"error\")\n        candy = result.get('candy_awarded', \"error\")\n        xp = result.get('experience_awarded', \"error\")\n        sleep(self.hatching_animation_delay)\n        self.bot.latest_inventory = None\n        try:\n            pokemon_data = self._check_inventory(pokemon_ids)\n            for pokemon in pokemon_data:\n                # pokemon ids seem to be offset by one\n                if pokemon['pokemon_id']!=-1:\n                    pokemon['name'] = self.bot.pokemon_list[(pokemon.get('pokemon_id')-1)]['Name']\n                else:\n                    pokemon['name'] = \"error\"\n        except:\n            pokemon_data = [{\"name\":\"error\",\"cp\":\"error\",\"iv\":\"error\"}]\n        if not pokemon_ids or pokemon_data[0]['name'] == \"error\":\n            self.emit_event(\n                'egg_hatched',\n                data={\n                    'pokemon': 'error',\n                    'cp': 'error',\n                    'iv': 'error',\n                    'exp': 'error',\n                    'stardust': 'error',\n                    'candy': 'error',\n                }\n            )\n            return\n        for i in range(len(pokemon_data)):\n            msg = \"Egg hatched with a {pokemon} (CP {cp} - IV {iv}), {exp} exp, {stardust} stardust and {candy} candies.\"\n            self.bot.metrics.hatched_eggs(1)\n            self.emit_event(\n                'egg_hatched',\n                formatted=msg,\n                data={\n                    'pokemon': pokemon_data[i]['name'],\n                    'cp': pokemon_data[i]['cp'],\n                    'iv': \"{} {}\".format(\n                        \"/\".join(map(str, pokemon_data[i]['iv'])),\n                        round(sum(pokemon_data[i]['iv'])/self.max_iv, 2)\n                    ),\n                    'exp': xp[i],\n                    'stardust': stardust[i],\n                    'candy': candy[i],\n                }\n            )\n\n    def _print_eggs(self):\n        if not self.used_incubators:\n            return\n\n        self.used_incubators.sort(key=lambda x: x.get(\"km\"))\n        \n        eggs = ['{:.2f}/{} km'.format(e['km_needed']-e['km']+self.km_walked, e['km_needed']) for e in self.used_incubators]\n\n        self.emit_event(\n                    'next_egg_incubates',\n                    formatted='Eggs incubating: [{eggs}] (Eggs left: {eggs_left}, Incubating: {eggs_inc})',\n                    data={\n                        'eggs_left': len(self.eggs),\n                        'eggs_inc': len(self.used_incubators),\n                        'eggs': ', '.join(eggs)\n                    }\n                )\n        \n    def _should_print(self):\n        \"\"\"\n        Returns a value indicating whether the eggs should be displayed.\n        :return: True if the stats should be displayed; otherwise, False.\n        :rtype: bool\n        \"\"\"\n        return self.next_update is None or datetime.now() >= self.next_update\n\n    def _compute_next_update(self):\n        \"\"\"\n        Computes the next update datetime based on the minimum update interval.\n        :return: Nothing.\n        :rtype: None\n        \"\"\"\n        self.next_update = datetime.now() + timedelta(seconds=self.min_interval)", "hunk": "@@ -62,12 +62,12 @@ class IncubateEggs(BaseTask):\n                     continue\n                 \n                 if self.breakable_incubator:\n-                    if incubator.get('uses_remaining') is not None:\n+                    if incubator.get('uses_remaining') is not None: # test if the incubator is of type breakable\n                         if egg[\"km\"] not in self.breakable_incubator:\n                             continue\n                     \n                 if self.infinite_incubator:\n-                    if incubator.get('uses_remaining') is None: \n+                    if incubator.get('uses_remaining') is None: # test if the incubator is of type infinite\n                         if egg[\"km\"] not in self.infinite_incubator:\n                             continue\n                 \n", "comment": "Is this check necessary?", "ids": ["26568", "9fbd8e5acff6db1cf13c8a69469aea79f678bcf5", "76ec145a93c6e8e9a0116f9f075c403774a72d70"], "repo": "PokemonGoF/PokemonGo-Bot", "ghid": 4556, "old": "                     continue\n                 if self.breakable_incubator:\n-                    if incubator.get('uses_remaining') is not None:\n                         if egg[\"km\"] not in self.breakable_incubator:\n                             continue\n                 if self.infinite_incubator:\n-                    if incubator.get('uses_remaining') is None: \n                         if egg[\"km\"] not in self.infinite_incubator:\n                             continue", "new": "                     continue\n                 if self.breakable_incubator:\n+                    if incubator.get('uses_remaining') is not None: # test if the incubator is of type breakable\n                         if egg[\"km\"] not in self.breakable_incubator:\n                             continue\n                 if self.infinite_incubator:\n+                    if incubator.get('uses_remaining') is None: # test if the incubator is of type infinite\n                         if egg[\"km\"] not in self.infinite_incubator:\n                             continue", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -96,6 +96,46 @@ def setUp(self):\n                 'title': {'text': 'Figure Title'}}\n         }).to_dict()\n \n+        self.mapbox_fig_dict = go.Figure({\n+            'data': [\n+                {'type': 'scattermapbox', 'uid': 'first', 'subplot': 'mapbox'},\n+                {'type': 'scattermapbox', 'uid': 'second', 'subplot': 'mapbox2'},\n+                {'type': 'scattermapbox', 'uid': 'third', 'subplot': 'mapbox3'}\n+            ],\n+            'layout': {\n+                'title': {'text': 'Figure Title'},\n+            }\n+        }).to_dict()\n+\n+        # Precompue pair so lat/lon, easting/northing, mapbox coord values", "oldf": "from unittest import TestCase, SkipTest\n\ntry:\n    from unittest.mock import Mock\nexcept:\n    from mock import Mock\n\nimport uuid\nfrom holoviews import Tiles\ntry:\n    import plotly.graph_objs as go\nexcept:\n    go = None\n\nfrom holoviews.streams import (\n    BoundsXY, BoundsX, BoundsY, RangeXY, RangeX, RangeY, Selection1D\n)\n\ntry:\n    from holoviews.plotting.plotly.callbacks import (\n        RangeXYCallback, RangeXCallback, RangeYCallback,\n        BoundsXYCallback, BoundsXCallback, BoundsYCallback,\n        Selection1DCallback\n    )\nexcept:\n    pass\n\n\ndef mock_plot(trace_uid=None):\n    # Build a mock to stand in for a PlotlyPlot subclass\n    if trace_uid is None:\n        trace_uid = str(uuid.uuid4())\n\n    plot = Mock()\n    plot.trace_uid = trace_uid\n    return plot\n\n\ndef build_callback_set(callback_cls, trace_uids, stream_type, num_streams=2):\n    \"\"\"\n    Build a collection of plots, callbacks, and streams for a given callback class and\n    a list of trace_uids\n    \"\"\"\n    plots = []\n    streamss = []\n    callbacks = []\n    eventss = []\n    for trace_uid in trace_uids:\n        plot = mock_plot(trace_uid)\n        streams, event_list = [], []\n        for _ in range(num_streams):\n            events = []\n            stream = stream_type()\n            def cb(events=events, **kwargs):\n                events.append(kwargs)\n            stream.add_subscriber(cb)\n            streams.append(stream)\n            event_list.append(events)\n        callback = callback_cls(plot, streams, None)\n\n        plots.append(plot)\n        streamss.append(streams)\n        callbacks.append(callback)\n        eventss.append(event_list)\n\n    return plots, streamss, callbacks, eventss\n\n\nclass TestCallbacks(TestCase):\n\n    def setUp(self):\n        if go is None:\n            raise SkipTest(\"Plotly required to test plotly callbacks\")\n        self.fig_dict = go.Figure({\n            'data': [\n                {'type': 'scatter',\n                 'y': [1, 2, 3],\n                 'uid': 'first'},\n                {'type': 'bar',\n                 'y': [1, 2, 3],\n                 'uid': 'second',\n                 'xaxis': 'x',\n                 'yaxis': 'y'},\n                {'type': 'scatter',\n                 'y': [1, 2, 3],\n                 'uid': 'third',\n                 'xaxis': 'x2',\n                 'yaxis': 'y2'},\n                {'type': 'bar',\n                 'y': [1, 2, 3],\n                 'uid': 'forth',\n                 'xaxis': 'x3',\n                 'yaxis': 'y3'},\n            ],\n            'layout': {\n                'title': {'text': 'Figure Title'}}\n        }).to_dict()\n\n        self.mapbox_fig_dict = go.Figure({\n            'data': [\n                {'type': 'scattermapbox', 'uid': 'first', 'subplot': 'mapbox'},\n                {'type': 'scattermapbox', 'uid': 'second', 'subplot': 'mapbox2'},\n                {'type': 'scattermapbox', 'uid': 'third', 'subplot': 'mapbox3'}\n            ],\n            'layout': {\n                'title': {'text': 'Figure Title'},\n            }\n        }).to_dict()\n\n        # Precompue pair so lat/lon, easting/northing, mapbox coord values\n        self.lon_range1, self.lat_range1 = (10, 30), (20, 40)\n        self.easting_range1, self.northing_range1 = Tiles.lon_lat_to_easting_northing(\n            self.lon_range1, self.lat_range1\n        )\n        self.easting_range1 = tuple(self.easting_range1)\n        self.northing_range1 = tuple(self.northing_range1)\n\n        self.mapbox_coords1 = [\n            [self.lon_range1[0], self.lat_range1[1]],\n            [self.lon_range1[1], self.lat_range1[1]],\n            [self.lon_range1[1], self.lat_range1[0]],\n            [self.lon_range1[0], self.lat_range1[0]]\n        ]\n\n        self.lon_range2, self.lat_range2 = (-50, -30), (-70, -40)\n        self.easting_range2, self.northing_range2 = Tiles.lon_lat_to_easting_northing(\n            self.lon_range2, self.lat_range2\n        )\n        self.easting_range2 = tuple(self.easting_range2)\n        self.northing_range2 = tuple(self.northing_range2)\n\n        self.mapbox_coords2 = [\n            [self.lon_range2[0], self.lat_range2[1]],\n            [self.lon_range2[1], self.lat_range2[1]],\n            [self.lon_range2[1], self.lat_range2[0]],\n            [self.lon_range2[0], self.lat_range2[0]]\n        ]\n\n    def testCallbackClassInstanceTracking(self):\n        # Each callback class should track all active instances of its own class in a\n        # weak value dictionary. Here we make sure that instances stay separated per\n        # class\n        plot1 = mock_plot()\n        plot2 = mock_plot()\n        plot3 = mock_plot()\n\n        # Check RangeXYCallback\n        rangexy_cb = RangeXYCallback(plot1, [], None)\n        self.assertIn(plot1.trace_uid, RangeXYCallback.instances)\n        self.assertIs(rangexy_cb, RangeXYCallback.instances[plot1.trace_uid])\n\n        # Check BoundsXYCallback\n        boundsxy_cb = BoundsXYCallback(plot2, [], None)\n        self.assertIn(plot2.trace_uid, BoundsXYCallback.instances)\n        self.assertIs(boundsxy_cb, BoundsXYCallback.instances[plot2.trace_uid])\n\n        # Check Selection1DCallback\n        selection1d_cb = Selection1DCallback(plot3, [], None)\n        self.assertIn(plot3.trace_uid, Selection1DCallback.instances)\n        self.assertIs(selection1d_cb, Selection1DCallback.instances[plot3.trace_uid])\n\n        # Check that objects don't show up as instances in the wrong class\n        self.assertNotIn(plot1.trace_uid, BoundsXYCallback.instances)\n        self.assertNotIn(plot1.trace_uid, Selection1DCallback.instances)\n        self.assertNotIn(plot2.trace_uid, RangeXYCallback.instances)\n        self.assertNotIn(plot2.trace_uid, Selection1DCallback.instances)\n        self.assertNotIn(plot3.trace_uid, RangeXYCallback.instances)\n        self.assertNotIn(plot3.trace_uid, BoundsXYCallback.instances)\n\n    def testRangeXYCallbackEventData(self):\n        for viewport in [\n            {'xaxis.range': [1, 4], 'yaxis.range': [-1, 5]},\n            {'xaxis.range[0]': 1, 'xaxis.range[1]': 4,\n             'yaxis.range[0]': -1, 'yaxis.range[1]': 5},\n        ]:\n            event_data = RangeXYCallback.get_event_data_from_property_update(\n                \"viewport\", viewport, self.fig_dict\n            )\n\n            self.assertEqual(event_data, {\n                'first': {'x_range': (1, 4), 'y_range': (-1, 5)},\n                'second': {'x_range': (1, 4), 'y_range': (-1, 5)},\n            })\n\n    def testRangeXCallbackEventData(self):\n        for viewport in [\n            {'xaxis.range': [1, 4], 'yaxis.range': [-1, 5]},\n            {'xaxis.range[0]': 1, 'xaxis.range[1]': 4,\n             'yaxis.range[0]': -1, 'yaxis.range[1]': 5},\n        ]:\n            event_data = RangeXCallback.get_event_data_from_property_update(\n                \"viewport\", viewport, self.fig_dict\n            )\n\n            self.assertEqual(event_data, {\n                'first': {'x_range': (1, 4)},\n                'second': {'x_range': (1, 4)},\n            })\n\n    def testRangeYCallbackEventData(self):\n        for viewport in [\n            {'xaxis.range': [1, 4], 'yaxis.range': [-1, 5]},\n            {'xaxis.range[0]': 1, 'xaxis.range[1]': 4,\n             'yaxis.range[0]': -1, 'yaxis.range[1]': 5},\n        ]:\n            event_data = RangeYCallback.get_event_data_from_property_update(\n                \"viewport\", viewport, self.fig_dict\n            )\n\n            self.assertEqual(event_data, {\n                'first': {'y_range': (-1, 5)},\n                'second': {'y_range': (-1, 5)},\n            })\n\n    def testMapboxRangeXYCallbackEventData(self):\n        relayout_data = {\n            'mapbox._derived': {\"coordinates\": self.mapbox_coords1},\n            'mapbox3._derived': {\"coordinates\": self.mapbox_coords2}\n        }\n\n        event_data = RangeXYCallback.get_event_data_from_property_update(\n            \"relayout_data\", relayout_data, self.mapbox_fig_dict\n        )\n\n        self.assertEqual(event_data, {\n            'first': {'x_range': self.easting_range1, 'y_range': self.northing_range1},\n            'third': {'x_range': self.easting_range2, 'y_range': self.northing_range2},\n        })\n\n    def testMapboxRangeXCallbackEventData(self):\n        relayout_data = {\n            'mapbox._derived': {\"coordinates\": self.mapbox_coords1},\n            'mapbox3._derived': {\"coordinates\": self.mapbox_coords2}\n        }\n\n        event_data = RangeXCallback.get_event_data_from_property_update(\n            \"relayout_data\", relayout_data, self.mapbox_fig_dict\n        )\n\n        self.assertEqual(event_data, {\n            'first': {'x_range': self.easting_range1},\n            'third': {'x_range': self.easting_range2},\n        })\n\n    def testMapboxRangeYCallbackEventData(self):\n        relayout_data = {\n            'mapbox._derived': {\"coordinates\": self.mapbox_coords1},\n            'mapbox3._derived': {\"coordinates\": self.mapbox_coords2}\n        }\n\n        event_data = RangeYCallback.get_event_data_from_property_update(\n            \"relayout_data\", relayout_data, self.mapbox_fig_dict\n        )\n\n        self.assertEqual(event_data, {\n            'first': {'y_range': self.northing_range1},\n            'third': {'y_range': self.northing_range2},\n        })\n\n    def testRangeCallbacks(self):\n\n        # Build callbacks\n        range_classes = [RangeXYCallback, RangeXCallback, RangeYCallback]\n\n        xyplots, xystreamss, xycallbacks, xyevents = build_callback_set(\n            RangeXYCallback, ['first', 'second', 'third', 'forth', 'other'],\n            RangeXY, 2\n        )\n\n        xplots, xstreamss, xcallbacks, xevents = build_callback_set(\n            RangeXCallback, ['first', 'second', 'third', 'forth', 'other'],\n            RangeX, 2\n        )\n\n        yplots, ystreamss, ycallbacks, yevents = build_callback_set(\n            RangeYCallback, ['first', 'second', 'third', 'forth', 'other'],\n            RangeY, 2\n        )\n\n        # Sanity check the length of the streams lists\n        for xystreams in xystreamss:\n            self.assertEqual(len(xystreams), 2)\n\n        # Change viewport on first set of axes\n        viewport1 = {'xaxis.range': [1, 4], 'yaxis.range': [-1, 5]}\n        for cb_cls in range_classes:\n            cb_cls.update_streams_from_property_update(\n                \"viewport\", viewport1, self.fig_dict\n            )\n\n        # Check that all streams attached to 'first' and 'second' plots were triggered\n        for xystream, xstream, ystream in zip(\n                xystreamss[0] + xystreamss[1],\n                xstreamss[0] + xstreamss[1],\n                ystreamss[0] + ystreamss[1],\n        ):\n            assert xystream.x_range == (1, 4)\n            assert xystream.y_range == (-1, 5)\n            assert xstream.x_range == (1, 4)\n            assert ystream.y_range == (-1, 5)\n\n        # And that no other streams were triggered\n        for xystream, xstream, ystream in zip(\n                xystreamss[2] + xystreamss[3],\n                xstreamss[2] + xstreamss[3],\n                ystreamss[2] + ystreamss[3],\n        ):\n            assert xystream.x_range is None\n            assert xystream.y_range is None\n            assert xstream.x_range is None\n            assert ystream.y_range is None\n\n        # Change viewport on second set of axes\n        viewport2 = {'xaxis2.range': [2, 5], 'yaxis2.range': [0, 6]}\n        for cb_cls in range_classes:\n            cb_cls.update_streams_from_property_update(\n                \"viewport\", viewport2, self.fig_dict\n            )\n\n        # Check that all streams attached to 'third' were triggered\n        for xystream, xstream, ystream in zip(\n                xystreamss[2], xstreamss[2], ystreamss[2]\n        ):\n            assert xystream.x_range == (2, 5)\n            assert xystream.y_range == (0, 6)\n            assert xstream.x_range == (2, 5)\n            assert ystream.y_range == (0, 6)\n\n        # Change viewport on third set of axes\n        viewport3 = {'xaxis3.range': [3, 6], 'yaxis3.range': [1, 7]}\n        for cb_cls in range_classes:\n            cb_cls.update_streams_from_property_update(\n                \"viewport\", viewport3, self.fig_dict\n            )\n\n        # Check that all streams attached to 'forth' were triggered\n        for xystream, xstream, ystream in zip(\n                xystreamss[3], xstreamss[3], ystreamss[3]\n        ):\n            assert xystream.x_range == (3, 6)\n            assert xystream.y_range == (1, 7)\n            assert xstream.x_range == (3, 6)\n            assert ystream.y_range == (1, 7)\n\n        # Check that streams attached to a trace not in this plot are not triggered\n        for xyevent, xevent, yevent in zip(\n                xyevents[4], xevents[4], yevents[4]\n        ):\n            assert len(xyevent) == 0\n            assert len(yevent) == 0\n            assert len(yevent) == 0\n\n    def testBoundsXYCallbackEventData(self):\n        selected_data1 = {'range': {'x': [1, 4], 'y': [-1, 5]}}\n        event_data = BoundsXYCallback.get_event_data_from_property_update(\n            \"selected_data\", selected_data1, self.fig_dict\n        )\n\n        self.assertEqual(event_data, {\n            'first': {'bounds': (1, -1, 4, 5)},\n            'second': {'bounds': (1, -1, 4, 5)},\n            'third': {'bounds': None},\n            'forth': {'bounds': None}\n        })\n\n    def testBoundsXCallbackEventData(self):\n        selected_data1 = {'range': {'x': [1, 4], 'y': [-1, 5]}}\n        event_data = BoundsXCallback.get_event_data_from_property_update(\n            \"selected_data\", selected_data1, self.fig_dict\n        )\n\n        self.assertEqual(event_data, {\n            'first': {'boundsx': (1, 4)},\n            'second': {'boundsx': (1, 4)},\n            'third': {'boundsx': None},\n            'forth': {'boundsx': None}\n        })\n\n    def testBoundsYCallbackEventData(self):\n        selected_data1 = {'range': {'x': [1, 4], 'y': [-1, 5]}}\n        event_data = BoundsYCallback.get_event_data_from_property_update(\n            \"selected_data\", selected_data1, self.fig_dict\n        )\n\n        self.assertEqual(event_data, {\n            'first': {'boundsy': (-1, 5)},\n            'second': {'boundsy': (-1, 5)},\n            'third': {'boundsy': None},\n            'forth': {'boundsy': None}\n        })\n\n    def testMapboxBoundsXYCallbackEventData(self):\n        selected_data = {\"range\": {'mapbox2': [\n            [self.lon_range1[0], self.lat_range1[0]],\n            [self.lon_range1[1], self.lat_range1[1]]\n        ]}}\n\n        event_data = BoundsXYCallback.get_event_data_from_property_update(\n            \"selected_data\", selected_data, self.mapbox_fig_dict\n        )\n\n        self.assertEqual(event_data, {\n            'first': {'bounds': None},\n            'second': {'bounds': (\n                self.easting_range1[0], self.northing_range1[0],\n                self.easting_range1[1], self.northing_range1[1]\n            )},\n            'third': {'bounds': None}\n        })\n\n    def testMapboxBoundsXCallbackEventData(self):\n        selected_data = {\"range\": {'mapbox': [\n            [self.lon_range1[0], self.lat_range1[0]],\n            [self.lon_range1[1], self.lat_range1[1]]\n        ]}}\n\n        event_data = BoundsXCallback.get_event_data_from_property_update(\n            \"selected_data\", selected_data, self.mapbox_fig_dict\n        )\n\n        self.assertEqual(event_data, {\n            'first': {'boundsx': (\n                self.easting_range1[0], self.easting_range1[1],\n            )},\n            'second': {'boundsx': None},\n            'third': {'boundsx': None}\n        })\n\n    def testMapboxBoundsYCallbackEventData(self):\n        selected_data = {\"range\": {'mapbox3': [\n            [self.lon_range1[0], self.lat_range1[0]],\n            [self.lon_range1[1], self.lat_range1[1]]\n        ]}}\n\n        event_data = BoundsYCallback.get_event_data_from_property_update(\n            \"selected_data\", selected_data, self.mapbox_fig_dict\n        )\n\n        self.assertEqual(event_data, {\n            'first': {'boundsy': None},\n            'second': {'boundsy': None},\n            'third': {'boundsy': (\n               self.northing_range1[0], self.northing_range1[1]\n            )},\n        })\n\n    def testBoundsCallbacks(self):\n\n        # Build callbacks\n        bounds_classes = [BoundsXYCallback, BoundsXCallback, BoundsYCallback]\n\n        xyplots, xystreamss, xycallbacks, xyevents = build_callback_set(\n            BoundsXYCallback, ['first', 'second', 'third', 'forth', 'other'],\n            BoundsXY, 2\n        )\n\n        xplots, xstreamss, xcallbacks, xevents = build_callback_set(\n            BoundsXCallback, ['first', 'second', 'third', 'forth', 'other'],\n            BoundsX, 2\n        )\n\n        yplots, ystreamss, ycallbacks, yevents = build_callback_set(\n            BoundsYCallback, ['first', 'second', 'third', 'forth', 'other'],\n            BoundsY, 2\n        )\n\n        # box selection on first set of axes\n        selected_data1 = {'range': {'x': [1, 4], 'y': [-1, 5]}}\n        for cb_cls in bounds_classes:\n            cb_cls.update_streams_from_property_update(\n                \"selected_data\", selected_data1, self.fig_dict\n            )\n\n        # Check that all streams attached to 'first' and 'second' plots were triggered\n        for xystream, xstream, ystream in zip(\n                xystreamss[0] + xystreamss[1],\n                xstreamss[0] + xstreamss[1],\n                ystreamss[0] + ystreamss[1],\n        ):\n            assert xystream.bounds == (1, -1, 4, 5)\n            assert xstream.boundsx == (1, 4)\n            assert ystream.boundsy == (-1, 5)\n\n        # Check that streams attached to plots in other subplots are called with None\n        # to clear their bounds\n        for xystream, xstream, ystream in zip(\n                xystreamss[2] + xystreamss[3],\n                xstreamss[2] + xstreamss[3],\n                ystreamss[2] + ystreamss[3],\n        ):\n            assert xystream.bounds is None\n            assert xstream.boundsx is None\n            assert ystream.boundsy is None\n\n        # box select on second set of axes\n        selected_data2 = {'range': {'x2': [2, 5], 'y2': [0, 6]}}\n        for cb_cls in bounds_classes:\n            cb_cls.update_streams_from_property_update(\n                \"selected_data\", selected_data2, self.fig_dict\n            )\n\n        # Check that all streams attached to 'second' were triggered\n        for xystream, xstream, ystream in zip(\n                xystreamss[2], xstreamss[2], ystreamss[2],\n        ):\n            assert xystream.bounds == (2, 0, 5, 6)\n            assert xstream.boundsx == (2, 5)\n            assert ystream.boundsy == (0, 6)\n\n        # box select on third set of axes\n        selected_data3 = {'range': {'x3': [3, 6], 'y3': [1, 7]}}\n        for cb_cls in bounds_classes:\n            cb_cls.update_streams_from_property_update(\n                \"selected_data\", selected_data3, self.fig_dict\n            )\n\n        # Check that all streams attached to 'third' were triggered\n        for xystream, xstream, ystream in zip(\n                xystreamss[3], xstreamss[3], ystreamss[3],\n        ):\n            assert xystream.bounds == (3, 1, 6, 7)\n            assert xstream.boundsx == (3, 6)\n            assert ystream.boundsy == (1, 7)\n\n        # lasso select on first set of axes should clear all bounds\n        selected_data_lasso = {'lassoPoints': {'x': [1, 4, 2], 'y': [-1, 5, 2]}}\n        for cb_cls in bounds_classes:\n            cb_cls.update_streams_from_property_update(\n                \"selected_data\", selected_data_lasso, self.fig_dict\n            )\n\n        # Check that all streams attached to this figure are called with None\n        # to clear their bounds\n        for xystream, xstream, ystream in zip(\n                xystreamss[0] + xystreamss[1] + xystreamss[2] + xystreamss[3],\n                xstreamss[0] + xstreamss[1] + xstreamss[2] + xstreamss[3],\n                ystreamss[0] + ystreamss[1] + ystreamss[2] + ystreamss[3],\n        ):\n            assert xystream.bounds is None\n            assert xstream.boundsx is None\n            assert ystream.boundsy is None\n\n        # Check that streams attached to plots not in this figure are not called\n        for xyevent, xevent, yevent in zip(\n                xyevents[4], xevents[4], yevents[4]\n        ):\n            assert xyevent == []\n            assert xevent == []\n            assert yevent == []\n\n    def testSelection1DCallbackEventData(self):\n        selected_data1 = {'points': [\n            {\"pointNumber\": 0, \"curveNumber\": 0},\n            {\"pointNumber\": 2, \"curveNumber\": 0},\n        ]}\n\n        event_data = Selection1DCallback.get_event_data_from_property_update(\n            \"selected_data\", selected_data1, self.fig_dict\n        )\n\n        self.assertEqual(event_data, {\n            'first': {'index': [0, 2]},\n            'second': {'index': []},\n            'third': {'index': []},\n            'forth': {'index': []}\n        })\n\n    def testMapboxSelection1DCallbackEventData(self):\n        selected_data1 = {'points': [\n            {\"pointNumber\": 0, \"curveNumber\": 1},\n            {\"pointNumber\": 2, \"curveNumber\": 1},\n        ]}\n\n        event_data = Selection1DCallback.get_event_data_from_property_update(\n            \"selected_data\", selected_data1, self.mapbox_fig_dict\n        )\n\n        self.assertEqual(event_data, {\n            'first': {'index': []},\n            'second': {'index': [0, 2]},\n            'third': {'index': []},\n        })\n\n    def testSelection1DCallback(self):\n        plots, streamss, callbacks, sel_events = build_callback_set(\n            Selection1DCallback, ['first', 'second', 'third', 'forth', 'other'],\n            Selection1D, 2\n        )\n\n        # Select points from the 'first' plot (first set of axes)\n        selected_data1 = {'points': [\n            {\"pointNumber\": 0, \"curveNumber\": 0},\n            {\"pointNumber\": 2, \"curveNumber\": 0},\n        ]}\n        Selection1DCallback.update_streams_from_property_update(\n            \"selected_data\", selected_data1, self.fig_dict\n        )\n\n        # Check that all streams attached to the 'first' plots were triggered\n        for stream, events in zip(streamss[0], sel_events[0]):\n            assert stream.index == [0, 2]\n            assert len(events) == 1\n\n        # Check that all streams attached to other plots in this figure were triggered\n        # with empty selection\n        for stream in streamss[1] + streamss[2] + streamss[3]:\n            assert stream.index == []\n\n        # Select points from the 'first' and 'second' plot (first set of axes)\n        selected_data1 = {'points': [\n            {\"pointNumber\": 0, \"curveNumber\": 0},\n            {\"pointNumber\": 1, \"curveNumber\": 0},\n            {\"pointNumber\": 1, \"curveNumber\": 1},\n            {\"pointNumber\": 2, \"curveNumber\": 1},\n        ]}\n        Selection1DCallback.update_streams_from_property_update(\n            \"selected_data\", selected_data1, self.fig_dict\n        )\n\n        # Check that all streams attached to the 'first' plot were triggered\n        for stream in streamss[0]:\n            assert stream.index == [0, 1]\n\n        # Check that all streams attached to the 'second' plot were triggered\n        for stream in streamss[1]:\n            assert stream.index == [1, 2]\n\n        # Check that all streams attached to other plots in this figure were triggered\n        # with empty selection\n        for stream in streamss[2] + streamss[3]:\n            assert stream.index == []\n\n        # Select points from the 'forth' plot (third set of axes)\n        selected_data1 = {'points': [\n            {\"pointNumber\": 0, \"curveNumber\": 3},\n            {\"pointNumber\": 2, \"curveNumber\": 3},\n        ]}\n        Selection1DCallback.update_streams_from_property_update(\n            \"selected_data\", selected_data1, self.fig_dict\n        )\n\n        # Check that all streams attached to the 'forth' plot were triggered\n        for stream, events in zip(streamss[3], sel_events[3]):\n            assert stream.index == [0, 2]\n\n        # Check that streams attached to plots not in this figure are not called\n        for stream, events in zip(streamss[4], sel_events[4]):\n            assert len(events) == 0\n", "hunk": "@@ -107,7 +107,7 @@ class TestCallbacks(TestCase):\n             }\n         }).to_dict()\n \n-        # Precompue pair so lat/lon, easting/northing, mapbox coord values\n+        # Precompute pair so lat/lon, easting/northing, mapbox coord values\n         self.lon_range1, self.lat_range1 = (10, 30), (20, 40)\n         self.easting_range1, self.northing_range1 = Tiles.lon_lat_to_easting_northing(\n             self.lon_range1, self.lat_range1\n", "comment": "```suggestion # Precompute pair so lat/lon, easting/northing, mapbox coord values ``` (Can't quite parse it, though!)", "ids": ["24091", "206dde8208f10eac3c582ae8174197598fb5cbba", "afaccffd6b7e6d35995e1b4af04e5cc874b56821"], "repo": "holoviz/holoviews", "ghid": 4686, "old": "             }\n         }).to_dict()\n-        # Precompue pair so lat/lon, easting/northing, mapbox coord values\n         self.lon_range1, self.lat_range1 = (10, 30), (20, 40)\n         self.easting_range1, self.northing_range1 = Tiles.lon_lat_to_easting_northing(\n             self.lon_range1, self.lat_range1", "new": "             }\n         }).to_dict()\n+        # Precompute pair so lat/lon, easting/northing, mapbox coord values\n         self.lon_range1, self.lat_range1 = (10, 30), (20, 40)\n         self.easting_range1, self.northing_range1 = Tiles.lon_lat_to_easting_northing(\n             self.lon_range1, self.lat_range1", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -8,4 +8,16 @@ window.addEventListener(\n     window.location.origin)\n );\n \n+window.addEventListener(\n+  \"message\",\n+  (m) => {\n+    let node = document.getElementById(m.data.show),", "oldf": "/*jslint browser: true */\n\n\nwindow.addEventListener(\n  \"load\",\n  () => window.parent.postMessage(\n    {height: document.body.scrollHeight},\n    window.location.origin)\n);\n\nwindow.addEventListener(\n  \"message\",\n  (m) => {\n    let node = document.getElementById(m.data.show),\n      boundingRect = node.getBoundingClientRect();\n\n    window.parent.postMessage({\n      scrollX: boundingRect.x, scrollY: boundingRect.y,\n      clipWidth: boundingRect.width, clipHeight: boundingRect.height\n    },\n    window.location.origin);\n  }\n)\n", "hunk": "@@ -11,6 +11,9 @@ window.addEventListener(\n window.addEventListener(\n   \"message\",\n   (m) => {\n+    if (m.origin !== location.origin) {\n+      return;\n+    }\n     let node = document.getElementById(m.data.show),\n       boundingRect = node.getBoundingClientRect();\n \n", "comment": "We should be doing some assertion here, I think that `m.origin == location.origin` (i.e., the message comes from our site)", "ids": ["7773", "84bcb8f7e0f2a1c821a0c90a1d2784ce1eea7717", "aaf8b85214762503b5070c6bed491aa5413cd0af"], "repo": "mozilla-services/screenshots", "ghid": 336, "old": " window.addEventListener(\n   \"message\",\n   (m) => {\n     let node = document.getElementById(m.data.show),\n       boundingRect = node.getBoundingClientRect();", "new": " window.addEventListener(\n   \"message\",\n   (m) => {\n+    if (m.origin !== location.origin) {\n+      return;\n+    }\n     let node = document.getElementById(m.data.show),\n       boundingRect = node.getBoundingClientRect();", "lang": "js", "norm_lang": "javascript"}
{"old_hunk": "@@ -310,15 +371,14 @@ class TestEdgeQLScope(tb.QueryTestCase):\n             ]\n         ])\n \n-    # XXX: this test is not longer correct with respect to inline aliases.\n     @unittest.expectedFailure\n-    async def test_edgeql_scope_tuple_07(self):\n+    async def test_edgeql_scope_tuple_08(self):", "oldf": "#\n# This source file is part of the EdgeDB open source project.\n#\n# Copyright 2017-present MagicStack Inc. and the EdgeDB authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport os.path\nimport unittest  # NOQA\n\nfrom edb.client import exceptions as exc\nfrom edb.server import _testbase as tb\n\n\nclass TestEdgeQLScope(tb.QueryTestCase):\n    '''This tests the scoping rules.'''\n\n    SCHEMA = os.path.join(os.path.dirname(__file__), 'schemas',\n                          'cards.eschema')\n\n    SETUP = os.path.join(os.path.dirname(__file__), 'schemas',\n                         'cards_setup.eql')\n\n    async def test_edgeql_scope_sort_01(self):\n        await self.assert_query_result(r'''\n            WITH\n                MODULE test,\n                A := {1, 2},\n                U := (SELECT User FILTER User.name IN {'Alice', 'Bob'})\n            SELECT _ := (U{name}, A)\n            # specifically test the ORDER clause\n            ORDER BY _.1 THEN _.0.name DESC;\n        ''', [\n            [\n                [{'name': 'Bob'}, 1],\n                [{'name': 'Alice'}, 1],\n                [{'name': 'Bob'}, 2],\n                [{'name': 'Alice'}, 2],\n            ]\n        ])\n\n    async def test_edgeql_scope_tuple_01(self):\n        await self.assert_query_result(r'''\n            WITH\n                MODULE test,\n                A := {1, 2}\n            SELECT _ := (User{name, a := A}, A)\n            ORDER BY _.1 THEN _.0.name;\n        ''', [\n            [\n                [{'a': [1], 'name': 'Alice'}, 1],\n                [{'a': [1], 'name': 'Bob'}, 1],\n                [{'a': [1], 'name': 'Carol'}, 1],\n                [{'a': [1], 'name': 'Dave'}, 1],\n                [{'a': [2], 'name': 'Alice'}, 2],\n                [{'a': [2], 'name': 'Bob'}, 2],\n                [{'a': [2], 'name': 'Carol'}, 2],\n                [{'a': [2], 'name': 'Dave'}, 2],\n            ]\n        ])\n\n    async def test_edgeql_scope_tuple_02(self):\n        await self.assert_query_result(r'''\n            WITH\n                MODULE test,\n                A := {1, 2}\n            SELECT _ := (A, User{name, a := A})\n            ORDER BY _.0 THEN _.1.name;\n        ''', [\n            [\n                [1, {'a': 1, 'name': 'Alice'}],\n                [1, {'a': 1, 'name': 'Bob'}],\n                [1, {'a': 1, 'name': 'Carol'}],\n                [1, {'a': 1, 'name': 'Dave'}],\n                [2, {'a': 2, 'name': 'Alice'}],\n                [2, {'a': 2, 'name': 'Bob'}],\n                [2, {'a': 2, 'name': 'Carol'}],\n                [2, {'a': 2, 'name': 'Dave'}],\n            ]\n        ])\n\n    @unittest.expectedFailure\n    async def test_edgeql_scope_tuple_03(self):\n        await self.assert_query_result(r'''\n            WITH MODULE test\n            SELECT _ := (\n                # User.friends is a common path, so it refers to the\n                # SAME object in both tuple elements. In particular\n                # that means that in the User shape there will always\n                # be a single object appearing in friends link\n                # (although it's a ** link).\n                User {\n                    name,\n                    friends: {\n                        @nickname\n                    }\n                },\n                User.friends {name}\n            )\n            ORDER BY _.0.name THEN _.1.name;\n        ''', [\n            [\n                [\n                    {\n                        'name': 'Alice',\n                        'friends': [{'@nickname': 'Swampy'}],\n                    },\n                    {\n                        'name': 'Bob',\n                    },\n                ],\n                [\n                    {\n                        'name': 'Alice',\n                        'friends': [{'@nickname': 'Firefighter'}],\n                    },\n                    {\n                        'name': 'Carol',\n                    },\n                ],\n                [\n                    {\n                        'name': 'Alice',\n                        'friends': [{'@nickname': 'Grumpy'}],\n                    },\n                    {\n                        'name': 'Dave',\n                    },\n                ],\n                [\n                    {\n                        'name': 'Dave',\n                        'friends': [{'@nickname': None}],\n                    },\n                    {\n                        'name': 'Bob',\n                    },\n                ],\n            ]\n        ])\n\n    @unittest.expectedFailure\n    async def test_edgeql_scope_tuple_04(self):\n        await self.assert_query_result(r'''\n            # Same as above, but with a computable instead of real \"friends\"\n            WITH MODULE test\n            SELECT _ := (\n                User {\n                    name,\n                    fr := User.friends {\n                        @nickname\n                    }\n                },\n                User.friends {name}\n            )\n            ORDER BY _.0.name THEN _.1.name;\n        ''', [\n            [\n                [\n                    {\n                        'name': 'Alice',\n                        'fr': [{'@nickname': 'Swampy'}],\n                    },\n                    {\n                        'name': 'Bob',\n                    },\n                ],\n                [\n                    {\n                        'name': 'Alice',\n                        'fr': [{'@nickname': 'Firefighter'}],\n                    },\n                    {\n                        'name': 'Carol',\n                    },\n                ],\n                [\n                    {\n                        'name': 'Alice',\n                        'fr': [{'@nickname': 'Grumpy'}],\n                    },\n                    {\n                        'name': 'Dave',\n                    },\n                ],\n                [\n                    {\n                        'name': 'Dave',\n                        'fr': [{'@nickname': None}],\n                    },\n                    {\n                        'name': 'Bob',\n                    },\n                ],\n            ]\n        ])\n\n    async def test_edgeql_scope_tuple_05(self):\n        await self.assert_query_result(r'''\n            WITH\n                MODULE test,\n                U2 := User\n            SELECT x := (\n                User {name, foo := U2 {name}},\n                U2 { name }\n            )\n            FILTER x.1.name = 'Alice'\n            ORDER BY x.0.name THEN x.1.name;\n        ''', [\n            [\n                [\n                    {\n                        'name': 'Alice',\n                        'foo': [\n                            {'name': 'Alice'},\n                        ],\n                    },\n                    {\n                        'name': 'Alice',\n                    },\n                ],\n                [\n                    {\n                        'name': 'Bob',\n                        'foo': [\n                            {'name': 'Alice'},\n                        ],\n                    },\n                    {\n                        'name': 'Alice',\n                    },\n                ],\n                [\n                    {\n                        'name': 'Carol',\n                        'foo': [\n                            {'name': 'Alice'},\n                        ],\n                    },\n                    {\n                        'name': 'Alice',\n                    },\n                ],\n                [\n                    {\n                        'name': 'Dave',\n                        'foo': [\n                            {'name': 'Alice'},\n                        ],\n                    },\n                    {\n                        'name': 'Alice',\n                    },\n                ],\n            ]\n        ])\n\n    async def test_edgeql_scope_tuple_06(self):\n        await self.assert_query_result(r'''\n            WITH MODULE test\n            SELECT User {\n                name,\n                foo := (\n                    # this is the same as enclosing User\n                    WITH U2 := User\n                    SELECT U2 {name} ORDER BY U2.name\n                )\n            }\n            ORDER BY User.name;\n        ''', [\n            [\n                {\n                    'name': 'Alice',\n                    'foo': {'name': 'Alice'},\n                },\n                {\n                    'name': 'Bob',\n                    'foo': {'name': 'Bob'},\n                },\n                {\n                    'name': 'Carol',\n                    'foo': {'name': 'Carol'},\n                },\n                {\n                    'name': 'Dave',\n                    'foo': {'name': 'Dave'},\n                },\n            ]\n        ])\n\n    @unittest.expectedFailure\n    async def test_edgeql_scope_tuple_07(self):\n        await self.assert_query_result(r'''\n            # compare to test_edgeql_scope_filter_03 to see how it\n            # works out without tuples\n            WITH\n                MODULE test,\n                U2 := User\n            SELECT _ := (\n                User {\n                    name,\n                    friends_of_others := (\n                        SELECT U2.friends {name}\n                        FILTER\n                            # not me\n                            U2.friends != User\n                            AND\n                            # not one of my friends\n                            U2.friends NOT IN User.friends\n                        ORDER BY U2.friends.name\n                    )\n                },\n                U2.friends {\n                    name\n                }\n            )\n            FILTER U2.friends.name = 'Bob'\n            ORDER BY _.0.name THEN _.1;\n        ''', [\n            [\n                [\n                    {\n                        'name': 'Alice',\n                        'friends_of_others': None,  # Bob is a direct friend\n                    },\n                    {\n                        'name': 'Bob',\n                    }\n                ],\n                [\n                    {\n                        'name': 'Bob',\n                        'friends_of_others': None,  # this is Bob\n                    },\n                    {\n                        'name': 'Bob',\n                    }\n                ],\n                [\n                    {\n                        'name': 'Carol',\n                        'friends_of_others': [\n                            {'name': 'Bob'},\n                        ],\n                    },\n                    {\n                        'name': 'Bob',\n                    }\n                ],\n                [\n                    {\n                        'name': 'Dave',\n                        'friends_of_others': None,  # Bob is a direct friend\n                    },\n                    {\n                        'name': 'Bob',\n                    }\n                ],\n            ]\n        ])\n\n    @unittest.expectedFailure\n    async def test_edgeql_scope_tuple_08(self):\n        await self.assert_query_result(r'''\n            # compare to test_edgeql_scope_filter_03 to see how it\n            # works out without tuples\n            WITH\n                MODULE test,\n                U2 := User\n            SELECT _ := (\n                User {\n                    name,\n                    friends_of_others := (\n                        # similar to previous test, but refactored\n                        WITH F := (\n                            SELECT U2.friends\n                            FILTER\n                                # not me\n                                U2.friends != User\n                                AND\n                                # not one of my friends\n                                U2.friends NOT IN User.friends\n                        )\n                        SELECT F {name}\n                        ORDER BY F.name\n                    )\n                },\n                U2.friends {\n                    name\n                }\n            )\n            FILTER U2.friends.name = 'Bob'\n            ORDER BY _.0.name THEN _.1;\n        ''', [\n            [\n                [\n                    {\n                        'name': 'Alice',\n                        'friends_of_others': None,  # Bob is a direct friend\n                    },\n                    {\n                        'name': 'Bob',\n                    },\n                ],\n                [\n                    {\n                        'name': 'Bob',\n                        'friends_of_others': None,  # this is Bob\n                    },\n                    {\n                        'name': 'Bob',\n                    },\n                ],\n                [\n                    {\n                        'name': 'Carol',\n                        'friends_of_others': [\n                            {'name': 'Bob'},\n                        ],\n                    },\n                    {\n                        'name': 'Bob',\n                    },\n                ],\n                [\n                    {\n                        'name': 'Dave',\n                        'friends_of_others': None,  # Bob is a direct friend\n                    },\n                    {\n                        'name': 'Bob',\n                    },\n                ],\n            ]\n        ])\n\n    async def test_edgeql_scope_tuple_09(self):\n        await self.assert_query_result(r'''\n        WITH MODULE test\n        SELECT (User.name, User.deck_cost, count(User.deck),\n                User.deck_cost / count(User.deck))\n        ORDER BY User.name;\n\n        WITH MODULE test\n        # in the below expression User.friends is the\n        # longest common prefix, so we know that for\n        # each friend, the average cost will be\n        # calculated.\n        SELECT User.friends.deck_cost / count(User.friends.deck)\n        ORDER BY User.friends.name;\n\n        WITH MODULE test\n        # in the below expression User.friends is the\n        # longest common prefix, so we know that for\n        # each friend, the average cost will be\n        # calculated.\n        SELECT User.friends.deck_cost / count(User.friends.deck)\n        FILTER User.friends.name = 'Bob';\n        ''', [\n            [\n                ['Alice', 11, 4, 2.75],\n                ['Bob', 9, 4, 2.25],\n                ['Carol', 16, 7, 2.2857142857142856],\n                ['Dave', 20, 7, 2.857142857142857],\n            ],\n            [\n                2.25,                # Bob (friend of Alice and Dave)\n                2.2857142857142856,  # Carol\n                2.857142857142857    # Dave\n            ],\n            [2.25],\n        ])\n\n    async def test_edgeql_scope_tuple_10(self):\n        await self.assert_query_result(r'''\n            WITH MODULE test\n            SELECT x := (\n                Card {\n                    name,\n                    percent_cost := (\n                        WITH CARDINALITY '1'\n                        SELECT <int64>(100 * Card.cost / Card.<deck.deck_cost)\n                    ),\n                },\n                Card.<deck { name }\n            )\n            ORDER BY x.1.name THEN x.0.name;\n        ''', [\n            [\n                [{'name': 'Bog monster', 'percent_cost': 18},\n                 {'name': 'Alice'}],\n                [{'name': 'Dragon', 'percent_cost': 45},\n                 {'name': 'Alice'}],\n                [{'name': 'Giant turtle', 'percent_cost': 27},\n                 {'name': 'Alice'}],\n                [{'name': 'Imp', 'percent_cost': 9},\n                 {'name': 'Alice'}],\n\n                [{'name': 'Bog monster', 'percent_cost': 22},\n                 {'name': 'Bob'}],\n                [{'name': 'Dwarf', 'percent_cost': 11},\n                 {'name': 'Bob'}],\n                [{'name': 'Giant turtle', 'percent_cost': 33},\n                 {'name': 'Bob'}],\n                [{'name': 'Golem', 'percent_cost': 33},\n                 {'name': 'Bob'}],\n\n                [{'name': 'Bog monster', 'percent_cost': 13},\n                 {'name': 'Carol'}],\n                [{'name': 'Djinn', 'percent_cost': 25},\n                 {'name': 'Carol'}],\n                [{'name': 'Dwarf', 'percent_cost': 6},\n                 {'name': 'Carol'}],\n                [{'name': 'Giant eagle', 'percent_cost': 13},\n                 {'name': 'Carol'}],\n                [{'name': 'Giant turtle', 'percent_cost': 19},\n                 {'name': 'Carol'}],\n                [{'name': 'Golem', 'percent_cost': 19},\n                 {'name': 'Carol'}],\n                [{'name': 'Sprite', 'percent_cost': 6},\n                 {'name': 'Carol'}],\n\n                [{'name': 'Bog monster', 'percent_cost': 10},\n                 {'name': 'Dave'}],\n                [{'name': 'Djinn', 'percent_cost': 20},\n                 {'name': 'Dave'}],\n                [{'name': 'Dragon', 'percent_cost': 25},\n                 {'name': 'Dave'}],\n                [{'name': 'Giant eagle', 'percent_cost': 10},\n                 {'name': 'Dave'}],\n                [{'name': 'Giant turtle', 'percent_cost': 15},\n                 {'name': 'Dave'}],\n                [{'name': 'Golem', 'percent_cost': 15},\n                 {'name': 'Dave'}],\n                [{'name': 'Sprite', 'percent_cost': 5},\n                 {'name': 'Dave'}],\n            ]\n        ])\n\n    @unittest.expectedFailure\n    async def test_edgeql_scope_tuple_11(self):\n        await self.assert_query_result(r'''\n            # this is similar to test_edgeql_scope_tuple_03\n            WITH MODULE test\n            SELECT _ := (\n                # User.friends is a common path, so it refers to the\n                # SAME object in both tuple elements. In particular\n                # that means that in the User shape there will always\n                # be a single object appearing in friends link\n                # (although it's a ** link).\n                User {\n                    name,\n                    friends: {\n                        name\n                    }\n                },\n                User.friends@nickname\n            )\n            ORDER BY _.0.name THEN _.1;\n        ''', [\n            [\n                # the only User who has nicknames for friends is Alice\n                [\n                    {'name': 'Alice', 'friends': [{'name': 'Carol'}]},\n                    'Firefighter'\n                ],\n                [\n                    {'name': 'Alice', 'friends': [{'name': 'Dave'}]},\n                    'Grumpy'\n                ],\n                [\n                    {'name': 'Alice', 'friends': [{'name': 'Bob'}]},\n                    'Swampy'\n                ],\n            ]\n        ])\n\n    @unittest.expectedFailure\n    async def test_edgeql_scope_filter_01(self):\n        await self.assert_query_result(r'''\n            WITH\n                MODULE test,\n                U2 := User\n            SELECT User {\n                name,\n                foo := (SELECT U2 {name} ORDER BY U2.name)\n            }\n            # the FILTER clause is irrelevant because it's in a\n            # parallel scope to the other mentions of U2\n            FILTER U2.name = 'Alice'\n            ORDER BY User.name;\n        ''', [\n            [\n                {\n                    'name': 'Alice',\n                    'foo': [\n                        {'name': 'Alice'},\n                        {'name': 'Bob'},\n                        {'name': 'Carol'},\n                        {'name': 'Dave'},\n                    ],\n                },\n                {\n                    'name': 'Bob',\n                    'foo': [\n                        {'name': 'Alice'},\n                        {'name': 'Bob'},\n                        {'name': 'Carol'},\n                        {'name': 'Dave'},\n                    ],\n                },\n                {\n                    'name': 'Carol',\n                    'foo': [\n                        {'name': 'Alice'},\n                        {'name': 'Bob'},\n                        {'name': 'Carol'},\n                        {'name': 'Dave'},\n                    ],\n                },\n                {\n                    'name': 'Dave',\n                    'foo': [\n                        {'name': 'Alice'},\n                        {'name': 'Bob'},\n                        {'name': 'Carol'},\n                        {'name': 'Dave'},\n                    ],\n                },\n            ]\n        ])\n\n    async def test_edgeql_scope_filter_02(self):\n        await self.assert_query_result(r'''\n            WITH MODULE test\n            SELECT User.friends {name}\n            FILTER User.friends NOT IN {}\n            ORDER BY User.friends.name;\n        ''', [\n            [\n                {'name': 'Bob'},\n                {'name': 'Carol'},\n                {'name': 'Dave'},\n            ]\n        ])\n\n    async def test_edgeql_scope_filter_03(self):\n        await self.assert_query_result(r'''\n            WITH\n                MODULE test,\n                U2 := User\n            SELECT User {\n                name,\n                friends_of_others := (\n                    SELECT U2.friends {name}\n                    FILTER\n                        # not me\n                        U2.friends != User\n                        AND\n                        # not one of my friends\n                        U2.friends NOT IN User.friends\n                    ORDER BY U2.friends.name\n                )\n            }\n            ORDER BY User.name;\n        ''', [\n            [\n                {\n                    'name': 'Alice',\n                    'friends_of_others': None,\n                },\n                {\n                    'name': 'Bob',\n                    'friends_of_others': [\n                        {'name': 'Carol'},\n                        {'name': 'Dave'},\n                    ],\n                },\n                {\n                    'name': 'Carol',\n                    'friends_of_others': [\n                        {'name': 'Bob'},\n                        {'name': 'Dave'},\n                    ],\n                },\n                {\n                    'name': 'Dave',\n                    'friends_of_others': [\n                        {'name': 'Carol'},\n                    ],\n                }\n            ]\n        ])\n\n    async def test_edgeql_scope_filter_04(self):\n        await self.assert_query_result(r'''\n            WITH MODULE test\n            SELECT User {\n                name,\n                friends: {\n                    name\n                } ORDER BY User.friends.name\n            }\n            FILTER User.friends.name = 'Carol';\n        ''', [\n            [\n                {\n                    'name': 'Alice',\n                    'friends': [\n                        {'name': 'Bob'},\n                        {'name': 'Carol'},\n                        {'name': 'Dave'},\n                    ],\n                },\n            ]\n        ])\n\n    async def test_edgeql_scope_filter_05(self):\n        await self.assert_query_result(r'''\n            # User.name is wrapped into a SELECT, so it's a SET OF\n            # w.r.t FILTER\n            WITH MODULE test\n            SELECT (SELECT User.name)\n            FILTER User.name = 'Alice';\n        ''', [\n            {'Alice', 'Bob', 'Carol', 'Dave'}\n        ])\n\n    async def test_edgeql_scope_filter_06(self):\n        await self.assert_query_result(r'''\n            # User is wrapped into a SELECT, so it's a SET OF\n            # w.r.t FILTER\n            WITH MODULE test\n            SELECT (SELECT User).name\n            FILTER User.name = 'Alice';\n        ''', [\n            {'Alice', 'Bob', 'Carol', 'Dave'}\n        ])\n\n    async def test_edgeql_scope_filter_07(self):\n        await self.assert_query_result(r'''\n            # User.name is a SET OF argument of ??, so it's unaffected\n            # by the FILTER\n            WITH MODULE test\n            SELECT ({} ?? User.name)\n            FILTER User.name = 'Alice';\n        ''', [\n            {'Alice', 'Bob', 'Carol', 'Dave'}\n        ])\n\n    async def test_edgeql_scope_filter_08(self):\n        await self.assert_query_result(r'''\n            # User is a SET OF argument of ??, so it's unaffected\n            # by the FILTER\n            WITH MODULE test\n            SELECT ({} ?? User).name\n            FILTER User.name = 'Alice';\n        ''', [\n            {'Alice', 'Bob', 'Carol', 'Dave'}\n        ])\n\n    async def test_edgeql_scope_order_01(self):\n        await self.assert_query_result(r'''\n            WITH MODULE test\n            SELECT User {\n                name,\n                friends: {\n                    name\n                } ORDER BY User.friends.name\n            }\n            ORDER BY (\n                (SELECT User.friends\n                 FILTER User.friends@nickname = 'Firefighter'\n                 LIMIT 1).name\n            ) EMPTY FIRST\n            THEN User.name;\n        ''', [\n            [\n                {\n                    'name': 'Bob',\n                    'friends': None\n                },\n                {\n                    'name': 'Carol',\n                    'friends': None\n                },\n                {\n                    'name': 'Dave',\n                    'friends': [\n                        {'name': 'Bob'},\n                    ],\n                },\n                {\n                    'name': 'Alice',\n                    'friends': [\n                        {'name': 'Bob'},\n                        {'name': 'Carol'},\n                        {'name': 'Dave'},\n                    ],\n                }\n            ]\n        ])\n\n    # NOTE: LIMIT tests are largely identical to OFFSET tests, any\n    # time there is a new OFFSET test, there should be a corresponding\n    # LIMIT one.\n    @unittest.expectedFailure\n    async def test_edgeql_scope_offset_01(self):\n        await self.assert_query_result(r'''\n            WITH MODULE test\n            SELECT User {\n                name,\n                friends: {\n                    name\n                } ORDER BY User.friends.name\n            }\n            ORDER BY User.name\n            # the OFFSET clause is in a sibling scope to SELECT, so\n            # the User.friends are completely independent in them.\n            OFFSET (# NOTE: effectively it's OFFSET 2\n                    #\n                    # Select the average card value (rounded to an\n                    # int) for the user who is someone's friend AND\n                    # nicknamed 'Firefighter':\n                    # - the user happens to be Carol\n                    # - her average deck cost is 2\n                    #   (see test_edgeql_scope_tuple_08)\n                    SELECT\n                        # in the below expression User.friends is the\n                        # longest common prefix, so we know that for\n                        # each friend, the average cost will be\n                        # calculated.\n                        <int64>(User.friends.deck_cost /\n                                count(User.friends.deck))\n                    # finally, filter out the only answer we need, to\n                    # satisfy the CARDINALITY requirement of the OFFSET\n                    FILTER User.friends@nickname = 'Firefighter'\n                    LIMIT 1\n                );\n        ''', [\n            [\n                {\n                    'name': 'Carol',\n                    'friends': None\n                },\n                {\n                    'name': 'Dave',\n                    'friends': [\n                        {'name': 'Bob'},\n                    ],\n                },\n            ]\n        ])\n\n    async def test_edgeql_scope_offset_02(self):\n        await self.assert_query_result(r'''\n            WITH MODULE test\n            SELECT User {\n                name,\n                friends: {\n                    name\n                }  # User.friends is scoped from the enclosing shape\n                ORDER BY User.friends.name\n                OFFSET (count(User.friends) - 1) IF EXISTS User.friends ELSE 0\n                # the above is equivalent to getting the last friend,\n                # ordered by name\n            }\n            ORDER BY User.name;\n        ''', [\n            [\n                {\n                    'name': 'Alice',\n                    'friends': [\n                        {'name': 'Dave'},\n                    ],\n                },\n                {\n                    'name': 'Bob',\n                    'friends': None\n                },\n                {\n                    'name': 'Carol',\n                    'friends': None\n                },\n                {\n                    'name': 'Dave',\n                    'friends': [\n                        {'name': 'Bob'},\n                    ],\n                },\n            ]\n        ])\n\n    @unittest.expectedFailure\n    async def test_edgeql_scope_limit_01(self):\n        await self.assert_query_result(r'''\n            WITH MODULE test\n            SELECT User {\n                name,\n                friends: {\n                    name\n                } ORDER BY User.friends.name\n            }\n            ORDER BY User.name\n            # the LIMIT clause is in a sibling scope to SELECT, so\n            # the User.<friends are completely independent in them.\n            LIMIT ( # NOTE: effectively it's LIMIT 2\n                    #\n                    # Select the average card value (rounded to an\n                    # int) for the user who is someone's friend AND\n                    # nicknamed 'Firefighter':\n                    # - the user happens to be Carol\n                    # - her average deck cost is 2\n                    #   (see test_edgeql_scope_tuple_08)\n                    WITH\n                        F := (\n                            SELECT User\n                            FILTER User.<friends@nickname = 'Firefighter'\n                        )\n                    SELECT\n                        # cardinality should be inferable here:\n                        # - deck_cost is a computable based on sum\n                        # - count also has cardinality 1 of the return set\n                        <int64>(F.deck_cost / count(F.deck))\n                );\n        ''', [\n            [\n                {\n                    'name': 'Alice',\n                    'friends': [\n                        {'name': 'Bob'},\n                        {'name': 'Carol'},\n                        {'name': 'Dave'},\n                    ],\n                },\n                {\n                    'name': 'Bob',\n                    'friends': None\n                },\n            ]\n        ])\n\n    async def test_edgeql_scope_limit_02(self):\n        await self.assert_query_result(r'''\n            WITH MODULE test\n            SELECT User {\n                name,\n                friends: {\n                    name\n                }  # User.friends is scoped from the enclosing shape\n                ORDER BY User.friends.name\n                LIMIT (count(User.friends) - 1) IF EXISTS User.friends ELSE 0\n                # the above is equivalent to getting the all except\n                # last friend, ordered by name\n            }\n            ORDER BY User.name;\n        ''', [\n            [\n                {\n                    'name': 'Alice',\n                    'friends': [\n                        {'name': 'Bob'},\n                        {'name': 'Carol'},\n                    ],\n                },\n                {\n                    'name': 'Bob',\n                    'friends': None\n                },\n                {\n                    'name': 'Carol',\n                    'friends': None\n                },\n                {\n                    'name': 'Dave',\n                    'friends': None,\n                },\n            ]\n        ])\n\n    async def test_edgeql_scope_nested_01(self):\n        await self.assert_query_result(r'''\n            # control query Q1\n            WITH MODULE test\n            SELECT Card.element + ' ' + Card.name\n            FILTER Card.name > Card.element\n            ORDER BY Card.name;\n        ''', [\n            ['Air Djinn', 'Air Giant eagle', 'Earth Golem', 'Fire Imp',\n             'Air Sprite']\n        ])\n\n    async def test_edgeql_scope_nested_02(self):\n        await self.assert_query_result(r'''\n            # Semantically this is same as control query Q1, with lots\n            # of nested views. SELECT sets up A to be the longest\n            # common prefix to be iterated over, so the rest of views\n            # work out because they all refer to a singleton with same\n            # value as A.\n            WITH\n                MODULE test,\n                A := Card\n            SELECT\n                A.element + ' ' + (WITH B := A SELECT B).name\n            FILTER (\n                WITH C := A\n                SELECT (\n                    WITH D := C\n                    SELECT D.name\n                ) > C.element\n            )\n            ORDER BY\n                (WITH E := A SELECT E.name);\n        ''', [\n            ['Air Djinn', 'Air Giant eagle', 'Earth Golem', 'Fire Imp',\n             'Air Sprite'],\n        ])\n\n    async def test_edgeql_scope_nested_03(self):\n        await self.assert_query_result(r'''\n            # semantically same as control query Q1, with lots of\n            # nested views\n            WITH\n                MODULE test,\n                A := Card\n            SELECT\n                A.element + ' ' + (WITH B := A SELECT B).name\n            FILTER (\n                WITH C := A\n                SELECT (\n                    WITH D := A\n                    SELECT D.name\n                ) > C.element\n            )\n            ORDER BY\n                (WITH E := A SELECT E.name);\n        ''', [\n            ['Air Djinn', 'Air Giant eagle', 'Earth Golem', 'Fire Imp',\n             'Air Sprite'],\n        ])\n\n    async def test_edgeql_scope_nested_05(self):\n        await self.assert_query_result(r'''\n            WITH MODULE test\n            SELECT\n                Card {\n                    foo := Card.element + <str>count(Card.name)\n                }\n            FILTER\n                Card.name > Card.element\n            ORDER BY\n                Card.name;\n        ''', [\n            [\n                {'foo': 'Air1'},\n                {'foo': 'Air1'},\n                {'foo': 'Earth1'},\n                {'foo': 'Fire1'},\n                {'foo': 'Air1'},\n            ]\n        ])\n\n    async def test_edgeql_scope_nested_06(self):\n        await self.assert_query_result(r'''\n            # control query Q2\n            WITH MODULE test\n            # combination of element + SET OF with a common prefix\n            SELECT Card.name + <str>count(Card.owners)\n            FILTER\n                # some element filters\n                Card.name < Card.element\n                AND\n                # a SET OF filter that shares a prefix with SELECT SET\n                # OF, but is actually independent\n                count(Card.owners.friends) > 2\n            ORDER BY Card.name;\n        ''', [\n            ['Bog monster4', 'Dragon2', 'Giant turtle4']\n        ])\n\n    async def test_edgeql_scope_nested_07(self):\n        await self.assert_query_result(r'''\n            # semantically same as control query Q2, with lots of\n            # nested aliases\n            WITH\n                MODULE test,\n                A := Card\n            SELECT\n                A.name + (WITH B := A SELECT <str>count(B.owners))\n            FILTER (\n                WITH C := A\n                SELECT (\n                    WITH D := C\n                    SELECT D.name\n                ) < C.element\n                AND\n                (\n                    WITH E := A\n                    SELECT count((WITH F := E SELECT F.owners.friends)) > 2\n                )\n            )\n            ORDER BY\n                (WITH E := A SELECT E.name);\n        ''', [\n            ['Bog monster4', 'Dragon2', 'Giant turtle4']\n        ])\n\n    async def test_edgeql_scope_nested_08(self):\n        await self.assert_query_result(r'''\n            # semantically same as control query Q2, with lots of\n            # nested aliases, all referring to the top level alias\n            WITH\n                MODULE test,\n                A := Card\n            SELECT\n                A.name + (WITH B := A SELECT <str>count(B.owners))\n            FILTER (\n                WITH C := A\n                SELECT (\n                    WITH D := A\n                    SELECT D.name\n                ) < C.element\n                AND\n                (\n                    WITH E := A\n                    SELECT count((WITH F := A SELECT F.owners.friends)) > 2\n                )\n            )\n            ORDER BY\n                (WITH E := A SELECT E.name);\n        ''', [\n            ['Bog monster4', 'Dragon2', 'Giant turtle4']\n        ])\n\n    async def test_edgeql_scope_nested_09(self):\n        await self.assert_query_result(r'''\n            # control query Q3\n            WITH MODULE test\n            SELECT Card.name + <str>count(Card.owners);\n        ''', [\n            {'Imp1', 'Dragon2', 'Bog monster4', 'Giant turtle4', 'Dwarf2',\n             'Golem3', 'Sprite2', 'Giant eagle2', 'Djinn2'}\n        ])\n\n    async def test_edgeql_scope_nested_11(self):\n        await self.assert_query_result(r'''\n            # semantically same as control query Q3, except that some\n            # aliases are introduced\n            WITH MODULE test\n            SELECT Card.name + <str>count((WITH A := Card SELECT A).owners);\n\n            WITH MODULE test\n            SELECT Card.name + <str>count((WITH A := Card SELECT A.owners));\n        ''', [\n            {'Imp1', 'Dragon2', 'Bog monster4', 'Giant turtle4', 'Dwarf2',\n             'Golem3', 'Sprite2', 'Giant eagle2', 'Djinn2'},\n            {'Imp1', 'Dragon2', 'Bog monster4', 'Giant turtle4', 'Dwarf2',\n             'Golem3', 'Sprite2', 'Giant eagle2', 'Djinn2'},\n        ])\n\n    @unittest.expectedFailure\n    async def test_edgeql_scope_nested_12(self):\n        await self.assert_query_result(r'''\n            WITH\n                MODULE test\n            SELECT Card {\n                name,\n                owner := (\n                    SELECT User {\n                        # masking a real `name` link\n                        name := 'Elvis'\n                    }\n                    # this filter should be impossible with the new `name`\n                    FILTER User.name = 'Alice'\n                )\n            }\n            FILTER Card.name = 'Dragon';\n        ''', [\n            [{'name': 'Dragon', 'owner': None}]\n        ])\n\n    async def test_edgeql_scope_detached_01(self):\n        names = {'Alice', 'Bob', 'Carol', 'Dave'}\n\n        await self.assert_query_result(r\"\"\"\n            # U2 is a combination of DETACHED and non-DETACHED expression\n            WITH\n                MODULE test,\n                U2 := User.name + DETACHED User.name\n            SELECT U2 + U2;\n\n            # DETACHED is reused directly\n            WITH MODULE test\n            SELECT User.name + DETACHED User.name +\n                   User.name + DETACHED User.name;\n            \"\"\", [\n            {u + u for u in\n                (a + b\n                    for a in names\n                    for b in names)},\n            {a + b + a + c\n                for a in names\n                for b in names\n                for c in names},\n        ])\n\n    @unittest.expectedFailure\n    async def test_edgeql_scope_detached_02(self):\n        # calculate some useful base expression\n        names = await self.con.execute(r\"\"\"\n            WITH MODULE test\n            SELECT User.name + <str>count(User.deck);\n        \"\"\")\n        names = names[0]\n\n        await self.assert_query_result(r\"\"\"\n            # Let's say we need a tournament where everybody will play\n            # with everybody twice.\n            WITH\n                MODULE test,\n                # calculate some expression (\"full\" name)\n                U0 := User.name + <str>count(User.deck),\n                # make a copy of U0 so that we can do cross product\n                U1 := U0\n            SELECT U0 + ' vs ' + U1\n            # get rid of players matching themselves\n            FILTER U0 != U1;\n        \"\"\", [\n            {f'{a} vs {b}' for a in names for b in names if a != b},\n        ])\n\n    async def test_edgeql_scope_detached_03(self):\n        names = {'Alice', 'Bob', 'Carol', 'Dave'}\n\n        # No good narrative here, just a bigger cross-product\n        # computed in straight-forward and alternative ways.\n        await self.assert_query_result(r\"\"\"\n            WITH\n                MODULE test,\n                # make 3 copies of User.name\n                U0 := DETACHED User.name,\n                U1 := DETACHED User.name,\n                U2 := DETACHED User.name\n            SELECT User.name + U0 + U1 + U2;\n\n            # same thing, but building it up differently\n            WITH\n                MODULE test,\n                # calculate some expression (\"full\" name)\n                U0 := User.name,\n                # make that expression DETACHED so that we can do\n                # cross product\n                U1 := U0,\n                # cross product of players\n                U2 := U0 + U1,\n                # a copy of the players cross product\n                U3 := U2\n            # compute what is effectively a cross product of a cross\n            # product of names (expecting 256 results)\n            SELECT U2 + U3;\n            \"\"\", [\n            {a + b + c + d\n                for a in names\n                for b in names\n                for c in names\n                for d in names},\n            {a + b + c + d\n                for a in names\n                for b in names\n                for c in names\n                for d in names},\n        ])\n\n    async def test_edgeql_scope_detached_04(self):\n        # Natural, but incorrect way of getting a bunch of friends\n        # filtered by @nickname.\n        with self.assertRaisesRegex(\n                exc.EdgeQLError,\n                r\"'User' changes the interpretation of 'User'\"):\n            await self.con.execute(r\"\"\"\n                WITH MODULE test\n                SELECT User.friends\n                FILTER User.friends@nickname = 'Firefighter';\n            \"\"\")\n\n        # The above query is illegal, but the reason why may be\n        # more obvious with the equivalent query below.\n        with self.assertRaisesRegex(\n                exc.EdgeQLError,\n                r\"'User' changes the interpretation of 'User'\"):\n            await self.con.execute(r\"\"\"\n                WITH MODULE test\n                SELECT User.friends\n                FILTER (\n                    # create an independent link target set\n                    WITH F := DETACHED User.friends\n                    # explicitly connect it back to our User\n                    SELECT F\n                    FILTER F.<friends = User\n                ).friends@nickname = 'Firefighter';\n                \"\"\")\n\n    async def test_edgeql_scope_detached_05(self):\n        await self.assert_query_result(r\"\"\"\n            # Natural syntax for filtering friends based on nickname:\n            WITH MODULE test\n            SELECT User {\n                name,\n                friends: {\n                    name\n                } FILTER @nickname = 'Firefighter'\n            }\n            ORDER BY .name;\n\n            # Alternative natural syntax for filtering friends based\n            # on nickname:\n            WITH MODULE test\n            SELECT User {\n                name,\n                fr := (\n                    SELECT User.friends {\n                        name\n                    }\n                    FILTER @nickname = 'Firefighter'\n                )\n            }\n            ORDER BY .name;\n\n            # The above query is legal, but the reason why may be more\n            # obvious with the equivalent query below.\n            WITH MODULE test\n            SELECT User {\n                name,\n                fr := (\n                    WITH F0 := (\n                        WITH F1 := DETACHED User.friends\n                        SELECT F1\n                        # explicitly connect it back to our User\n                        FILTER .<friends = User\n                    )\n                    SELECT F0 {name}\n                    FILTER .<friends@nickname = 'Firefighter'\n                )\n            }\n            ORDER BY .name;\n            \"\"\", [\n            [\n                {'name': 'Alice', 'friends': [{'name': 'Carol'}]},\n                {'name': 'Bob', 'friends': None},\n                {'name': 'Carol', 'friends': None},\n                {'name': 'Dave', 'friends': None},\n            ],\n            [\n                {'name': 'Alice', 'fr': [{'name': 'Carol'}]},\n                {'name': 'Bob', 'fr': None},\n                {'name': 'Carol', 'fr': None},\n                {'name': 'Dave', 'fr': None},\n            ],\n            [\n                {'name': 'Alice', 'fr': [{'name': 'Carol'}]},\n                {'name': 'Bob', 'fr': None},\n                {'name': 'Carol', 'fr': None},\n                {'name': 'Dave', 'fr': None},\n            ],\n        ])\n\n    async def test_edgeql_scope_detached_06(self):\n        # this is very similar to test_edgeql_scope_filter_01\n        await self.assert_query_result(r'''\n            WITH\n                MODULE test,\n                U2 := DETACHED User\n            SELECT User {\n                name,\n                foo := (SELECT U2 {name} ORDER BY U2.name)\n            }\n            # the FILTER clause is irrelevant because it's in a\n            # parallel scope to the other mentions of U2\n            FILTER U2.name = 'Alice'\n            ORDER BY User.name;\n        ''', [\n            [\n                {\n                    'name': 'Alice',\n                    'foo': [\n                        {'name': 'Alice'},\n                        {'name': 'Bob'},\n                        {'name': 'Carol'},\n                        {'name': 'Dave'},\n                    ],\n                },\n                {\n                    'name': 'Bob',\n                    'foo': [\n                        {'name': 'Alice'},\n                        {'name': 'Bob'},\n                        {'name': 'Carol'},\n                        {'name': 'Dave'},\n                    ],\n                },\n                {\n                    'name': 'Carol',\n                    'foo': [\n                        {'name': 'Alice'},\n                        {'name': 'Bob'},\n                        {'name': 'Carol'},\n                        {'name': 'Dave'},\n                    ],\n                },\n                {\n                    'name': 'Dave',\n                    'foo': [\n                        {'name': 'Alice'},\n                        {'name': 'Bob'},\n                        {'name': 'Carol'},\n                        {'name': 'Dave'},\n                    ],\n                },\n            ]\n        ])\n\n    async def test_edgeql_scope_union_01(self):\n        await self.assert_sorted_query_result(r'''\n            # UNION and `{...}` should create SET OF scoped operands,\n            # therefore `count` should operate on the entire set\n            WITH MODULE test\n            SELECT len(User.name) UNION count(User);\n\n            WITH MODULE test\n            SELECT {len(User.name), count(User)};\n        ''', lambda x: x, [\n            [3, 4, 4, 5, 5],\n            [3, 4, 4, 5, 5],\n        ])\n\n    async def test_edgeql_scope_union_02(self):\n        await self.assert_sorted_query_result(r'''\n            # UNION and `{...}` should create SET OF scoped operands,\n            # therefore FILTER should not be effective\n            WITH MODULE test\n            SELECT len(User.name)\n            FILTER User.name > 'C';\n\n            WITH MODULE test\n            SELECT {len(User.name)}\n            FILTER User.name > 'C';\n\n            WITH MODULE test\n            SELECT {len(User.name), count(User)}\n            FILTER User.name > 'C';\n        ''', lambda x: x, [\n            [4, 5],\n            [3, 4, 5, 5],\n            [3, 4, 4, 5, 5],\n        ])\n\n    async def test_edgeql_scope_computables_01(self):\n        # Test that expressions in schema link computables\n        # do not leak out into the query.\n        await self.assert_query_result(r\"\"\"\n            WITH\n                MODULE test\n            SELECT x := (User.name, User.deck.name, User.deck_cost)\n            FILTER x.0 = 'Alice'\n            ORDER BY x.1;\n        \"\"\", [[\n            ['Alice', 'Bog monster', 11],\n            ['Alice', 'Dragon', 11],\n            ['Alice', 'Giant turtle', 11],\n            ['Alice', 'Imp', 11],\n        ]])\n\n        await self.assert_query_result(r\"\"\"\n            WITH\n                MODULE test\n            SELECT x := (User.name, User.deck.name, sum(User.deck.cost))\n            FILTER x.0 = 'Alice'\n            ORDER BY x.1;\n        \"\"\", [[\n            ['Alice', 'Bog monster', 2],\n            ['Alice', 'Dragon', 5],\n            ['Alice', 'Giant turtle', 3],\n            ['Alice', 'Imp', 1],\n        ]])\n\n    async def test_edgeql_scope_computables_02(self):\n        # Test that expressions in view link computables\n        # do not leak out into the query.\n        await self.assert_query_result(r\"\"\"\n            WITH\n                MODULE test\n            SELECT Card {\n                name,\n                alice := (SELECT User FILTER User.name = 'Alice')\n            } FILTER Card.alice != User AND Card.name = 'Bog monster';\n        \"\"\", [[\n            {'name': 'Bog monster'}\n        ]])\n\n    async def test_edgeql_scope_with_01(self):\n        # Test that same symbol can be re-used in WITH block.\n        await self.assert_query_result(r\"\"\"\n            WITH\n                MODULE test,\n                User := User,\n                User := User,\n                User := User\n            SELECT User.name;\n\n            WITH\n                MODULE test,\n                User := User,\n                User := Card,\n                User := User\n            # this is a Card.name now\n            SELECT User.name;\n\n            WITH\n                MODULE test,\n                User := User,\n                User := User.deck,\n                User := User.element,\n                User := User\n            # this is a User.deck.element now\n            SELECT DISTINCT User;\n        \"\"\", [\n            {'Alice', 'Bob', 'Carol', 'Dave'},\n            {'Imp', 'Dragon', 'Bog monster', 'Giant turtle', 'Dwarf', 'Golem',\n             'Sprite', 'Giant eagle', 'Djinn'},\n            {'Fire', 'Water', 'Earth', 'Air'},\n        ])\n", "hunk": "@@ -371,8 +371,7 @@ class TestEdgeQLScope(tb.QueryTestCase):\n             ]\n         ])\n \n-    @unittest.expectedFailure\n-    async def test_edgeql_scope_tuple_08(self):\n+    async def test_edgeql_scope_tuple_09(self):\n         await self.assert_query_result(r'''\n             # compare to test_edgeql_scope_filter_03 to see how it\n             # works out without tuples\n", "comment": "`FILTER` should refer to `_.1.name` and this will pass.", "ids": ["6680", "7bc36ba89b448478d3637015019df88dd22214e7", "29c9fe24b699ddcf1ead5457216ff5a408a7c745"], "repo": "edgedb/edgedb", "ghid": 198, "old": "             ]\n         ])\n-    @unittest.expectedFailure\n-    async def test_edgeql_scope_tuple_08(self):\n         await self.assert_query_result(r'''\n             # compare to test_edgeql_scope_filter_03 to see how it\n             # works out without tuples", "new": "             ]\n         ])\n+    async def test_edgeql_scope_tuple_09(self):\n         await self.assert_query_result(r'''\n             # compare to test_edgeql_scope_filter_03 to see how it\n             # works out without tuples", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -10,18 +10,14 @@\n \n from kombu.utils.encoding import bytes_to_str, str_to_bytes\n \n-from celery.five import bytes_if_py2, items, reraise, string_t\n-\n+from celery.five import bytes_if_py2, items, string_t", "oldf": "\"\"\"Utilities for safely pickling exceptions.\"\"\"\nimport datetime\nimport numbers\nimport sys\nfrom base64 import b64decode as base64decode\nfrom base64 import b64encode as base64encode\nfrom functools import partial\nfrom inspect import getmro\nfrom itertools import takewhile\n\nfrom kombu.utils.encoding import bytes_to_str, str_to_bytes\n\nfrom celery.five import bytes_if_py2, items, string_t\nfrom .encoding import safe_repr\n\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle  # noqa\n\n__all__ = (\n    'UnpickleableExceptionWrapper', 'subclass_exception',\n    'find_pickleable_exception', 'create_exception_cls',\n    'get_pickleable_exception', 'get_pickleable_etype',\n    'get_pickled_exception', 'strtobool',\n)\n\n#: List of base classes we probably don't want to reduce to.\nunwanted_base_classes = (Exception, BaseException, object)\n\n\nSTRTOBOOL_DEFAULT_TABLE = {'false': False, 'no': False, '0': False,\n                           'true': True, 'yes': True, '1': True,\n                           'on': True, 'off': False}\n\n\ndef subclass_exception(name, parent, module):  # noqa\n    \"\"\"Create new exception class.\"\"\"\n    return type(bytes_if_py2(name), (parent,), {'__module__': module})\n\n\ndef find_pickleable_exception(exc, loads=pickle.loads,\n                              dumps=pickle.dumps):\n    \"\"\"Find first pickleable exception base class.\n\n    With an exception instance, iterate over its super classes (by MRO)\n    and find the first super exception that's pickleable.  It does\n    not go below :exc:`Exception` (i.e., it skips :exc:`Exception`,\n    :class:`BaseException` and :class:`object`).  If that happens\n    you should use :exc:`UnpickleableException` instead.\n\n    Arguments:\n        exc (BaseException): An exception instance.\n        loads: decoder to use.\n        dumps: encoder to use\n\n    Returns:\n        Exception: Nearest pickleable parent exception class\n            (except :exc:`Exception` and parents), or if the exception is\n            pickleable it will return :const:`None`.\n    \"\"\"\n    exc_args = getattr(exc, 'args', [])\n    for supercls in itermro(exc.__class__, unwanted_base_classes):\n        try:\n            superexc = supercls(*exc_args)\n            loads(dumps(superexc))\n        except Exception:  # pylint: disable=broad-except\n            pass\n        else:\n            return superexc\n\n\ndef itermro(cls, stop):\n    return takewhile(lambda sup: sup not in stop, getmro(cls))\n\n\ndef create_exception_cls(name, module, parent=None):\n    \"\"\"Dynamically create an exception class.\"\"\"\n    if not parent:\n        parent = Exception\n    return subclass_exception(name, parent, module)\n\n\ndef ensure_serializable(items, encoder):\n    \"\"\"Ensure items will serialize.\n\n    For a given list of arbitrary objects, return the object\n    or a string representation, safe for serialization.\n\n    Arguments:\n        items (Iterable[Any]): Objects to serialize.\n        encoder (Callable): Callable function to serialize with.\n    \"\"\"\n    safe_exc_args = []\n    for arg in items:\n        try:\n            encoder(arg)\n            safe_exc_args.append(arg)\n        except Exception:  # pylint: disable=broad-except\n            safe_exc_args.append(safe_repr(arg))\n    return tuple(safe_exc_args)\n\n\nclass UnpickleableExceptionWrapper(Exception):\n    \"\"\"Wraps unpickleable exceptions.\n\n    Arguments:\n        exc_module (str): See :attr:`exc_module`.\n        exc_cls_name (str): See :attr:`exc_cls_name`.\n        exc_args (Tuple[Any, ...]): See :attr:`exc_args`.\n\n    Example:\n        >>> def pickle_it(raising_function):\n        ...     try:\n        ...         raising_function()\n        ...     except Exception as e:\n        ...         exc = UnpickleableExceptionWrapper(\n        ...             e.__class__.__module__,\n        ...             e.__class__.__name__,\n        ...             e.args,\n        ...         )\n        ...         pickle.dumps(exc)  # Works fine.\n    \"\"\"\n\n    #: The module of the original exception.\n    exc_module = None\n\n    #: The name of the original exception class.\n    exc_cls_name = None\n\n    #: The arguments for the original exception.\n    exc_args = None\n\n    def __init__(self, exc_module, exc_cls_name, exc_args, text=None):\n        safe_exc_args = ensure_serializable(exc_args, pickle.dumps)\n        self.exc_module = exc_module\n        self.exc_cls_name = exc_cls_name\n        self.exc_args = safe_exc_args\n        self.text = text\n        Exception.__init__(self, exc_module, exc_cls_name, safe_exc_args, text)\n\n    def restore(self):\n        return create_exception_cls(self.exc_cls_name,\n                                    self.exc_module)(*self.exc_args)\n\n    def __str__(self):\n        return self.text\n\n    @classmethod\n    def from_exception(cls, exc):\n        return cls(exc.__class__.__module__,\n                   exc.__class__.__name__,\n                   getattr(exc, 'args', []),\n                   safe_repr(exc))\n\n\ndef get_pickleable_exception(exc):\n    \"\"\"Make sure exception is pickleable.\"\"\"\n    try:\n        pickle.loads(pickle.dumps(exc))\n    except Exception:  # pylint: disable=broad-except\n        pass\n    else:\n        return exc\n    nearest = find_pickleable_exception(exc)\n    if nearest:\n        return nearest\n    return UnpickleableExceptionWrapper.from_exception(exc)\n\n\ndef get_pickleable_etype(cls, loads=pickle.loads, dumps=pickle.dumps):\n    \"\"\"Get pickleable exception type.\"\"\"\n    try:\n        loads(dumps(cls))\n    except Exception:  # pylint: disable=broad-except\n        return Exception\n    else:\n        return cls\n\n\ndef get_pickled_exception(exc):\n    \"\"\"Reverse of :meth:`get_pickleable_exception`.\"\"\"\n    if isinstance(exc, UnpickleableExceptionWrapper):\n        return exc.restore()\n    return exc\n\n\ndef b64encode(s):\n    return bytes_to_str(base64encode(str_to_bytes(s)))\n\n\ndef b64decode(s):\n    return base64decode(str_to_bytes(s))\n\n\ndef strtobool(term, table=None):\n    \"\"\"Convert common terms for true/false to bool.\n\n    Examples (true/false/yes/no/on/off/1/0).\n    \"\"\"\n    if table is None:\n        table = STRTOBOOL_DEFAULT_TABLE\n    if isinstance(term, string_t):\n        try:\n            return table[term.lower()]\n        except KeyError:\n            raise TypeError(f'Cannot coerce {term!r} to type bool')\n    return term\n\n\ndef _datetime_to_json(dt):\n    # See \"Date Time String Format\" in the ECMA-262 specification.\n    if isinstance(dt, datetime.datetime):\n        r = dt.isoformat()\n        if dt.microsecond:\n            r = r[:23] + r[26:]\n        if r.endswith('+00:00'):\n            r = r[:-6] + 'Z'\n        return r\n    elif isinstance(dt, datetime.time):\n        r = dt.isoformat()\n        if dt.microsecond:\n            r = r[:12]\n        return r\n    else:\n        return dt.isoformat()\n\n\ndef jsonify(obj,\n            builtin_types=(numbers.Real, string_t), key=None,\n            keyfilter=None,\n            unknown_type_filter=None):\n    \"\"\"Transform object making it suitable for json serialization.\"\"\"\n    from kombu.abstract import Object as KombuDictType\n    _jsonify = partial(jsonify, builtin_types=builtin_types, key=key,\n                       keyfilter=keyfilter,\n                       unknown_type_filter=unknown_type_filter)\n\n    if isinstance(obj, KombuDictType):\n        obj = obj.as_dict(recurse=True)\n\n    if obj is None or isinstance(obj, builtin_types):\n        return obj\n    elif isinstance(obj, (tuple, list)):\n        return [_jsonify(v) for v in obj]\n    elif isinstance(obj, dict):\n        return {\n            k: _jsonify(v, key=k) for k, v in items(obj)\n            if (keyfilter(k) if keyfilter else 1)\n        }\n    elif isinstance(obj, (datetime.date, datetime.time)):\n        return _datetime_to_json(obj)\n    elif isinstance(obj, datetime.timedelta):\n        return str(obj)\n    else:\n        if unknown_type_filter is None:\n            raise ValueError(\n                f'Unsupported type: {type(obj)!r} {obj!r} (parent: {key})'\n            )\n        return unknown_type_filter(obj)\n\n\nfrom vine.five import exec_\n_raise_with_context = None  # for flake8\nexec_(\"\"\"def _raise_with_context(exc, ctx): raise exc from ctx\"\"\")\n\ndef raise_with_context(exc):\n    exc_info = sys.exc_info()\n    if not exc_info:\n        raise exc\n    elif exc_info[1] is exc:\n        raise\n    _raise_with_context(exc, exc_info[1])\n", "hunk": "@@ -10,7 +10,9 @@ from itertools import takewhile\n \n from kombu.utils.encoding import bytes_to_str, str_to_bytes\n \n-from celery.five import bytes_if_py2, items, string_t\n+from celery.five import (bytes_if_py2, items, python_2_unicode_compatible,\n+                         reraise, string_t)\n+\n from .encoding import safe_repr\n \n try:\n", "comment": "Perhaps out of scope for this PR, but I wonder if we still need `bytes_if_py2` here.", "ids": ["19683", "4234d5f0362121ec4c83a7f73088418722c65c2f", "7cf08574449f7c051fca7fac44f41d56385466a7"], "repo": "celery/celery", "ghid": 5954, "old": " from kombu.utils.encoding import bytes_to_str, str_to_bytes\n-from celery.five import bytes_if_py2, items, string_t\n from .encoding import safe_repr\n try:", "new": " from kombu.utils.encoding import bytes_to_str, str_to_bytes\n+from celery.five import (bytes_if_py2, items, python_2_unicode_compatible,\n+                         reraise, string_t)\n+\n from .encoding import safe_repr\n try:", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -52,15 +53,21 @@ def generator_fun():\n ]\n \n \n-@raises(TypeError)\n def check_source_build(source):\n     pipe = create_pipe(source, 'cpu', 10, py_num_workers=4, py_start_method='spawn', parallel=True)\n     pipe.build()\n \n \n def test_wrong_source():\n-    for source in disallowed_sources:\n-        yield check_source_build, source\n+    common_msg = \"External Source in parallel mode (when `parallel=True`) accepts as `source` only *. Got {} instead\"\n+    expected_error_msgs = (\n+        common_msg.format(\"a callable that does not accept arguments\"),\n+        \"External source callback must be a callable with 0 or 1 argument\",\n+        common_msg.format(\"an iterable\"),\n+        common_msg.format(\"a generator function\"))", "oldf": "# Copyright (c) 2020-2021, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom nose.tools import with_setup\nfrom nose_utils import raises\n\nfrom test_pool_utils import *\nfrom test_utils import compare_pipelines\nfrom test_external_source_parallel_utils import *\n\n\ndef no_arg_fun():\n    pass\n\n\ndef multi_arg_fun(a, b, c):\n    pass\n\n\nclass Iterable:\n    def __init__(self):\n        pass\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return np.full((2, 2), 42)\n\n\ndef generator_fun():\n    while True:\n        yield np.full((2, 2), 42)\n\n\ndisallowed_sources = [\n    no_arg_fun,\n    multi_arg_fun,\n    Iterable(),\n    generator_fun\n]\n\n\ndef check_source_build(source):\n    pipe = create_pipe(source, 'cpu', 10, py_num_workers=4, py_start_method='spawn', parallel=True)\n    pipe.build()\n\n\ndef test_wrong_source():\n    common_msg = \"External Source in parallel mode (when `parallel=True`) accepts as `source` only *. Got {} instead\"\n    expected_error_msgs = (\n        common_msg.format(\"a callable that does not accept arguments\"),\n        \"External source callback must be a callable with 0 or 1 argument\",\n        common_msg.format(\"an iterable\"),\n        common_msg.format(\"a generator function\"))\n    assert len(disallowed_sources) == len(expected_error_msgs)\n    for source, error_msg in zip(disallowed_sources, expected_error_msgs):\n        yield raises(TypeError, error_msg)(check_source_build), source\n\n\n# Test that we can launch several CPU-only pipelines by fork as we don't touch CUDA context.\n@with_setup(setup_function, teardown_function)\ndef test_parallel_fork_cpu_only():\n    global pipe_processes\n    pipeline_pairs = 4\n    batch_size = 10\n    iters = 40\n    callback = ExtCallback((4, 5), iters * batch_size, np.int32)\n    parallel_pipes = [(create_pipe(callback, 'cpu', batch_size, py_num_workers=4,\n                                   py_start_method='fork', parallel=True, device_id=None),\n                       create_pipe(callback, 'cpu', batch_size, py_num_workers=4,\n                                   py_start_method='fork', parallel=True, device_id=None))\n                      for i in range(pipeline_pairs)]\n    for pipe0, pipe1 in parallel_pipes:\n        pipe0.build()\n        pipe1.build()\n        capture_processes(pipe0._py_pool)\n        capture_processes(pipe1._py_pool)\n        compare_pipelines(pipe0, pipe1, batch_size, iters)\n\n@with_setup(setup_function, teardown_function)\ndef test_parallel_no_workers():\n    batch_size = 10\n    iters = 4\n    callback = ExtCallback((4, 5), iters * batch_size, np.int32)\n    parallel_pipe = create_pipe(callback, 'cpu', batch_size, py_num_workers=0,\n                                py_start_method='spawn', parallel=True, device_id=None)\n    parallel_pipe.build()\n    capture_processes(parallel_pipe._py_pool)\n    assert parallel_pipe._py_pool is None\n    assert parallel_pipe._py_pool_started == False\n\n\n@with_setup(setup_function, teardown_function)\ndef test_parallel_fork():\n    epoch_size = 250\n    callback = ExtCallback((4, 5), epoch_size, np.int32)\n    pipes = [(\n        create_pipe(\n            callback, 'cpu', batch_size, py_num_workers=num_workers, py_start_method='fork',\n            parallel=True),\n        create_pipe(callback, 'cpu', batch_size, parallel=False),\n        dtype, batch_size)\n        for dtype in [np.float32, np.int16]\n        for num_workers in [1, 3, 4] for batch_size in [1, 16, 150, 250]]\n    for parallel_pipe, _, _, _ in pipes:\n        parallel_pipe.start_py_workers()\n    for parallel_pipe, pipe, dtype, batch_size in pipes:\n        yield check_callback, parallel_pipe, pipe, epoch_size, batch_size, dtype\n    # test that another pipline with forking initialization fails as there is CUDA contexts already initialized\n    parallel_pipe = create_pipe(callback, 'cpu', 16, py_num_workers=4,\n                                py_start_method='fork', parallel=True)\n    yield raises(RuntimeError, \"Cannot fork a process when there is a CUDA context already bound to the process.\")(\n        build_and_run_pipeline), parallel_pipe, 1\n\n@with_setup(setup_function, teardown_function)\ndef test_dtypes():\n    yield from check_spawn_with_callback(ExtCallback)\n\n@with_setup(setup_function, teardown_function)\ndef test_random_data():\n    yield from check_spawn_with_callback(ExtCallback, shapes=[(100, 40, 3), (8, 64, 64, 3)], random_data=True)\n\n@with_setup(setup_function, teardown_function)\ndef test_randomly_shaped_data():\n    yield from check_spawn_with_callback(ExtCallback, shapes=[(100, 40, 3), (8, 64, 64, 3)], random_data=True, random_shape=True)\n\n@with_setup(setup_function, teardown_function)\ndef test_num_outputs():\n    yield from check_spawn_with_callback(ExtCallbackMultipleOutputs, ExtCallbackMultipleOutputs, num_outputs=2, dtypes=[np.uint8, np.float])\n\n@with_setup(setup_function, teardown_function)\ndef test_tensor_cpu():\n    yield from check_spawn_with_callback(ExtCallbackTensorCPU)\n\n@with_setup(setup_function, teardown_function)\ndef test_exception_propagation():\n    for raised, expected in [(StopIteration, StopIteration), (CustomException, Exception)]:\n        callback = ExtCallback((4, 4), 250, np.int32, exception_class=raised)\n        for num_workers in [1, 4]:\n            for batch_size in [1, 15, 150]:\n                pipe = create_pipe(\n                    callback, 'cpu', batch_size, py_num_workers=num_workers,\n                    py_start_method='spawn', parallel=True)\n                yield raises(expected)(build_and_run_pipeline), pipe, None, raised, expected\n\n@with_setup(setup_function, teardown_function)\ndef test_stop_iteration_resume():\n    callback = ExtCallback((4, 4), 250, 'int32')\n    layout = \"XY\"\n    for num_workers in [1, 4]:\n        for batch_size in [1, 15, 150]:\n            pipe = create_pipe(callback, 'cpu', batch_size, layout=layout,\n                               py_num_workers=num_workers, py_start_method='spawn', parallel=True)\n            yield check_stop_iteration_resume, pipe, batch_size, layout\n\n@with_setup(setup_function, teardown_function)\ndef test_layout():\n    for layout, dims in zip([\"X\", \"XY\", \"XYZ\"], ((4,), (4, 4), (4, 4, 4))):\n        callback = ExtCallback(dims, 1024, 'int32')\n        for num_workers in [1, 4]:\n            for batch_size in [1, 256, 600]:\n                pipe = create_pipe(\n                    callback, 'cpu', batch_size, layout=layout, py_num_workers=num_workers,\n                    py_start_method='spawn', parallel=True)\n                yield check_layout, pipe, layout\n\nclass ext_cb():\n    def __init__(self, name, shape):\n        self.name = name\n        self.shape = shape\n    def __call__(self, sinfo):\n        return np.full(self.shape, sinfo.idx_in_epoch, dtype=np.int32)\n\ndef _test_vs_non_parallel(shape):\n    bs = 50\n    pipe = dali.Pipeline(batch_size=bs, device_id=None, num_threads=5, py_num_workers=14, py_start_method='spawn')\n    with pipe:\n        ext_seq = dali.fn.external_source(ext_cb(\"cb 1\", shape), batch=False, parallel=False)\n        ext_par = dali.fn.external_source(ext_cb(\"cb 2\", shape), batch=False, parallel=True)\n        pipe.set_outputs(ext_seq, ext_par)\n    pipe.build()\n    for i in range(10):\n        seq, par = pipe.run()\n        for j in range(bs):\n            s = seq.at(j)\n            p = par.at(j)\n            assert np.array_equal(s, p)\n\ndef test_vs_non_parallel():\n    for shape in [[], [10], [100, 100, 100]]:\n        yield _test_vs_non_parallel, shape\n\n\ndef ext_cb2(sinfo):\n    return np.array([sinfo.idx_in_epoch, sinfo.idx_in_batch, sinfo.iteration], dtype=np.int32)\n\ndef test_discard():\n    bs = 5\n    pipe = dali.Pipeline(batch_size=bs, device_id=None, num_threads=5, py_num_workers=4, py_start_method='spawn')\n    with pipe:\n        sh = []\n        ext1 = dali.fn.external_source([[np.float32(i) for i in range(bs)]]*3, cycle='raise')\n        ext2 = dali.fn.external_source(ext_cb2, batch=False, parallel=True)\n        ext3 = dali.fn.external_source(ext_cb2, batch=False, parallel=False)\n        pipe.set_outputs(ext1, ext2, ext3)\n    pipe.build()\n    sample_in_epoch = 0\n    iteration = 0\n    for i in range(10):\n        try:\n            e1, e2, e3 = pipe.run()\n            for i in range(bs):\n                assert e1.at(i) == i\n                assert np.array_equal(e2.at(i), np.array([sample_in_epoch, i, iteration]))\n                assert np.array_equal(e3.at(i), np.array([sample_in_epoch, i, iteration]))\n                sample_in_epoch += 1\n            iteration += 1\n        except StopIteration:\n            sample_in_epoch = 0\n            iteration = 0\n            pipe.reset()\n", "hunk": "@@ -60,11 +60,11 @@ def check_source_build(source):\n \n def test_wrong_source():\n     common_msg = \"External Source in parallel mode (when `parallel=True`) accepts as `source` only *. Got {} instead\"\n-    expected_error_msgs = (\n+    expected_error_msgs = [\n         common_msg.format(\"a callable that does not accept arguments\"),\n         \"External source callback must be a callable with 0 or 1 argument\",\n         common_msg.format(\"an iterable\"),\n-        common_msg.format(\"a generator function\"))\n+        common_msg.format(\"a generator function\")]\n     assert len(disallowed_sources) == len(expected_error_msgs)\n     for source, error_msg in zip(disallowed_sources, expected_error_msgs):\n         yield raises(TypeError, error_msg)(check_source_build), source\n", "comment": "Any reason for using tuple instead of list of the expected error msgs?", "ids": ["55321", "c19ecaccc465dfe7ccb769b34d5820b6835a0d1e", "1f50df513fa9ad89a6d49a84e4cfe11831eee644"], "repo": "NVIDIA/DALI", "ghid": 3324, "old": " def test_wrong_source():\n     common_msg = \"External Source in parallel mode (when `parallel=True`) accepts as `source` only *. Got {} instead\"\n-    expected_error_msgs = (\n         common_msg.format(\"a callable that does not accept arguments\"),\n         \"External source callback must be a callable with 0 or 1 argument\",\n         common_msg.format(\"an iterable\"),\n-        common_msg.format(\"a generator function\"))\n     assert len(disallowed_sources) == len(expected_error_msgs)\n     for source, error_msg in zip(disallowed_sources, expected_error_msgs):\n         yield raises(TypeError, error_msg)(check_source_build), source", "new": " def test_wrong_source():\n     common_msg = \"External Source in parallel mode (when `parallel=True`) accepts as `source` only *. Got {} instead\"\n+    expected_error_msgs = [\n         common_msg.format(\"a callable that does not accept arguments\"),\n         \"External source callback must be a callable with 0 or 1 argument\",\n         common_msg.format(\"an iterable\"),\n+        common_msg.format(\"a generator function\")]\n     assert len(disallowed_sources) == len(expected_error_msgs)\n     for source, error_msg in zip(disallowed_sources, expected_error_msgs):\n         yield raises(TypeError, error_msg)(check_source_build), source", "lang": "py", "norm_lang": "python"}
{"old_hunk": "@@ -20,7 +20,7 @@ subject to the following restrictions:\n #include \"LinearMath/btSerializer.h\"\n #include \"BulletDynamics/Featherstone/btMultiBodyLinkCollider.h\"\n #include \"BulletDynamics/Featherstone/btMultiBodyConstraint.h\"\n-\n+#include <iostream>", "oldf": "/*\nBullet Continuous Collision Detection and Physics Library\nCopyright (c) 2003-2006 Erwin Coumans  http://continuousphysics.com/Bullet/\n\nThis software is provided 'as-is', without any express or implied warranty.\nIn no event will the authors be held liable for any damages arising from the use of this software.\nPermission is granted to anyone to use this software for any purpose,\nincluding commercial applications, and to alter it and redistribute it freely,\nsubject to the following restrictions:\n\n1. The origin of this software must not be misrepresented; you must not claim that you wrote the original software. If you use this software in a product, an acknowledgment in the product documentation would be appreciated but is not required.\n2. Altered source versions must be plainly marked as such, and must not be misrepresented as being the original software.\n3. This notice may not be removed or altered from any source distribution.\n*/\n///btSoftBody implementation by Nathanael Presson\n\n#include \"btSoftBodyInternals.h\"\n#include \"BulletSoftBody/btSoftBodySolvers.h\"\n#include \"btSoftBodyData.h\"\n#include \"LinearMath/btSerializer.h\"\n#include \"BulletDynamics/Featherstone/btMultiBodyLinkCollider.h\"\n#include \"BulletDynamics/Featherstone/btMultiBodyConstraint.h\"\n#include <iostream>\n//\nbtSoftBody::btSoftBody(btSoftBodyWorldInfo* worldInfo, int node_count, const btVector3* x, const btScalar* m)\n\t: m_softBodySolver(0), m_worldInfo(worldInfo)\n{\n\t/* Init\t\t*/\n\tinitDefaults();\n\n\t/* Default material\t*/\n\tMaterial* pm = appendMaterial();\n\tpm->m_kLST = 1;\n\tpm->m_kAST = 1;\n\tpm->m_kVST = 1;\n\tpm->m_flags = fMaterial::Default;\n\n\t/* Nodes\t\t\t*/\n\tconst btScalar margin = getCollisionShape()->getMargin();\n\tm_nodes.resize(node_count);\n\tfor (int i = 0, ni = node_count; i < ni; ++i)\n\t{\n\t\tNode& n = m_nodes[i];\n\t\tZeroInitialize(n);\n\t\tn.m_x = x ? *x++ : btVector3(0, 0, 0);\n\t\tn.m_q = n.m_x;\n\t\tn.m_im = m ? *m++ : 1;\n\t\tn.m_im = n.m_im > 0 ? 1 / n.m_im : 0;\n\t\tn.m_leaf = m_ndbvt.insert(btDbvtVolume::FromCR(n.m_x, margin), &n);\n\t\tn.m_material = pm;\n\t}\n\tupdateBounds();\n}\n\nbtSoftBody::btSoftBody(btSoftBodyWorldInfo* worldInfo)\n\t: m_worldInfo(worldInfo)\n{\n\tinitDefaults();\n}\n\nvoid btSoftBody::initDefaults()\n{\n\tm_internalType = CO_SOFT_BODY;\n\tm_cfg.aeromodel = eAeroModel::V_Point;\n\tm_cfg.kVCF = 1;\n\tm_cfg.kDG = 0;\n\tm_cfg.kLF = 0;\n\tm_cfg.kDP = 0;\n\tm_cfg.kPR = 0;\n\tm_cfg.kVC = 0;\n\tm_cfg.kDF = (btScalar)0.2;\n\tm_cfg.kMT = 0;\n\tm_cfg.kCHR = (btScalar)1.0;\n\tm_cfg.kKHR = (btScalar)0.1;\n\tm_cfg.kSHR = (btScalar)1.0;\n\tm_cfg.kAHR = (btScalar)0.7;\n\tm_cfg.kSRHR_CL = (btScalar)0.1;\n\tm_cfg.kSKHR_CL = (btScalar)1;\n\tm_cfg.kSSHR_CL = (btScalar)0.5;\n\tm_cfg.kSR_SPLT_CL = (btScalar)0.5;\n\tm_cfg.kSK_SPLT_CL = (btScalar)0.5;\n\tm_cfg.kSS_SPLT_CL = (btScalar)0.5;\n\tm_cfg.maxvolume = (btScalar)1;\n\tm_cfg.timescale = 1;\n\tm_cfg.viterations = 0;\n\tm_cfg.piterations = 1;\n\tm_cfg.diterations = 0;\n\tm_cfg.citerations = 4;\n\tm_cfg.collisions = fCollision::Default;\n\tm_pose.m_bvolume = false;\n\tm_pose.m_bframe = false;\n\tm_pose.m_volume = 0;\n\tm_pose.m_com = btVector3(0, 0, 0);\n\tm_pose.m_rot.setIdentity();\n\tm_pose.m_scl.setIdentity();\n\tm_tag = 0;\n\tm_timeacc = 0;\n\tm_bUpdateRtCst = true;\n\tm_bounds[0] = btVector3(0, 0, 0);\n\tm_bounds[1] = btVector3(0, 0, 0);\n\tm_worldTransform.setIdentity();\n\tsetSolver(eSolverPresets::Positions);\n\n\t/* Collision shape\t*/\n\t///for now, create a collision shape internally\n\tm_collisionShape = new btSoftBodyCollisionShape(this);\n\tm_collisionShape->setMargin(0.25f);\n\n\tm_initialWorldTransform.setIdentity();\n\n\tm_windVelocity = btVector3(0, 0, 0);\n\tm_restLengthScale = btScalar(1.0);\n    m_dampingCoefficient = 1;\n}\n\n//\nbtSoftBody::~btSoftBody()\n{\n\t//for now, delete the internal shape\n\tdelete m_collisionShape;\n\tint i;\n\n\treleaseClusters();\n\tfor (i = 0; i < m_materials.size(); ++i)\n\t\tbtAlignedFree(m_materials[i]);\n\tfor (i = 0; i < m_joints.size(); ++i)\n\t\tbtAlignedFree(m_joints[i]);\n}\n\n//\nbool btSoftBody::checkLink(int node0, int node1) const\n{\n\treturn (checkLink(&m_nodes[node0], &m_nodes[node1]));\n}\n\n//\nbool btSoftBody::checkLink(const Node* node0, const Node* node1) const\n{\n\tconst Node* n[] = {node0, node1};\n\tfor (int i = 0, ni = m_links.size(); i < ni; ++i)\n\t{\n\t\tconst Link& l = m_links[i];\n\t\tif ((l.m_n[0] == n[0] && l.m_n[1] == n[1]) ||\n\t\t\t(l.m_n[0] == n[1] && l.m_n[1] == n[0]))\n\t\t{\n\t\t\treturn (true);\n\t\t}\n\t}\n\treturn (false);\n}\n\n//\nbool btSoftBody::checkFace(int node0, int node1, int node2) const\n{\n\tconst Node* n[] = {&m_nodes[node0],\n\t\t\t\t\t   &m_nodes[node1],\n\t\t\t\t\t   &m_nodes[node2]};\n\tfor (int i = 0, ni = m_faces.size(); i < ni; ++i)\n\t{\n\t\tconst Face& f = m_faces[i];\n\t\tint c = 0;\n\t\tfor (int j = 0; j < 3; ++j)\n\t\t{\n\t\t\tif ((f.m_n[j] == n[0]) ||\n\t\t\t\t(f.m_n[j] == n[1]) ||\n\t\t\t\t(f.m_n[j] == n[2]))\n\t\t\t\tc |= 1 << j;\n\t\t\telse\n\t\t\t\tbreak;\n\t\t}\n\t\tif (c == 7) return (true);\n\t}\n\treturn (false);\n}\n\n//\nbtSoftBody::Material* btSoftBody::appendMaterial()\n{\n\tMaterial* pm = new (btAlignedAlloc(sizeof(Material), 16)) Material();\n\tif (m_materials.size() > 0)\n\t\t*pm = *m_materials[0];\n\telse\n\t\tZeroInitialize(*pm);\n\tm_materials.push_back(pm);\n\treturn (pm);\n}\n\n//\nvoid btSoftBody::appendNote(const char* text,\n\t\t\t\t\t\t\tconst btVector3& o,\n\t\t\t\t\t\t\tconst btVector4& c,\n\t\t\t\t\t\t\tNode* n0,\n\t\t\t\t\t\t\tNode* n1,\n\t\t\t\t\t\t\tNode* n2,\n\t\t\t\t\t\t\tNode* n3)\n{\n\tNote n;\n\tZeroInitialize(n);\n\tn.m_rank = 0;\n\tn.m_text = text;\n\tn.m_offset = o;\n\tn.m_coords[0] = c.x();\n\tn.m_coords[1] = c.y();\n\tn.m_coords[2] = c.z();\n\tn.m_coords[3] = c.w();\n\tn.m_nodes[0] = n0;\n\tn.m_rank += n0 ? 1 : 0;\n\tn.m_nodes[1] = n1;\n\tn.m_rank += n1 ? 1 : 0;\n\tn.m_nodes[2] = n2;\n\tn.m_rank += n2 ? 1 : 0;\n\tn.m_nodes[3] = n3;\n\tn.m_rank += n3 ? 1 : 0;\n\tm_notes.push_back(n);\n}\n\n//\nvoid btSoftBody::appendNote(const char* text,\n\t\t\t\t\t\t\tconst btVector3& o,\n\t\t\t\t\t\t\tNode* feature)\n{\n\tappendNote(text, o, btVector4(1, 0, 0, 0), feature);\n}\n\n//\nvoid btSoftBody::appendNote(const char* text,\n\t\t\t\t\t\t\tconst btVector3& o,\n\t\t\t\t\t\t\tLink* feature)\n{\n\tstatic const btScalar w = 1 / (btScalar)2;\n\tappendNote(text, o, btVector4(w, w, 0, 0), feature->m_n[0],\n\t\t\t   feature->m_n[1]);\n}\n\n//\nvoid btSoftBody::appendNote(const char* text,\n\t\t\t\t\t\t\tconst btVector3& o,\n\t\t\t\t\t\t\tFace* feature)\n{\n\tstatic const btScalar w = 1 / (btScalar)3;\n\tappendNote(text, o, btVector4(w, w, w, 0), feature->m_n[0],\n\t\t\t   feature->m_n[1],\n\t\t\t   feature->m_n[2]);\n}\n\n//\nvoid btSoftBody::appendNode(const btVector3& x, btScalar m)\n{\n\tif (m_nodes.capacity() == m_nodes.size())\n\t{\n\t\tpointersToIndices();\n\t\tm_nodes.reserve(m_nodes.size() * 2 + 1);\n\t\tindicesToPointers();\n\t}\n\tconst btScalar margin = getCollisionShape()->getMargin();\n\tm_nodes.push_back(Node());\n\tNode& n = m_nodes[m_nodes.size() - 1];\n\tZeroInitialize(n);\n\tn.m_x = x;\n\tn.m_q = n.m_x;\n\tn.m_im = m > 0 ? 1 / m : 0;\n\tn.m_material = m_materials[0];\n\tn.m_leaf = m_ndbvt.insert(btDbvtVolume::FromCR(n.m_x, margin), &n);\n}\n\n//\nvoid btSoftBody::appendLink(int model, Material* mat)\n{\n\tLink l;\n\tif (model >= 0)\n\t\tl = m_links[model];\n\telse\n\t{\n\t\tZeroInitialize(l);\n\t\tl.m_material = mat ? mat : m_materials[0];\n\t}\n\tm_links.push_back(l);\n}\n\n//\nvoid btSoftBody::appendLink(int node0,\n\t\t\t\t\t\t\tint node1,\n\t\t\t\t\t\t\tMaterial* mat,\n\t\t\t\t\t\t\tbool bcheckexist)\n{\n\tappendLink(&m_nodes[node0], &m_nodes[node1], mat, bcheckexist);\n}\n\n//\nvoid btSoftBody::appendLink(Node* node0,\n\t\t\t\t\t\t\tNode* node1,\n\t\t\t\t\t\t\tMaterial* mat,\n\t\t\t\t\t\t\tbool bcheckexist)\n{\n\tif ((!bcheckexist) || (!checkLink(node0, node1)))\n\t{\n\t\tappendLink(-1, mat);\n\t\tLink& l = m_links[m_links.size() - 1];\n\t\tl.m_n[0] = node0;\n\t\tl.m_n[1] = node1;\n\t\tl.m_rl = (l.m_n[0]->m_x - l.m_n[1]->m_x).length();\n\t\tm_bUpdateRtCst = true;\n\t}\n}\n\n//\nvoid btSoftBody::appendFace(int model, Material* mat)\n{\n\tFace f;\n\tif (model >= 0)\n\t{\n\t\tf = m_faces[model];\n\t}\n\telse\n\t{\n\t\tZeroInitialize(f);\n\t\tf.m_material = mat ? mat : m_materials[0];\n\t}\n\tm_faces.push_back(f);\n}\n\n//\nvoid btSoftBody::appendFace(int node0, int node1, int node2, Material* mat)\n{\n\tif (node0 == node1)\n\t\treturn;\n\tif (node1 == node2)\n\t\treturn;\n\tif (node2 == node0)\n\t\treturn;\n\n\tappendFace(-1, mat);\n\tFace& f = m_faces[m_faces.size() - 1];\n\tbtAssert(node0 != node1);\n\tbtAssert(node1 != node2);\n\tbtAssert(node2 != node0);\n\tf.m_n[0] = &m_nodes[node0];\n\tf.m_n[1] = &m_nodes[node1];\n\tf.m_n[2] = &m_nodes[node2];\n\tf.m_ra = AreaOf(f.m_n[0]->m_x,\n\t\t\t\t\tf.m_n[1]->m_x,\n\t\t\t\t\tf.m_n[2]->m_x);\n\tm_bUpdateRtCst = true;\n}\n\n//\nvoid btSoftBody::appendTetra(int model, Material* mat)\n{\n\tTetra t;\n\tif (model >= 0)\n\t\tt = m_tetras[model];\n\telse\n\t{\n\t\tZeroInitialize(t);\n\t\tt.m_material = mat ? mat : m_materials[0];\n\t}\n\tm_tetras.push_back(t);\n}\n\n//\nvoid btSoftBody::appendTetra(int node0,\n\t\t\t\t\t\t\t int node1,\n\t\t\t\t\t\t\t int node2,\n\t\t\t\t\t\t\t int node3,\n\t\t\t\t\t\t\t Material* mat)\n{\n\tappendTetra(-1, mat);\n\tTetra& t = m_tetras[m_tetras.size() - 1];\n\tt.m_n[0] = &m_nodes[node0];\n\tt.m_n[1] = &m_nodes[node1];\n\tt.m_n[2] = &m_nodes[node2];\n\tt.m_n[3] = &m_nodes[node3];\n\tt.m_rv = VolumeOf(t.m_n[0]->m_x, t.m_n[1]->m_x, t.m_n[2]->m_x, t.m_n[3]->m_x);\n\tm_bUpdateRtCst = true;\n}\n\n//\n\nvoid btSoftBody::appendAnchor(int node, btRigidBody* body, bool disableCollisionBetweenLinkedBodies, btScalar influence)\n{\n\tbtVector3 local = body->getWorldTransform().inverse() * m_nodes[node].m_x;\n\tappendAnchor(node, body, local, disableCollisionBetweenLinkedBodies, influence);\n}\n\n//\nvoid btSoftBody::appendAnchor(int node, btRigidBody* body, const btVector3& localPivot, bool disableCollisionBetweenLinkedBodies, btScalar influence)\n{\n\tif (disableCollisionBetweenLinkedBodies)\n\t{\n\t\tif (m_collisionDisabledObjects.findLinearSearch(body) == m_collisionDisabledObjects.size())\n\t\t{\n\t\t\tm_collisionDisabledObjects.push_back(body);\n\t\t}\n\t}\n\n\tAnchor a;\n\ta.m_node = &m_nodes[node];\n\ta.m_body = body;\n\ta.m_local = localPivot;\n\ta.m_node->m_battach = 1;\n\ta.m_influence = influence;\n\tm_anchors.push_back(a);\n}\n\n//\nvoid btSoftBody::appendLinearJoint(const LJoint::Specs& specs, Cluster* body0, Body body1)\n{\n\tLJoint* pj = new (btAlignedAlloc(sizeof(LJoint), 16)) LJoint();\n\tpj->m_bodies[0] = body0;\n\tpj->m_bodies[1] = body1;\n\tpj->m_refs[0] = pj->m_bodies[0].xform().inverse() * specs.position;\n\tpj->m_refs[1] = pj->m_bodies[1].xform().inverse() * specs.position;\n\tpj->m_cfm = specs.cfm;\n\tpj->m_erp = specs.erp;\n\tpj->m_split = specs.split;\n\tm_joints.push_back(pj);\n}\n\n//\nvoid btSoftBody::appendLinearJoint(const LJoint::Specs& specs, Body body)\n{\n\tappendLinearJoint(specs, m_clusters[0], body);\n}\n\n//\nvoid btSoftBody::appendLinearJoint(const LJoint::Specs& specs, btSoftBody* body)\n{\n\tappendLinearJoint(specs, m_clusters[0], body->m_clusters[0]);\n}\n\n//\nvoid btSoftBody::appendAngularJoint(const AJoint::Specs& specs, Cluster* body0, Body body1)\n{\n\tAJoint* pj = new (btAlignedAlloc(sizeof(AJoint), 16)) AJoint();\n\tpj->m_bodies[0] = body0;\n\tpj->m_bodies[1] = body1;\n\tpj->m_refs[0] = pj->m_bodies[0].xform().inverse().getBasis() * specs.axis;\n\tpj->m_refs[1] = pj->m_bodies[1].xform().inverse().getBasis() * specs.axis;\n\tpj->m_cfm = specs.cfm;\n\tpj->m_erp = specs.erp;\n\tpj->m_split = specs.split;\n\tpj->m_icontrol = specs.icontrol;\n\tm_joints.push_back(pj);\n}\n\n//\nvoid btSoftBody::appendAngularJoint(const AJoint::Specs& specs, Body body)\n{\n\tappendAngularJoint(specs, m_clusters[0], body);\n}\n\n//\nvoid btSoftBody::appendAngularJoint(const AJoint::Specs& specs, btSoftBody* body)\n{\n\tappendAngularJoint(specs, m_clusters[0], body->m_clusters[0]);\n}\n\n//\nvoid btSoftBody::addForce(const btVector3& force)\n{\n\tfor (int i = 0, ni = m_nodes.size(); i < ni; ++i) addForce(force, i);\n}\n\n//\nvoid btSoftBody::addForce(const btVector3& force, int node)\n{\n\tNode& n = m_nodes[node];\n\tif (n.m_im > 0)\n\t{\n\t\tn.m_f += force;\n\t}\n}\n\nvoid btSoftBody::addAeroForceToNode(const btVector3& windVelocity, int nodeIndex)\n{\n\tbtAssert(nodeIndex >= 0 && nodeIndex < m_nodes.size());\n\n\tconst btScalar dt = m_sst.sdt;\n\tconst btScalar kLF = m_cfg.kLF;\n\tconst btScalar kDG = m_cfg.kDG;\n\t//const btScalar kPR = m_cfg.kPR;\n\t//const btScalar kVC = m_cfg.kVC;\n\tconst bool as_lift = kLF > 0;\n\tconst bool as_drag = kDG > 0;\n\tconst bool as_aero = as_lift || as_drag;\n\tconst bool as_vaero = as_aero && (m_cfg.aeromodel < btSoftBody::eAeroModel::F_TwoSided);\n\n\tNode& n = m_nodes[nodeIndex];\n\n\tif (n.m_im > 0)\n\t{\n\t\tbtSoftBody::sMedium medium;\n\n\t\tEvaluateMedium(m_worldInfo, n.m_x, medium);\n\t\tmedium.m_velocity = windVelocity;\n\t\tmedium.m_density = m_worldInfo->air_density;\n\n\t\t/* Aerodynamics\t\t\t*/\n\t\tif (as_vaero)\n\t\t{\n\t\t\tconst btVector3 rel_v = n.m_v - medium.m_velocity;\n\t\t\tconst btScalar rel_v_len = rel_v.length();\n\t\t\tconst btScalar rel_v2 = rel_v.length2();\n\n\t\t\tif (rel_v2 > SIMD_EPSILON)\n\t\t\t{\n\t\t\t\tconst btVector3 rel_v_nrm = rel_v.normalized();\n\t\t\t\tbtVector3 nrm = n.m_n;\n\n\t\t\t\tif (m_cfg.aeromodel == btSoftBody::eAeroModel::V_TwoSidedLiftDrag)\n\t\t\t\t{\n\t\t\t\t\tnrm *= (btScalar)((btDot(nrm, rel_v) < 0) ? -1 : +1);\n\t\t\t\t\tbtVector3 fDrag(0, 0, 0);\n\t\t\t\t\tbtVector3 fLift(0, 0, 0);\n\n\t\t\t\t\tbtScalar n_dot_v = nrm.dot(rel_v_nrm);\n\t\t\t\t\tbtScalar tri_area = 0.5f * n.m_area;\n\n\t\t\t\t\tfDrag = 0.5f * kDG * medium.m_density * rel_v2 * tri_area * n_dot_v * (-rel_v_nrm);\n\n\t\t\t\t\t// Check angle of attack\n\t\t\t\t\t// cos(10\ufffd) = 0.98480\n\t\t\t\t\tif (0 < n_dot_v && n_dot_v < 0.98480f)\n\t\t\t\t\t\tfLift = 0.5f * kLF * medium.m_density * rel_v_len * tri_area * btSqrt(1.0f - n_dot_v * n_dot_v) * (nrm.cross(rel_v_nrm).cross(rel_v_nrm));\n\n\t\t\t\t\t// Check if the velocity change resulted by aero drag force exceeds the current velocity of the node.\n\t\t\t\t\tbtVector3 del_v_by_fDrag = fDrag * n.m_im * m_sst.sdt;\n\t\t\t\t\tbtScalar del_v_by_fDrag_len2 = del_v_by_fDrag.length2();\n\t\t\t\t\tbtScalar v_len2 = n.m_v.length2();\n\n\t\t\t\t\tif (del_v_by_fDrag_len2 >= v_len2 && del_v_by_fDrag_len2 > 0)\n\t\t\t\t\t{\n\t\t\t\t\t\tbtScalar del_v_by_fDrag_len = del_v_by_fDrag.length();\n\t\t\t\t\t\tbtScalar v_len = n.m_v.length();\n\t\t\t\t\t\tfDrag *= btScalar(0.8) * (v_len / del_v_by_fDrag_len);\n\t\t\t\t\t}\n\n\t\t\t\t\tn.m_f += fDrag;\n\t\t\t\t\tn.m_f += fLift;\n\t\t\t\t}\n\t\t\t\telse if (m_cfg.aeromodel == btSoftBody::eAeroModel::V_Point || m_cfg.aeromodel == btSoftBody::eAeroModel::V_OneSided || m_cfg.aeromodel == btSoftBody::eAeroModel::V_TwoSided)\n\t\t\t\t{\n\t\t\t\t\tif (m_cfg.aeromodel == btSoftBody::eAeroModel::V_TwoSided)\n\t\t\t\t\t\tnrm *= (btScalar)((btDot(nrm, rel_v) < 0) ? -1 : +1);\n\n\t\t\t\t\tconst btScalar dvn = btDot(rel_v, nrm);\n\t\t\t\t\t/* Compute forces\t*/\n\t\t\t\t\tif (dvn > 0)\n\t\t\t\t\t{\n\t\t\t\t\t\tbtVector3 force(0, 0, 0);\n\t\t\t\t\t\tconst btScalar c0 = n.m_area * dvn * rel_v2 / 2;\n\t\t\t\t\t\tconst btScalar c1 = c0 * medium.m_density;\n\t\t\t\t\t\tforce += nrm * (-c1 * kLF);\n\t\t\t\t\t\tforce += rel_v.normalized() * (-c1 * kDG);\n\t\t\t\t\t\tApplyClampedForce(n, force, dt);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid btSoftBody::addAeroForceToFace(const btVector3& windVelocity, int faceIndex)\n{\n\tconst btScalar dt = m_sst.sdt;\n\tconst btScalar kLF = m_cfg.kLF;\n\tconst btScalar kDG = m_cfg.kDG;\n\t//\tconst btScalar kPR = m_cfg.kPR;\n\t//\tconst btScalar kVC = m_cfg.kVC;\n\tconst bool as_lift = kLF > 0;\n\tconst bool as_drag = kDG > 0;\n\tconst bool as_aero = as_lift || as_drag;\n\tconst bool as_faero = as_aero && (m_cfg.aeromodel >= btSoftBody::eAeroModel::F_TwoSided);\n\n\tif (as_faero)\n\t{\n\t\tbtSoftBody::Face& f = m_faces[faceIndex];\n\n\t\tbtSoftBody::sMedium medium;\n\n\t\tconst btVector3 v = (f.m_n[0]->m_v + f.m_n[1]->m_v + f.m_n[2]->m_v) / 3;\n\t\tconst btVector3 x = (f.m_n[0]->m_x + f.m_n[1]->m_x + f.m_n[2]->m_x) / 3;\n\t\tEvaluateMedium(m_worldInfo, x, medium);\n\t\tmedium.m_velocity = windVelocity;\n\t\tmedium.m_density = m_worldInfo->air_density;\n\t\tconst btVector3 rel_v = v - medium.m_velocity;\n\t\tconst btScalar rel_v_len = rel_v.length();\n\t\tconst btScalar rel_v2 = rel_v.length2();\n\n\t\tif (rel_v2 > SIMD_EPSILON)\n\t\t{\n\t\t\tconst btVector3 rel_v_nrm = rel_v.normalized();\n\t\t\tbtVector3 nrm = f.m_normal;\n\n\t\t\tif (m_cfg.aeromodel == btSoftBody::eAeroModel::F_TwoSidedLiftDrag)\n\t\t\t{\n\t\t\t\tnrm *= (btScalar)((btDot(nrm, rel_v) < 0) ? -1 : +1);\n\n\t\t\t\tbtVector3 fDrag(0, 0, 0);\n\t\t\t\tbtVector3 fLift(0, 0, 0);\n\n\t\t\t\tbtScalar n_dot_v = nrm.dot(rel_v_nrm);\n\t\t\t\tbtScalar tri_area = 0.5f * f.m_ra;\n\n\t\t\t\tfDrag = 0.5f * kDG * medium.m_density * rel_v2 * tri_area * n_dot_v * (-rel_v_nrm);\n\n\t\t\t\t// Check angle of attack\n\t\t\t\t// cos(10\ufffd) = 0.98480\n\t\t\t\tif (0 < n_dot_v && n_dot_v < 0.98480f)\n\t\t\t\t\tfLift = 0.5f * kLF * medium.m_density * rel_v_len * tri_area * btSqrt(1.0f - n_dot_v * n_dot_v) * (nrm.cross(rel_v_nrm).cross(rel_v_nrm));\n\n\t\t\t\tfDrag /= 3;\n\t\t\t\tfLift /= 3;\n\n\t\t\t\tfor (int j = 0; j < 3; ++j)\n\t\t\t\t{\n\t\t\t\t\tif (f.m_n[j]->m_im > 0)\n\t\t\t\t\t{\n\t\t\t\t\t\t// Check if the velocity change resulted by aero drag force exceeds the current velocity of the node.\n\t\t\t\t\t\tbtVector3 del_v_by_fDrag = fDrag * f.m_n[j]->m_im * m_sst.sdt;\n\t\t\t\t\t\tbtScalar del_v_by_fDrag_len2 = del_v_by_fDrag.length2();\n\t\t\t\t\t\tbtScalar v_len2 = f.m_n[j]->m_v.length2();\n\n\t\t\t\t\t\tif (del_v_by_fDrag_len2 >= v_len2 && del_v_by_fDrag_len2 > 0)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tbtScalar del_v_by_fDrag_len = del_v_by_fDrag.length();\n\t\t\t\t\t\t\tbtScalar v_len = f.m_n[j]->m_v.length();\n\t\t\t\t\t\t\tfDrag *= btScalar(0.8) * (v_len / del_v_by_fDrag_len);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tf.m_n[j]->m_f += fDrag;\n\t\t\t\t\t\tf.m_n[j]->m_f += fLift;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if (m_cfg.aeromodel == btSoftBody::eAeroModel::F_OneSided || m_cfg.aeromodel == btSoftBody::eAeroModel::F_TwoSided)\n\t\t\t{\n\t\t\t\tif (m_cfg.aeromodel == btSoftBody::eAeroModel::F_TwoSided)\n\t\t\t\t\tnrm *= (btScalar)((btDot(nrm, rel_v) < 0) ? -1 : +1);\n\n\t\t\t\tconst btScalar dvn = btDot(rel_v, nrm);\n\t\t\t\t/* Compute forces\t*/\n\t\t\t\tif (dvn > 0)\n\t\t\t\t{\n\t\t\t\t\tbtVector3 force(0, 0, 0);\n\t\t\t\t\tconst btScalar c0 = f.m_ra * dvn * rel_v2;\n\t\t\t\t\tconst btScalar c1 = c0 * medium.m_density;\n\t\t\t\t\tforce += nrm * (-c1 * kLF);\n\t\t\t\t\tforce += rel_v.normalized() * (-c1 * kDG);\n\t\t\t\t\tforce /= 3;\n\t\t\t\t\tfor (int j = 0; j < 3; ++j) ApplyClampedForce(*f.m_n[j], force, dt);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n//\nvoid btSoftBody::addVelocity(const btVector3& velocity)\n{\n\tfor (int i = 0, ni = m_nodes.size(); i < ni; ++i) addVelocity(velocity, i);\n}\n\n/* Set velocity for the entire body\t\t\t\t\t\t\t\t\t\t*/\nvoid btSoftBody::setVelocity(const btVector3& velocity)\n{\n\tfor (int i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t{\n\t\tNode& n = m_nodes[i];\n\t\tif (n.m_im > 0)\n\t\t{\n\t\t\tn.m_v = velocity;\n\t\t}\n\t}\n}\n\n//\nvoid btSoftBody::addVelocity(const btVector3& velocity, int node)\n{\n\tNode& n = m_nodes[node];\n\tif (n.m_im > 0)\n\t{\n\t\tn.m_v += velocity;\n\t}\n}\n\n//\nvoid btSoftBody::setMass(int node, btScalar mass)\n{\n\tm_nodes[node].m_im = mass > 0 ? 1 / mass : 0;\n\tm_bUpdateRtCst = true;\n}\n\n//\nbtScalar btSoftBody::getMass(int node) const\n{\n\treturn (m_nodes[node].m_im > 0 ? 1 / m_nodes[node].m_im : 0);\n}\n\n//\nbtScalar btSoftBody::getTotalMass() const\n{\n\tbtScalar mass = 0;\n\tfor (int i = 0; i < m_nodes.size(); ++i)\n\t{\n\t\tmass += getMass(i);\n\t}\n\treturn (mass);\n}\n\n//\nvoid btSoftBody::setTotalMass(btScalar mass, bool fromfaces)\n{\n\tint i;\n\n\tif (fromfaces)\n\t{\n\t\tfor (i = 0; i < m_nodes.size(); ++i)\n\t\t{\n\t\t\tm_nodes[i].m_im = 0;\n\t\t}\n\t\tfor (i = 0; i < m_faces.size(); ++i)\n\t\t{\n\t\t\tconst Face& f = m_faces[i];\n\t\t\tconst btScalar twicearea = AreaOf(f.m_n[0]->m_x,\n\t\t\t\t\t\t\t\t\t\t\t  f.m_n[1]->m_x,\n\t\t\t\t\t\t\t\t\t\t\t  f.m_n[2]->m_x);\n\t\t\tfor (int j = 0; j < 3; ++j)\n\t\t\t{\n\t\t\t\tf.m_n[j]->m_im += twicearea;\n\t\t\t}\n\t\t}\n\t\tfor (i = 0; i < m_nodes.size(); ++i)\n\t\t{\n\t\t\tm_nodes[i].m_im = 1 / m_nodes[i].m_im;\n\t\t}\n\t}\n\tconst btScalar tm = getTotalMass();\n\tconst btScalar itm = 1 / tm;\n\tfor (i = 0; i < m_nodes.size(); ++i)\n\t{\n\t\tm_nodes[i].m_im /= itm * mass;\n\t}\n\tm_bUpdateRtCst = true;\n}\n\n//\nvoid btSoftBody::setTotalDensity(btScalar density)\n{\n\tsetTotalMass(getVolume() * density, true);\n}\n\n//\nvoid btSoftBody::setVolumeMass(btScalar mass)\n{\n\tbtAlignedObjectArray<btScalar> ranks;\n\tranks.resize(m_nodes.size(), 0);\n\tint i;\n\n\tfor (i = 0; i < m_nodes.size(); ++i)\n\t{\n\t\tm_nodes[i].m_im = 0;\n\t}\n\tfor (i = 0; i < m_tetras.size(); ++i)\n\t{\n\t\tconst Tetra& t = m_tetras[i];\n\t\tfor (int j = 0; j < 4; ++j)\n\t\t{\n\t\t\tt.m_n[j]->m_im += btFabs(t.m_rv);\n\t\t\tranks[int(t.m_n[j] - &m_nodes[0])] += 1;\n\t\t}\n\t}\n\tfor (i = 0; i < m_nodes.size(); ++i)\n\t{\n\t\tif (m_nodes[i].m_im > 0)\n\t\t{\n\t\t\tm_nodes[i].m_im = ranks[i] / m_nodes[i].m_im;\n\t\t}\n\t}\n\tsetTotalMass(mass, false);\n}\n\n//\nvoid btSoftBody::setVolumeDensity(btScalar density)\n{\n\tbtScalar volume = 0;\n\tfor (int i = 0; i < m_tetras.size(); ++i)\n\t{\n\t\tconst Tetra& t = m_tetras[i];\n\t\tfor (int j = 0; j < 4; ++j)\n\t\t{\n\t\t\tvolume += btFabs(t.m_rv);\n\t\t}\n\t}\n\tsetVolumeMass(volume * density / 6);\n}\n\n//\nvoid btSoftBody::transform(const btTransform& trs)\n{\n\tconst btScalar margin = getCollisionShape()->getMargin();\n\tATTRIBUTE_ALIGNED16(btDbvtVolume)\n\tvol;\n\n\tfor (int i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t{\n\t\tNode& n = m_nodes[i];\n\t\tn.m_x = trs * n.m_x;\n\t\tn.m_q = trs * n.m_q;\n\t\tn.m_n = trs.getBasis() * n.m_n;\n\t\tvol = btDbvtVolume::FromCR(n.m_x, margin);\n\n\t\tm_ndbvt.update(n.m_leaf, vol);\n\t}\n\tupdateNormals();\n\tupdateBounds();\n\tupdateConstants();\n\tm_initialWorldTransform = trs;\n}\n\n//\nvoid btSoftBody::translate(const btVector3& trs)\n{\n\tbtTransform t;\n\tt.setIdentity();\n\tt.setOrigin(trs);\n\ttransform(t);\n}\n\n//\nvoid btSoftBody::rotate(const btQuaternion& rot)\n{\n\tbtTransform t;\n\tt.setIdentity();\n\tt.setRotation(rot);\n\ttransform(t);\n}\n\n//\nvoid btSoftBody::scale(const btVector3& scl)\n{\n\tconst btScalar margin = getCollisionShape()->getMargin();\n\tATTRIBUTE_ALIGNED16(btDbvtVolume)\n\tvol;\n\n\tfor (int i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t{\n\t\tNode& n = m_nodes[i];\n\t\tn.m_x *= scl;\n\t\tn.m_q *= scl;\n\t\tvol = btDbvtVolume::FromCR(n.m_x, margin);\n\t\tm_ndbvt.update(n.m_leaf, vol);\n\t}\n\tupdateNormals();\n\tupdateBounds();\n\tupdateConstants();\n    initializeDmInverse();\n}\n\n//\nbtScalar btSoftBody::getRestLengthScale()\n{\n\treturn m_restLengthScale;\n}\n\n//\nvoid btSoftBody::setRestLengthScale(btScalar restLengthScale)\n{\n\tfor (int i = 0, ni = m_links.size(); i < ni; ++i)\n\t{\n\t\tLink& l = m_links[i];\n\t\tl.m_rl = l.m_rl / m_restLengthScale * restLengthScale;\n\t\tl.m_c1 = l.m_rl * l.m_rl;\n\t}\n\tm_restLengthScale = restLengthScale;\n\n\tif (getActivationState() == ISLAND_SLEEPING)\n\t\tactivate();\n}\n\n//\nvoid btSoftBody::setPose(bool bvolume, bool bframe)\n{\n\tm_pose.m_bvolume = bvolume;\n\tm_pose.m_bframe = bframe;\n\tint i, ni;\n\n\t/* Weights\t\t*/\n\tconst btScalar omass = getTotalMass();\n\tconst btScalar kmass = omass * m_nodes.size() * 1000;\n\tbtScalar tmass = omass;\n\tm_pose.m_wgh.resize(m_nodes.size());\n\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t{\n\t\tif (m_nodes[i].m_im <= 0) tmass += kmass;\n\t}\n\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t{\n\t\tNode& n = m_nodes[i];\n\t\tm_pose.m_wgh[i] = n.m_im > 0 ? 1 / (m_nodes[i].m_im * tmass) : kmass / tmass;\n\t}\n\t/* Pos\t\t*/\n\tconst btVector3 com = evaluateCom();\n\tm_pose.m_pos.resize(m_nodes.size());\n\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t{\n\t\tm_pose.m_pos[i] = m_nodes[i].m_x - com;\n\t}\n\tm_pose.m_volume = bvolume ? getVolume() : 0;\n\tm_pose.m_com = com;\n\tm_pose.m_rot.setIdentity();\n\tm_pose.m_scl.setIdentity();\n\t/* Aqq\t\t*/\n\tm_pose.m_aqq[0] =\n\t\tm_pose.m_aqq[1] =\n\t\t\tm_pose.m_aqq[2] = btVector3(0, 0, 0);\n\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t{\n\t\tconst btVector3& q = m_pose.m_pos[i];\n\t\tconst btVector3 mq = m_pose.m_wgh[i] * q;\n\t\tm_pose.m_aqq[0] += mq.x() * q;\n\t\tm_pose.m_aqq[1] += mq.y() * q;\n\t\tm_pose.m_aqq[2] += mq.z() * q;\n\t}\n\tm_pose.m_aqq = m_pose.m_aqq.inverse();\n\n\tupdateConstants();\n}\n\nvoid btSoftBody::resetLinkRestLengths()\n{\n\tfor (int i = 0, ni = m_links.size(); i < ni; ++i)\n\t{\n\t\tLink& l = m_links[i];\n\t\tl.m_rl = (l.m_n[0]->m_x - l.m_n[1]->m_x).length();\n\t\tl.m_c1 = l.m_rl * l.m_rl;\n\t}\n}\n\n//\nbtScalar btSoftBody::getVolume() const\n{\n\tbtScalar vol = 0;\n\tif (m_nodes.size() > 0)\n\t{\n\t\tint i, ni;\n\n\t\tconst btVector3 org = m_nodes[0].m_x;\n\t\tfor (i = 0, ni = m_faces.size(); i < ni; ++i)\n\t\t{\n\t\t\tconst Face& f = m_faces[i];\n\t\t\tvol += btDot(f.m_n[0]->m_x - org, btCross(f.m_n[1]->m_x - org, f.m_n[2]->m_x - org));\n\t\t}\n\t\tvol /= (btScalar)6;\n\t}\n\treturn (vol);\n}\n\n//\nint btSoftBody::clusterCount() const\n{\n\treturn (m_clusters.size());\n}\n\n//\nbtVector3 btSoftBody::clusterCom(const Cluster* cluster)\n{\n\tbtVector3 com(0, 0, 0);\n\tfor (int i = 0, ni = cluster->m_nodes.size(); i < ni; ++i)\n\t{\n\t\tcom += cluster->m_nodes[i]->m_x * cluster->m_masses[i];\n\t}\n\treturn (com * cluster->m_imass);\n}\n\n//\nbtVector3 btSoftBody::clusterCom(int cluster) const\n{\n\treturn (clusterCom(m_clusters[cluster]));\n}\n\n//\nbtVector3 btSoftBody::clusterVelocity(const Cluster* cluster, const btVector3& rpos)\n{\n\treturn (cluster->m_lv + btCross(cluster->m_av, rpos));\n}\n\n//\nvoid btSoftBody::clusterVImpulse(Cluster* cluster, const btVector3& rpos, const btVector3& impulse)\n{\n\tconst btVector3 li = cluster->m_imass * impulse;\n\tconst btVector3 ai = cluster->m_invwi * btCross(rpos, impulse);\n\tcluster->m_vimpulses[0] += li;\n\tcluster->m_lv += li;\n\tcluster->m_vimpulses[1] += ai;\n\tcluster->m_av += ai;\n\tcluster->m_nvimpulses++;\n}\n\n//\nvoid btSoftBody::clusterDImpulse(Cluster* cluster, const btVector3& rpos, const btVector3& impulse)\n{\n\tconst btVector3 li = cluster->m_imass * impulse;\n\tconst btVector3 ai = cluster->m_invwi * btCross(rpos, impulse);\n\tcluster->m_dimpulses[0] += li;\n\tcluster->m_dimpulses[1] += ai;\n\tcluster->m_ndimpulses++;\n}\n\n//\nvoid btSoftBody::clusterImpulse(Cluster* cluster, const btVector3& rpos, const Impulse& impulse)\n{\n\tif (impulse.m_asVelocity) clusterVImpulse(cluster, rpos, impulse.m_velocity);\n\tif (impulse.m_asDrift) clusterDImpulse(cluster, rpos, impulse.m_drift);\n}\n\n//\nvoid btSoftBody::clusterVAImpulse(Cluster* cluster, const btVector3& impulse)\n{\n\tconst btVector3 ai = cluster->m_invwi * impulse;\n\tcluster->m_vimpulses[1] += ai;\n\tcluster->m_av += ai;\n\tcluster->m_nvimpulses++;\n}\n\n//\nvoid btSoftBody::clusterDAImpulse(Cluster* cluster, const btVector3& impulse)\n{\n\tconst btVector3 ai = cluster->m_invwi * impulse;\n\tcluster->m_dimpulses[1] += ai;\n\tcluster->m_ndimpulses++;\n}\n\n//\nvoid btSoftBody::clusterAImpulse(Cluster* cluster, const Impulse& impulse)\n{\n\tif (impulse.m_asVelocity) clusterVAImpulse(cluster, impulse.m_velocity);\n\tif (impulse.m_asDrift) clusterDAImpulse(cluster, impulse.m_drift);\n}\n\n//\nvoid btSoftBody::clusterDCImpulse(Cluster* cluster, const btVector3& impulse)\n{\n\tcluster->m_dimpulses[0] += impulse * cluster->m_imass;\n\tcluster->m_ndimpulses++;\n}\n\nstruct NodeLinks\n{\n\tbtAlignedObjectArray<int> m_links;\n};\n\n//\nint btSoftBody::generateBendingConstraints(int distance, Material* mat)\n{\n\tint i, j;\n\n\tif (distance > 1)\n\t{\n\t\t/* Build graph\t*/\n\t\tconst int n = m_nodes.size();\n\t\tconst unsigned inf = (~(unsigned)0) >> 1;\n\t\tunsigned* adj = new unsigned[n * n];\n\n#define IDX(_x_, _y_) ((_y_)*n + (_x_))\n\t\tfor (j = 0; j < n; ++j)\n\t\t{\n\t\t\tfor (i = 0; i < n; ++i)\n\t\t\t{\n\t\t\t\tif (i != j)\n\t\t\t\t{\n\t\t\t\t\tadj[IDX(i, j)] = adj[IDX(j, i)] = inf;\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tadj[IDX(i, j)] = adj[IDX(j, i)] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfor (i = 0; i < m_links.size(); ++i)\n\t\t{\n\t\t\tconst int ia = (int)(m_links[i].m_n[0] - &m_nodes[0]);\n\t\t\tconst int ib = (int)(m_links[i].m_n[1] - &m_nodes[0]);\n\t\t\tadj[IDX(ia, ib)] = 1;\n\t\t\tadj[IDX(ib, ia)] = 1;\n\t\t}\n\n\t\t//special optimized case for distance == 2\n\t\tif (distance == 2)\n\t\t{\n\t\t\tbtAlignedObjectArray<NodeLinks> nodeLinks;\n\n\t\t\t/* Build node links */\n\t\t\tnodeLinks.resize(m_nodes.size());\n\n\t\t\tfor (i = 0; i < m_links.size(); ++i)\n\t\t\t{\n\t\t\t\tconst int ia = (int)(m_links[i].m_n[0] - &m_nodes[0]);\n\t\t\t\tconst int ib = (int)(m_links[i].m_n[1] - &m_nodes[0]);\n\t\t\t\tif (nodeLinks[ia].m_links.findLinearSearch(ib) == nodeLinks[ia].m_links.size())\n\t\t\t\t\tnodeLinks[ia].m_links.push_back(ib);\n\n\t\t\t\tif (nodeLinks[ib].m_links.findLinearSearch(ia) == nodeLinks[ib].m_links.size())\n\t\t\t\t\tnodeLinks[ib].m_links.push_back(ia);\n\t\t\t}\n\t\t\tfor (int ii = 0; ii < nodeLinks.size(); ii++)\n\t\t\t{\n\t\t\t\tint i = ii;\n\n\t\t\t\tfor (int jj = 0; jj < nodeLinks[ii].m_links.size(); jj++)\n\t\t\t\t{\n\t\t\t\t\tint k = nodeLinks[ii].m_links[jj];\n\t\t\t\t\tfor (int kk = 0; kk < nodeLinks[k].m_links.size(); kk++)\n\t\t\t\t\t{\n\t\t\t\t\t\tint j = nodeLinks[k].m_links[kk];\n\t\t\t\t\t\tif (i != j)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tconst unsigned sum = adj[IDX(i, k)] + adj[IDX(k, j)];\n\t\t\t\t\t\t\tbtAssert(sum == 2);\n\t\t\t\t\t\t\tif (adj[IDX(i, j)] > sum)\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tadj[IDX(i, j)] = adj[IDX(j, i)] = sum;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\t///generic Floyd's algorithm\n\t\t\tfor (int k = 0; k < n; ++k)\n\t\t\t{\n\t\t\t\tfor (j = 0; j < n; ++j)\n\t\t\t\t{\n\t\t\t\t\tfor (i = j + 1; i < n; ++i)\n\t\t\t\t\t{\n\t\t\t\t\t\tconst unsigned sum = adj[IDX(i, k)] + adj[IDX(k, j)];\n\t\t\t\t\t\tif (adj[IDX(i, j)] > sum)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tadj[IDX(i, j)] = adj[IDX(j, i)] = sum;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/* Build links\t*/\n\t\tint nlinks = 0;\n\t\tfor (j = 0; j < n; ++j)\n\t\t{\n\t\t\tfor (i = j + 1; i < n; ++i)\n\t\t\t{\n\t\t\t\tif (adj[IDX(i, j)] == (unsigned)distance)\n\t\t\t\t{\n\t\t\t\t\tappendLink(i, j, mat);\n\t\t\t\t\tm_links[m_links.size() - 1].m_bbending = 1;\n\t\t\t\t\t++nlinks;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tdelete[] adj;\n\t\treturn (nlinks);\n\t}\n\treturn (0);\n}\n\n//\nvoid btSoftBody::randomizeConstraints()\n{\n\tunsigned long seed = 243703;\n#define NEXTRAND (seed = (1664525L * seed + 1013904223L) & 0xffffffff)\n\tint i, ni;\n\n\tfor (i = 0, ni = m_links.size(); i < ni; ++i)\n\t{\n\t\tbtSwap(m_links[i], m_links[NEXTRAND % ni]);\n\t}\n\tfor (i = 0, ni = m_faces.size(); i < ni; ++i)\n\t{\n\t\tbtSwap(m_faces[i], m_faces[NEXTRAND % ni]);\n\t}\n#undef NEXTRAND\n}\n\n//\nvoid btSoftBody::releaseCluster(int index)\n{\n\tCluster* c = m_clusters[index];\n\tif (c->m_leaf) m_cdbvt.remove(c->m_leaf);\n\tc->~Cluster();\n\tbtAlignedFree(c);\n\tm_clusters.remove(c);\n}\n\n//\nvoid btSoftBody::releaseClusters()\n{\n\twhile (m_clusters.size() > 0) releaseCluster(0);\n}\n\n//\nint btSoftBody::generateClusters(int k, int maxiterations)\n{\n\tint i;\n\treleaseClusters();\n\tm_clusters.resize(btMin(k, m_nodes.size()));\n\tfor (i = 0; i < m_clusters.size(); ++i)\n\t{\n\t\tm_clusters[i] = new (btAlignedAlloc(sizeof(Cluster), 16)) Cluster();\n\t\tm_clusters[i]->m_collide = true;\n\t}\n\tk = m_clusters.size();\n\tif (k > 0)\n\t{\n\t\t/* Initialize\t\t*/\n\t\tbtAlignedObjectArray<btVector3> centers;\n\t\tbtVector3 cog(0, 0, 0);\n\t\tint i;\n\t\tfor (i = 0; i < m_nodes.size(); ++i)\n\t\t{\n\t\t\tcog += m_nodes[i].m_x;\n\t\t\tm_clusters[(i * 29873) % m_clusters.size()]->m_nodes.push_back(&m_nodes[i]);\n\t\t}\n\t\tcog /= (btScalar)m_nodes.size();\n\t\tcenters.resize(k, cog);\n\t\t/* Iterate\t\t\t*/\n\t\tconst btScalar slope = 16;\n\t\tbool changed;\n\t\tint iterations = 0;\n\t\tdo\n\t\t{\n\t\t\tconst btScalar w = 2 - btMin<btScalar>(1, iterations / slope);\n\t\t\tchanged = false;\n\t\t\titerations++;\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < k; ++i)\n\t\t\t{\n\t\t\t\tbtVector3 c(0, 0, 0);\n\t\t\t\tfor (int j = 0; j < m_clusters[i]->m_nodes.size(); ++j)\n\t\t\t\t{\n\t\t\t\t\tc += m_clusters[i]->m_nodes[j]->m_x;\n\t\t\t\t}\n\t\t\t\tif (m_clusters[i]->m_nodes.size())\n\t\t\t\t{\n\t\t\t\t\tc /= (btScalar)m_clusters[i]->m_nodes.size();\n\t\t\t\t\tc = centers[i] + (c - centers[i]) * w;\n\t\t\t\t\tchanged |= ((c - centers[i]).length2() > SIMD_EPSILON);\n\t\t\t\t\tcenters[i] = c;\n\t\t\t\t\tm_clusters[i]->m_nodes.resize(0);\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor (i = 0; i < m_nodes.size(); ++i)\n\t\t\t{\n\t\t\t\tconst btVector3 nx = m_nodes[i].m_x;\n\t\t\t\tint kbest = 0;\n\t\t\t\tbtScalar kdist = ClusterMetric(centers[0], nx);\n\t\t\t\tfor (int j = 1; j < k; ++j)\n\t\t\t\t{\n\t\t\t\t\tconst btScalar d = ClusterMetric(centers[j], nx);\n\t\t\t\t\tif (d < kdist)\n\t\t\t\t\t{\n\t\t\t\t\t\tkbest = j;\n\t\t\t\t\t\tkdist = d;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tm_clusters[kbest]->m_nodes.push_back(&m_nodes[i]);\n\t\t\t}\n\t\t} while (changed && (iterations < maxiterations));\n\t\t/* Merge\t\t*/\n\t\tbtAlignedObjectArray<int> cids;\n\t\tcids.resize(m_nodes.size(), -1);\n\t\tfor (i = 0; i < m_clusters.size(); ++i)\n\t\t{\n\t\t\tfor (int j = 0; j < m_clusters[i]->m_nodes.size(); ++j)\n\t\t\t{\n\t\t\t\tcids[int(m_clusters[i]->m_nodes[j] - &m_nodes[0])] = i;\n\t\t\t}\n\t\t}\n\t\tfor (i = 0; i < m_faces.size(); ++i)\n\t\t{\n\t\t\tconst int idx[] = {int(m_faces[i].m_n[0] - &m_nodes[0]),\n\t\t\t\t\t\t\t   int(m_faces[i].m_n[1] - &m_nodes[0]),\n\t\t\t\t\t\t\t   int(m_faces[i].m_n[2] - &m_nodes[0])};\n\t\t\tfor (int j = 0; j < 3; ++j)\n\t\t\t{\n\t\t\t\tconst int cid = cids[idx[j]];\n\t\t\t\tfor (int q = 1; q < 3; ++q)\n\t\t\t\t{\n\t\t\t\t\tconst int kid = idx[(j + q) % 3];\n\t\t\t\t\tif (cids[kid] != cid)\n\t\t\t\t\t{\n\t\t\t\t\t\tif (m_clusters[cid]->m_nodes.findLinearSearch(&m_nodes[kid]) == m_clusters[cid]->m_nodes.size())\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tm_clusters[cid]->m_nodes.push_back(&m_nodes[kid]);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t/* Master\t\t*/\n\t\tif (m_clusters.size() > 1)\n\t\t{\n\t\t\tCluster* pmaster = new (btAlignedAlloc(sizeof(Cluster), 16)) Cluster();\n\t\t\tpmaster->m_collide = false;\n\t\t\tpmaster->m_nodes.reserve(m_nodes.size());\n\t\t\tfor (int i = 0; i < m_nodes.size(); ++i) pmaster->m_nodes.push_back(&m_nodes[i]);\n\t\t\tm_clusters.push_back(pmaster);\n\t\t\tbtSwap(m_clusters[0], m_clusters[m_clusters.size() - 1]);\n\t\t}\n\t\t/* Terminate\t*/\n\t\tfor (i = 0; i < m_clusters.size(); ++i)\n\t\t{\n\t\t\tif (m_clusters[i]->m_nodes.size() == 0)\n\t\t\t{\n\t\t\t\treleaseCluster(i--);\n\t\t\t}\n\t\t}\n\t}\n\telse\n\t{\n\t\t//create a cluster for each tetrahedron (if tetrahedra exist) or each face\n\t\tif (m_tetras.size())\n\t\t{\n\t\t\tm_clusters.resize(m_tetras.size());\n\t\t\tfor (i = 0; i < m_clusters.size(); ++i)\n\t\t\t{\n\t\t\t\tm_clusters[i] = new (btAlignedAlloc(sizeof(Cluster), 16)) Cluster();\n\t\t\t\tm_clusters[i]->m_collide = true;\n\t\t\t}\n\t\t\tfor (i = 0; i < m_tetras.size(); i++)\n\t\t\t{\n\t\t\t\tfor (int j = 0; j < 4; j++)\n\t\t\t\t{\n\t\t\t\t\tm_clusters[i]->m_nodes.push_back(m_tetras[i].m_n[j]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\tm_clusters.resize(m_faces.size());\n\t\t\tfor (i = 0; i < m_clusters.size(); ++i)\n\t\t\t{\n\t\t\t\tm_clusters[i] = new (btAlignedAlloc(sizeof(Cluster), 16)) Cluster();\n\t\t\t\tm_clusters[i]->m_collide = true;\n\t\t\t}\n\n\t\t\tfor (i = 0; i < m_faces.size(); ++i)\n\t\t\t{\n\t\t\t\tfor (int j = 0; j < 3; ++j)\n\t\t\t\t{\n\t\t\t\t\tm_clusters[i]->m_nodes.push_back(m_faces[i].m_n[j]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (m_clusters.size())\n\t{\n\t\tinitializeClusters();\n\t\tupdateClusters();\n\n\t\t//for self-collision\n\t\tm_clusterConnectivity.resize(m_clusters.size() * m_clusters.size());\n\t\t{\n\t\t\tfor (int c0 = 0; c0 < m_clusters.size(); c0++)\n\t\t\t{\n\t\t\t\tm_clusters[c0]->m_clusterIndex = c0;\n\t\t\t\tfor (int c1 = 0; c1 < m_clusters.size(); c1++)\n\t\t\t\t{\n\t\t\t\t\tbool connected = false;\n\t\t\t\t\tCluster* cla = m_clusters[c0];\n\t\t\t\t\tCluster* clb = m_clusters[c1];\n\t\t\t\t\tfor (int i = 0; !connected && i < cla->m_nodes.size(); i++)\n\t\t\t\t\t{\n\t\t\t\t\t\tfor (int j = 0; j < clb->m_nodes.size(); j++)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tif (cla->m_nodes[i] == clb->m_nodes[j])\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tconnected = true;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tm_clusterConnectivity[c0 + c1 * m_clusters.size()] = connected;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn (m_clusters.size());\n}\n\n//\nvoid btSoftBody::refine(ImplicitFn* ifn, btScalar accurary, bool cut)\n{\n\tconst Node* nbase = &m_nodes[0];\n\tint ncount = m_nodes.size();\n\tbtSymMatrix<int> edges(ncount, -2);\n\tint newnodes = 0;\n\tint i, j, k, ni;\n\n\t/* Filter out\t\t*/\n\tfor (i = 0; i < m_links.size(); ++i)\n\t{\n\t\tLink& l = m_links[i];\n\t\tif (l.m_bbending)\n\t\t{\n\t\t\tif (!SameSign(ifn->Eval(l.m_n[0]->m_x), ifn->Eval(l.m_n[1]->m_x)))\n\t\t\t{\n\t\t\t\tbtSwap(m_links[i], m_links[m_links.size() - 1]);\n\t\t\t\tm_links.pop_back();\n\t\t\t\t--i;\n\t\t\t}\n\t\t}\n\t}\n\t/* Fill edges\t\t*/\n\tfor (i = 0; i < m_links.size(); ++i)\n\t{\n\t\tLink& l = m_links[i];\n\t\tedges(int(l.m_n[0] - nbase), int(l.m_n[1] - nbase)) = -1;\n\t}\n\tfor (i = 0; i < m_faces.size(); ++i)\n\t{\n\t\tFace& f = m_faces[i];\n\t\tedges(int(f.m_n[0] - nbase), int(f.m_n[1] - nbase)) = -1;\n\t\tedges(int(f.m_n[1] - nbase), int(f.m_n[2] - nbase)) = -1;\n\t\tedges(int(f.m_n[2] - nbase), int(f.m_n[0] - nbase)) = -1;\n\t}\n\t/* Intersect\t\t*/\n\tfor (i = 0; i < ncount; ++i)\n\t{\n\t\tfor (j = i + 1; j < ncount; ++j)\n\t\t{\n\t\t\tif (edges(i, j) == -1)\n\t\t\t{\n\t\t\t\tNode& a = m_nodes[i];\n\t\t\t\tNode& b = m_nodes[j];\n\t\t\t\tconst btScalar t = ImplicitSolve(ifn, a.m_x, b.m_x, accurary);\n\t\t\t\tif (t > 0)\n\t\t\t\t{\n\t\t\t\t\tconst btVector3 x = Lerp(a.m_x, b.m_x, t);\n\t\t\t\t\tconst btVector3 v = Lerp(a.m_v, b.m_v, t);\n\t\t\t\t\tbtScalar m = 0;\n\t\t\t\t\tif (a.m_im > 0)\n\t\t\t\t\t{\n\t\t\t\t\t\tif (b.m_im > 0)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tconst btScalar ma = 1 / a.m_im;\n\t\t\t\t\t\t\tconst btScalar mb = 1 / b.m_im;\n\t\t\t\t\t\t\tconst btScalar mc = Lerp(ma, mb, t);\n\t\t\t\t\t\t\tconst btScalar f = (ma + mb) / (ma + mb + mc);\n\t\t\t\t\t\t\ta.m_im = 1 / (ma * f);\n\t\t\t\t\t\t\tb.m_im = 1 / (mb * f);\n\t\t\t\t\t\t\tm = mc * f;\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\ta.m_im /= 0.5f;\n\t\t\t\t\t\t\tm = 1 / a.m_im;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\telse\n\t\t\t\t\t{\n\t\t\t\t\t\tif (b.m_im > 0)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tb.m_im /= 0.5f;\n\t\t\t\t\t\t\tm = 1 / b.m_im;\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\tm = 0;\n\t\t\t\t\t}\n\t\t\t\t\tappendNode(x, m);\n\t\t\t\t\tedges(i, j) = m_nodes.size() - 1;\n\t\t\t\t\tm_nodes[edges(i, j)].m_v = v;\n\t\t\t\t\t++newnodes;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tnbase = &m_nodes[0];\n\t/* Refine links\t\t*/\n\tfor (i = 0, ni = m_links.size(); i < ni; ++i)\n\t{\n\t\tLink& feat = m_links[i];\n\t\tconst int idx[] = {int(feat.m_n[0] - nbase),\n\t\t\t\t\t\t   int(feat.m_n[1] - nbase)};\n\t\tif ((idx[0] < ncount) && (idx[1] < ncount))\n\t\t{\n\t\t\tconst int ni = edges(idx[0], idx[1]);\n\t\t\tif (ni > 0)\n\t\t\t{\n\t\t\t\tappendLink(i);\n\t\t\t\tLink* pft[] = {&m_links[i],\n\t\t\t\t\t\t\t   &m_links[m_links.size() - 1]};\n\t\t\t\tpft[0]->m_n[0] = &m_nodes[idx[0]];\n\t\t\t\tpft[0]->m_n[1] = &m_nodes[ni];\n\t\t\t\tpft[1]->m_n[0] = &m_nodes[ni];\n\t\t\t\tpft[1]->m_n[1] = &m_nodes[idx[1]];\n\t\t\t}\n\t\t}\n\t}\n\t/* Refine faces\t\t*/\n\tfor (i = 0; i < m_faces.size(); ++i)\n\t{\n\t\tconst Face& feat = m_faces[i];\n\t\tconst int idx[] = {int(feat.m_n[0] - nbase),\n\t\t\t\t\t\t   int(feat.m_n[1] - nbase),\n\t\t\t\t\t\t   int(feat.m_n[2] - nbase)};\n\t\tfor (j = 2, k = 0; k < 3; j = k++)\n\t\t{\n\t\t\tif ((idx[j] < ncount) && (idx[k] < ncount))\n\t\t\t{\n\t\t\t\tconst int ni = edges(idx[j], idx[k]);\n\t\t\t\tif (ni > 0)\n\t\t\t\t{\n\t\t\t\t\tappendFace(i);\n\t\t\t\t\tconst int l = (k + 1) % 3;\n\t\t\t\t\tFace* pft[] = {&m_faces[i],\n\t\t\t\t\t\t\t\t   &m_faces[m_faces.size() - 1]};\n\t\t\t\t\tpft[0]->m_n[0] = &m_nodes[idx[l]];\n\t\t\t\t\tpft[0]->m_n[1] = &m_nodes[idx[j]];\n\t\t\t\t\tpft[0]->m_n[2] = &m_nodes[ni];\n\t\t\t\t\tpft[1]->m_n[0] = &m_nodes[ni];\n\t\t\t\t\tpft[1]->m_n[1] = &m_nodes[idx[k]];\n\t\t\t\t\tpft[1]->m_n[2] = &m_nodes[idx[l]];\n\t\t\t\t\tappendLink(ni, idx[l], pft[0]->m_material);\n\t\t\t\t\t--i;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t/* Cut\t\t\t\t*/\n\tif (cut)\n\t{\n\t\tbtAlignedObjectArray<int> cnodes;\n\t\tconst int pcount = ncount;\n\t\tint i;\n\t\tncount = m_nodes.size();\n\t\tcnodes.resize(ncount, 0);\n\t\t/* Nodes\t\t*/\n\t\tfor (i = 0; i < ncount; ++i)\n\t\t{\n\t\t\tconst btVector3 x = m_nodes[i].m_x;\n\t\t\tif ((i >= pcount) || (btFabs(ifn->Eval(x)) < accurary))\n\t\t\t{\n\t\t\t\tconst btVector3 v = m_nodes[i].m_v;\n\t\t\t\tbtScalar m = getMass(i);\n\t\t\t\tif (m > 0)\n\t\t\t\t{\n\t\t\t\t\tm *= 0.5f;\n\t\t\t\t\tm_nodes[i].m_im /= 0.5f;\n\t\t\t\t}\n\t\t\t\tappendNode(x, m);\n\t\t\t\tcnodes[i] = m_nodes.size() - 1;\n\t\t\t\tm_nodes[cnodes[i]].m_v = v;\n\t\t\t}\n\t\t}\n\t\tnbase = &m_nodes[0];\n\t\t/* Links\t\t*/\n\t\tfor (i = 0, ni = m_links.size(); i < ni; ++i)\n\t\t{\n\t\t\tconst int id[] = {int(m_links[i].m_n[0] - nbase),\n\t\t\t\t\t\t\t  int(m_links[i].m_n[1] - nbase)};\n\t\t\tint todetach = 0;\n\t\t\tif (cnodes[id[0]] && cnodes[id[1]])\n\t\t\t{\n\t\t\t\tappendLink(i);\n\t\t\t\ttodetach = m_links.size() - 1;\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tif (((ifn->Eval(m_nodes[id[0]].m_x) < accurary) &&\n\t\t\t\t\t (ifn->Eval(m_nodes[id[1]].m_x) < accurary)))\n\t\t\t\t\ttodetach = i;\n\t\t\t}\n\t\t\tif (todetach)\n\t\t\t{\n\t\t\t\tLink& l = m_links[todetach];\n\t\t\t\tfor (int j = 0; j < 2; ++j)\n\t\t\t\t{\n\t\t\t\t\tint cn = cnodes[int(l.m_n[j] - nbase)];\n\t\t\t\t\tif (cn) l.m_n[j] = &m_nodes[cn];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t/* Faces\t\t*/\n\t\tfor (i = 0, ni = m_faces.size(); i < ni; ++i)\n\t\t{\n\t\t\tNode** n = m_faces[i].m_n;\n\t\t\tif ((ifn->Eval(n[0]->m_x) < accurary) &&\n\t\t\t\t(ifn->Eval(n[1]->m_x) < accurary) &&\n\t\t\t\t(ifn->Eval(n[2]->m_x) < accurary))\n\t\t\t{\n\t\t\t\tfor (int j = 0; j < 3; ++j)\n\t\t\t\t{\n\t\t\t\t\tint cn = cnodes[int(n[j] - nbase)];\n\t\t\t\t\tif (cn) n[j] = &m_nodes[cn];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t/* Clean orphans\t*/\n\t\tint nnodes = m_nodes.size();\n\t\tbtAlignedObjectArray<int> ranks;\n\t\tbtAlignedObjectArray<int> todelete;\n\t\tranks.resize(nnodes, 0);\n\t\tfor (i = 0, ni = m_links.size(); i < ni; ++i)\n\t\t{\n\t\t\tfor (int j = 0; j < 2; ++j) ranks[int(m_links[i].m_n[j] - nbase)]++;\n\t\t}\n\t\tfor (i = 0, ni = m_faces.size(); i < ni; ++i)\n\t\t{\n\t\t\tfor (int j = 0; j < 3; ++j) ranks[int(m_faces[i].m_n[j] - nbase)]++;\n\t\t}\n\t\tfor (i = 0; i < m_links.size(); ++i)\n\t\t{\n\t\t\tconst int id[] = {int(m_links[i].m_n[0] - nbase),\n\t\t\t\t\t\t\t  int(m_links[i].m_n[1] - nbase)};\n\t\t\tconst bool sg[] = {ranks[id[0]] == 1,\n\t\t\t\t\t\t\t   ranks[id[1]] == 1};\n\t\t\tif (sg[0] || sg[1])\n\t\t\t{\n\t\t\t\t--ranks[id[0]];\n\t\t\t\t--ranks[id[1]];\n\t\t\t\tbtSwap(m_links[i], m_links[m_links.size() - 1]);\n\t\t\t\tm_links.pop_back();\n\t\t\t\t--i;\n\t\t\t}\n\t\t}\n#if 0\t\n\t\tfor(i=nnodes-1;i>=0;--i)\n\t\t{\n\t\t\tif(!ranks[i]) todelete.push_back(i);\n\t\t}\t\n\t\tif(todelete.size())\n\t\t{\t\t\n\t\t\tbtAlignedObjectArray<int>&\tmap=ranks;\n\t\t\tfor(int i=0;i<nnodes;++i) map[i]=i;\n\t\t\tPointersToIndices(this);\n\t\t\tfor(int i=0,ni=todelete.size();i<ni;++i)\n\t\t\t{\n\t\t\t\tint\t\tj=todelete[i];\n\t\t\t\tint&\ta=map[j];\n\t\t\t\tint&\tb=map[--nnodes];\n\t\t\t\tm_ndbvt.remove(m_nodes[a].m_leaf);m_nodes[a].m_leaf=0;\n\t\t\t\tbtSwap(m_nodes[a],m_nodes[b]);\n\t\t\t\tj=a;a=b;b=j;\t\t\t\n\t\t\t}\n\t\t\tIndicesToPointers(this,&map[0]);\n\t\t\tm_nodes.resize(nnodes);\n\t\t}\n#endif\n\t}\n\tm_bUpdateRtCst = true;\n}\n\n//\nbool btSoftBody::cutLink(const Node* node0, const Node* node1, btScalar position)\n{\n\treturn (cutLink(int(node0 - &m_nodes[0]), int(node1 - &m_nodes[0]), position));\n}\n\n//\nbool btSoftBody::cutLink(int node0, int node1, btScalar position)\n{\n\tbool done = false;\n\tint i, ni;\n\t//\tconst btVector3\td=m_nodes[node0].m_x-m_nodes[node1].m_x;\n\tconst btVector3 x = Lerp(m_nodes[node0].m_x, m_nodes[node1].m_x, position);\n\tconst btVector3 v = Lerp(m_nodes[node0].m_v, m_nodes[node1].m_v, position);\n\tconst btScalar m = 1;\n\tappendNode(x, m);\n\tappendNode(x, m);\n\tNode* pa = &m_nodes[node0];\n\tNode* pb = &m_nodes[node1];\n\tNode* pn[2] = {&m_nodes[m_nodes.size() - 2],\n\t\t\t\t   &m_nodes[m_nodes.size() - 1]};\n\tpn[0]->m_v = v;\n\tpn[1]->m_v = v;\n\tfor (i = 0, ni = m_links.size(); i < ni; ++i)\n\t{\n\t\tconst int mtch = MatchEdge(m_links[i].m_n[0], m_links[i].m_n[1], pa, pb);\n\t\tif (mtch != -1)\n\t\t{\n\t\t\tappendLink(i);\n\t\t\tLink* pft[] = {&m_links[i], &m_links[m_links.size() - 1]};\n\t\t\tpft[0]->m_n[1] = pn[mtch];\n\t\t\tpft[1]->m_n[0] = pn[1 - mtch];\n\t\t\tdone = true;\n\t\t}\n\t}\n\tfor (i = 0, ni = m_faces.size(); i < ni; ++i)\n\t{\n\t\tfor (int k = 2, l = 0; l < 3; k = l++)\n\t\t{\n\t\t\tconst int mtch = MatchEdge(m_faces[i].m_n[k], m_faces[i].m_n[l], pa, pb);\n\t\t\tif (mtch != -1)\n\t\t\t{\n\t\t\t\tappendFace(i);\n\t\t\t\tFace* pft[] = {&m_faces[i], &m_faces[m_faces.size() - 1]};\n\t\t\t\tpft[0]->m_n[l] = pn[mtch];\n\t\t\t\tpft[1]->m_n[k] = pn[1 - mtch];\n\t\t\t\tappendLink(pn[0], pft[0]->m_n[(l + 1) % 3], pft[0]->m_material, true);\n\t\t\t\tappendLink(pn[1], pft[0]->m_n[(l + 1) % 3], pft[0]->m_material, true);\n\t\t\t}\n\t\t}\n\t}\n\tif (!done)\n\t{\n\t\tm_ndbvt.remove(pn[0]->m_leaf);\n\t\tm_ndbvt.remove(pn[1]->m_leaf);\n\t\tm_nodes.pop_back();\n\t\tm_nodes.pop_back();\n\t}\n\treturn (done);\n}\n\n//\nbool btSoftBody::rayTest(const btVector3& rayFrom,\n\t\t\t\t\t\t const btVector3& rayTo,\n\t\t\t\t\t\t sRayCast& results)\n{\n\tif (m_faces.size() && m_fdbvt.empty())\n\t\tinitializeFaceTree();\n\n\tresults.body = this;\n\tresults.fraction = 1.f;\n\tresults.feature = eFeature::None;\n\tresults.index = -1;\n\n\treturn (rayTest(rayFrom, rayTo, results.fraction, results.feature, results.index, false) != 0);\n}\n\n//\nvoid btSoftBody::setSolver(eSolverPresets::_ preset)\n{\n\tm_cfg.m_vsequence.clear();\n\tm_cfg.m_psequence.clear();\n\tm_cfg.m_dsequence.clear();\n\tswitch (preset)\n\t{\n\t\tcase eSolverPresets::Positions:\n\t\t\tm_cfg.m_psequence.push_back(ePSolver::Anchors);\n\t\t\tm_cfg.m_psequence.push_back(ePSolver::RContacts);\n\t\t\tm_cfg.m_psequence.push_back(ePSolver::SContacts);\n\t\t\tm_cfg.m_psequence.push_back(ePSolver::Linear);\n\t\t\tbreak;\n\t\tcase eSolverPresets::Velocities:\n\t\t\tm_cfg.m_vsequence.push_back(eVSolver::Linear);\n\n\t\t\tm_cfg.m_psequence.push_back(ePSolver::Anchors);\n\t\t\tm_cfg.m_psequence.push_back(ePSolver::RContacts);\n\t\t\tm_cfg.m_psequence.push_back(ePSolver::SContacts);\n\n\t\t\tm_cfg.m_dsequence.push_back(ePSolver::Linear);\n\t\t\tbreak;\n\t}\n}\n\nvoid btSoftBody::predictMotion(btScalar dt)\n{\n    int i, ni;\n    \n    /* Update                */\n    if (m_bUpdateRtCst)\n    {\n        m_bUpdateRtCst = false;\n        updateConstants();\n        m_fdbvt.clear();\n        if (m_cfg.collisions & fCollision::VF_SS)\n        {\n            initializeFaceTree();\n        }\n    }\n    \n    /* Prepare                */\n    m_sst.sdt = dt * m_cfg.timescale;\n    m_sst.isdt = 1 / m_sst.sdt;\n    m_sst.velmrg = m_sst.sdt * 3;\n    m_sst.radmrg = getCollisionShape()->getMargin();\n    m_sst.updmrg = m_sst.radmrg * (btScalar)0.25;\n    /* Forces                */\n    addVelocity(m_worldInfo->m_gravity * m_sst.sdt);\n    applyForces();\n    /* Integrate            */\n    for (i = 0, ni = m_nodes.size(); i < ni; ++i)\n    {\n        Node& n = m_nodes[i];\n        n.m_q = n.m_x;\n        btVector3 deltaV = n.m_f * n.m_im * m_sst.sdt;\n        {\n            btScalar maxDisplacement = m_worldInfo->m_maxDisplacement;\n            btScalar clampDeltaV = maxDisplacement / m_sst.sdt;\n            for (int c = 0; c < 3; c++)\n            {\n                if (deltaV[c] > clampDeltaV)\n                {\n                    deltaV[c] = clampDeltaV;\n                }\n                if (deltaV[c] < -clampDeltaV)\n                {\n                    deltaV[c] = -clampDeltaV;\n                }\n            }\n        }\n        n.m_v += deltaV;\n        n.m_x += n.m_v * m_sst.sdt;\n        n.m_f = btVector3(0, 0, 0);\n    }\n    /* Clusters                */\n    updateClusters();\n    /* Bounds                */\n    updateBounds();\n    /* Nodes                */\n    ATTRIBUTE_ALIGNED16(btDbvtVolume)\n    vol;\n    for (i = 0, ni = m_nodes.size(); i < ni; ++i)\n    {\n        Node& n = m_nodes[i];\n        vol = btDbvtVolume::FromCR(n.m_x, m_sst.radmrg);\n        m_ndbvt.update(n.m_leaf,\n                       vol,\n                       n.m_v * m_sst.velmrg,\n                       m_sst.updmrg);\n    }\n    /* Faces                */\n    if (!m_fdbvt.empty())\n    {\n        for (int i = 0; i < m_faces.size(); ++i)\n        {\n            Face& f = m_faces[i];\n            const btVector3 v = (f.m_n[0]->m_v +\n                                 f.m_n[1]->m_v +\n                                 f.m_n[2]->m_v) /\n            3;\n            vol = VolumeOf(f, m_sst.radmrg);\n            m_fdbvt.update(f.m_leaf,\n                           vol,\n                           v * m_sst.velmrg,\n                           m_sst.updmrg);\n        }\n    }\n    /* Pose                    */\n    updatePose();\n    /* Match                */\n    if (m_pose.m_bframe && (m_cfg.kMT > 0))\n    {\n        const btMatrix3x3 posetrs = m_pose.m_rot;\n        for (int i = 0, ni = m_nodes.size(); i < ni; ++i)\n        {\n            Node& n = m_nodes[i];\n            if (n.m_im > 0)\n            {\n                const btVector3 x = posetrs * m_pose.m_pos[i] + m_pose.m_com;\n                n.m_x = Lerp(n.m_x, x, m_cfg.kMT);\n            }\n        }\n    }\n    /* Clear contacts        */\n    m_rcontacts.resize(0);\n    m_scontacts.resize(0);\n    /* Optimize dbvt's        */\n    m_ndbvt.optimizeIncremental(1);\n    m_fdbvt.optimizeIncremental(1);\n    m_cdbvt.optimizeIncremental(1);\n}\n\n\n//\nvoid btSoftBody::solveConstraints()\n{\n\t/* Apply clusters\t\t*/\n\tapplyClusters(false);\n\t/* Prepare links\t\t*/\n\n\tint i, ni;\n\n\tfor (i = 0, ni = m_links.size(); i < ni; ++i)\n\t{\n\t\tLink& l = m_links[i];\n\t\tl.m_c3 = l.m_n[1]->m_q - l.m_n[0]->m_q;\n\t\tl.m_c2 = 1 / (l.m_c3.length2() * l.m_c0);\n\t}\n\t/* Prepare anchors\t\t*/\n\tfor (i = 0, ni = m_anchors.size(); i < ni; ++i)\n\t{\n\t\tAnchor& a = m_anchors[i];\n\t\tconst btVector3 ra = a.m_body->getWorldTransform().getBasis() * a.m_local;\n\t\ta.m_c0 = ImpulseMatrix(m_sst.sdt,\n\t\t\t\t\t\t\t   a.m_node->m_im,\n\t\t\t\t\t\t\t   a.m_body->getInvMass(),\n\t\t\t\t\t\t\t   a.m_body->getInvInertiaTensorWorld(),\n\t\t\t\t\t\t\t   ra);\n\t\ta.m_c1 = ra;\n\t\ta.m_c2 = m_sst.sdt * a.m_node->m_im;\n\t\ta.m_body->activate();\n\t}\n\t/* Solve velocities\t\t*/\n\tif (m_cfg.viterations > 0)\n\t{\n\t\t/* Solve\t\t\t*/\n\t\tfor (int isolve = 0; isolve < m_cfg.viterations; ++isolve)\n\t\t{\n\t\t\tfor (int iseq = 0; iseq < m_cfg.m_vsequence.size(); ++iseq)\n\t\t\t{\n\t\t\t\tgetSolver(m_cfg.m_vsequence[iseq])(this, 1);\n\t\t\t}\n\t\t}\n\t\t/* Update\t\t\t*/\n\t\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t\t{\n\t\t\tNode& n = m_nodes[i];\n\t\t\tn.m_x = n.m_q + n.m_v * m_sst.sdt;\n\t\t}\n\t}\n\t/* Solve positions\t\t*/\n\tif (m_cfg.piterations > 0)\n\t{\n\t\tfor (int isolve = 0; isolve < m_cfg.piterations; ++isolve)\n\t\t{\n\t\t\tconst btScalar ti = isolve / (btScalar)m_cfg.piterations;\n\t\t\tfor (int iseq = 0; iseq < m_cfg.m_psequence.size(); ++iseq)\n\t\t\t{\n\t\t\t\tgetSolver(m_cfg.m_psequence[iseq])(this, 1, ti);\n\t\t\t}\n\t\t}\n\t\tconst btScalar vc = m_sst.isdt * (1 - m_cfg.kDP);\n\t\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t\t{\n\t\t\tNode& n = m_nodes[i];\n\t\t\tn.m_v = (n.m_x - n.m_q) * vc;\n\t\t\tn.m_f = btVector3(0, 0, 0);\n\t\t}\n\t}\n\t/* Solve drift\t\t\t*/\n\tif (m_cfg.diterations > 0)\n\t{\n\t\tconst btScalar vcf = m_cfg.kVCF * m_sst.isdt;\n\t\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t\t{\n\t\t\tNode& n = m_nodes[i];\n\t\t\tn.m_q = n.m_x;\n\t\t}\n\t\tfor (int idrift = 0; idrift < m_cfg.diterations; ++idrift)\n\t\t{\n\t\t\tfor (int iseq = 0; iseq < m_cfg.m_dsequence.size(); ++iseq)\n\t\t\t{\n\t\t\t\tgetSolver(m_cfg.m_dsequence[iseq])(this, 1, 0);\n\t\t\t}\n\t\t}\n\t\tfor (int i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t\t{\n\t\t\tNode& n = m_nodes[i];\n\t\t\tn.m_v += (n.m_x - n.m_q) * vcf;\n\t\t}\n\t}\n\t/* Apply clusters\t\t*/\n\tdampClusters();\n\tapplyClusters(true);\n}\n\n//\nvoid btSoftBody::staticSolve(int iterations)\n{\n\tfor (int isolve = 0; isolve < iterations; ++isolve)\n\t{\n\t\tfor (int iseq = 0; iseq < m_cfg.m_psequence.size(); ++iseq)\n\t\t{\n\t\t\tgetSolver(m_cfg.m_psequence[iseq])(this, 1, 0);\n\t\t}\n\t}\n}\n\n//\nvoid btSoftBody::solveCommonConstraints(btSoftBody** /*bodies*/, int /*count*/, int /*iterations*/)\n{\n\t/// placeholder\n}\n\n//\nvoid btSoftBody::solveClusters(const btAlignedObjectArray<btSoftBody*>& bodies)\n{\n\tconst int nb = bodies.size();\n\tint iterations = 0;\n\tint i;\n\n\tfor (i = 0; i < nb; ++i)\n\t{\n\t\titerations = btMax(iterations, bodies[i]->m_cfg.citerations);\n\t}\n\tfor (i = 0; i < nb; ++i)\n\t{\n\t\tbodies[i]->prepareClusters(iterations);\n\t}\n\tfor (i = 0; i < iterations; ++i)\n\t{\n\t\tconst btScalar sor = 1;\n\t\tfor (int j = 0; j < nb; ++j)\n\t\t{\n\t\t\tbodies[j]->solveClusters(sor);\n\t\t}\n\t}\n\tfor (i = 0; i < nb; ++i)\n\t{\n\t\tbodies[i]->cleanupClusters();\n\t}\n}\n\n//\nvoid btSoftBody::integrateMotion()\n{\n\t/* Update\t\t\t*/\n\tupdateNormals();\n}\n\n//\nbtSoftBody::RayFromToCaster::RayFromToCaster(const btVector3& rayFrom, const btVector3& rayTo, btScalar mxt)\n{\n\tm_rayFrom = rayFrom;\n\tm_rayNormalizedDirection = (rayTo - rayFrom);\n\tm_rayTo = rayTo;\n\tm_mint = mxt;\n\tm_face = 0;\n\tm_tests = 0;\n}\n\n//\nvoid btSoftBody::RayFromToCaster::Process(const btDbvtNode* leaf)\n{\n\tbtSoftBody::Face& f = *(btSoftBody::Face*)leaf->data;\n\tconst btScalar t = rayFromToTriangle(m_rayFrom, m_rayTo, m_rayNormalizedDirection,\n\t\t\t\t\t\t\t\t\t\t f.m_n[0]->m_x,\n\t\t\t\t\t\t\t\t\t\t f.m_n[1]->m_x,\n\t\t\t\t\t\t\t\t\t\t f.m_n[2]->m_x,\n\t\t\t\t\t\t\t\t\t\t m_mint);\n\tif ((t > 0) && (t < m_mint))\n\t{\n\t\tm_mint = t;\n\t\tm_face = &f;\n\t}\n\t++m_tests;\n}\n\n//\nbtScalar btSoftBody::RayFromToCaster::rayFromToTriangle(const btVector3& rayFrom,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tconst btVector3& rayTo,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tconst btVector3& rayNormalizedDirection,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tconst btVector3& a,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tconst btVector3& b,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tconst btVector3& c,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tbtScalar maxt)\n{\n\tstatic const btScalar ceps = -SIMD_EPSILON * 10;\n\tstatic const btScalar teps = SIMD_EPSILON * 10;\n\n\tconst btVector3 n = btCross(b - a, c - a);\n\tconst btScalar d = btDot(a, n);\n\tconst btScalar den = btDot(rayNormalizedDirection, n);\n\tif (!btFuzzyZero(den))\n\t{\n\t\tconst btScalar num = btDot(rayFrom, n) - d;\n\t\tconst btScalar t = -num / den;\n\t\tif ((t > teps) && (t < maxt))\n\t\t{\n\t\t\tconst btVector3 hit = rayFrom + rayNormalizedDirection * t;\n\t\t\tif ((btDot(n, btCross(a - hit, b - hit)) > ceps) &&\n\t\t\t\t(btDot(n, btCross(b - hit, c - hit)) > ceps) &&\n\t\t\t\t(btDot(n, btCross(c - hit, a - hit)) > ceps))\n\t\t\t{\n\t\t\t\treturn (t);\n\t\t\t}\n\t\t}\n\t}\n\treturn (-1);\n}\n\n//\nvoid btSoftBody::pointersToIndices()\n{\n#define PTR2IDX(_p_, _b_) reinterpret_cast<btSoftBody::Node*>((_p_) - (_b_))\n\tbtSoftBody::Node* base = m_nodes.size() ? &m_nodes[0] : 0;\n\tint i, ni;\n\n\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t{\n\t\tif (m_nodes[i].m_leaf)\n\t\t{\n\t\t\tm_nodes[i].m_leaf->data = *(void**)&i;\n\t\t}\n\t}\n\tfor (i = 0, ni = m_links.size(); i < ni; ++i)\n\t{\n\t\tm_links[i].m_n[0] = PTR2IDX(m_links[i].m_n[0], base);\n\t\tm_links[i].m_n[1] = PTR2IDX(m_links[i].m_n[1], base);\n\t}\n\tfor (i = 0, ni = m_faces.size(); i < ni; ++i)\n\t{\n\t\tm_faces[i].m_n[0] = PTR2IDX(m_faces[i].m_n[0], base);\n\t\tm_faces[i].m_n[1] = PTR2IDX(m_faces[i].m_n[1], base);\n\t\tm_faces[i].m_n[2] = PTR2IDX(m_faces[i].m_n[2], base);\n\t\tif (m_faces[i].m_leaf)\n\t\t{\n\t\t\tm_faces[i].m_leaf->data = *(void**)&i;\n\t\t}\n\t}\n\tfor (i = 0, ni = m_anchors.size(); i < ni; ++i)\n\t{\n\t\tm_anchors[i].m_node = PTR2IDX(m_anchors[i].m_node, base);\n\t}\n\tfor (i = 0, ni = m_notes.size(); i < ni; ++i)\n\t{\n\t\tfor (int j = 0; j < m_notes[i].m_rank; ++j)\n\t\t{\n\t\t\tm_notes[i].m_nodes[j] = PTR2IDX(m_notes[i].m_nodes[j], base);\n\t\t}\n\t}\n#undef PTR2IDX\n}\n\n//\nvoid btSoftBody::indicesToPointers(const int* map)\n{\n#define IDX2PTR(_p_, _b_) map ? (&(_b_)[map[(((char*)_p_) - (char*)0)]]) : (&(_b_)[(((char*)_p_) - (char*)0)])\n\tbtSoftBody::Node* base = m_nodes.size() ? &m_nodes[0] : 0;\n\tint i, ni;\n\n\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t{\n\t\tif (m_nodes[i].m_leaf)\n\t\t{\n\t\t\tm_nodes[i].m_leaf->data = &m_nodes[i];\n\t\t}\n\t}\n\tfor (i = 0, ni = m_links.size(); i < ni; ++i)\n\t{\n\t\tm_links[i].m_n[0] = IDX2PTR(m_links[i].m_n[0], base);\n\t\tm_links[i].m_n[1] = IDX2PTR(m_links[i].m_n[1], base);\n\t}\n\tfor (i = 0, ni = m_faces.size(); i < ni; ++i)\n\t{\n\t\tm_faces[i].m_n[0] = IDX2PTR(m_faces[i].m_n[0], base);\n\t\tm_faces[i].m_n[1] = IDX2PTR(m_faces[i].m_n[1], base);\n\t\tm_faces[i].m_n[2] = IDX2PTR(m_faces[i].m_n[2], base);\n\t\tif (m_faces[i].m_leaf)\n\t\t{\n\t\t\tm_faces[i].m_leaf->data = &m_faces[i];\n\t\t}\n\t}\n\tfor (i = 0, ni = m_anchors.size(); i < ni; ++i)\n\t{\n\t\tm_anchors[i].m_node = IDX2PTR(m_anchors[i].m_node, base);\n\t}\n\tfor (i = 0, ni = m_notes.size(); i < ni; ++i)\n\t{\n\t\tfor (int j = 0; j < m_notes[i].m_rank; ++j)\n\t\t{\n\t\t\tm_notes[i].m_nodes[j] = IDX2PTR(m_notes[i].m_nodes[j], base);\n\t\t}\n\t}\n#undef IDX2PTR\n}\n\n//\nint btSoftBody::rayTest(const btVector3& rayFrom, const btVector3& rayTo,\n\t\t\t\t\t\tbtScalar& mint, eFeature::_& feature, int& index, bool bcountonly) const\n{\n\tint cnt = 0;\n\tbtVector3 dir = rayTo - rayFrom;\n\n\tif (bcountonly || m_fdbvt.empty())\n\t{ /* Full search\t*/\n\n\t\tfor (int i = 0, ni = m_faces.size(); i < ni; ++i)\n\t\t{\n\t\t\tconst btSoftBody::Face& f = m_faces[i];\n\n\t\t\tconst btScalar t = RayFromToCaster::rayFromToTriangle(rayFrom, rayTo, dir,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  f.m_n[0]->m_x,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  f.m_n[1]->m_x,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  f.m_n[2]->m_x,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  mint);\n\t\t\tif (t > 0)\n\t\t\t{\n\t\t\t\t++cnt;\n\t\t\t\tif (!bcountonly)\n\t\t\t\t{\n\t\t\t\t\tfeature = btSoftBody::eFeature::Face;\n\t\t\t\t\tindex = i;\n\t\t\t\t\tmint = t;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\telse\n\t{ /* Use dbvt\t*/\n\t\tRayFromToCaster collider(rayFrom, rayTo, mint);\n\n\t\tbtDbvt::rayTest(m_fdbvt.m_root, rayFrom, rayTo, collider);\n\t\tif (collider.m_face)\n\t\t{\n\t\t\tmint = collider.m_mint;\n\t\t\tfeature = btSoftBody::eFeature::Face;\n\t\t\tindex = (int)(collider.m_face - &m_faces[0]);\n\t\t\tcnt = 1;\n\t\t}\n\t}\n\n\tfor (int i = 0; i < m_tetras.size(); i++)\n\t{\n\t\tconst btSoftBody::Tetra& tet = m_tetras[i];\n\t\tint tetfaces[4][3] = {{0, 1, 2}, {0, 1, 3}, {1, 2, 3}, {0, 2, 3}};\n\t\tfor (int f = 0; f < 4; f++)\n\t\t{\n\t\t\tint index0 = tetfaces[f][0];\n\t\t\tint index1 = tetfaces[f][1];\n\t\t\tint index2 = tetfaces[f][2];\n\t\t\tbtVector3 v0 = tet.m_n[index0]->m_x;\n\t\t\tbtVector3 v1 = tet.m_n[index1]->m_x;\n\t\t\tbtVector3 v2 = tet.m_n[index2]->m_x;\n\n\t\t\tconst btScalar t = RayFromToCaster::rayFromToTriangle(rayFrom, rayTo, dir,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  v0, v1, v2,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  mint);\n\t\t\tif (t > 0)\n\t\t\t{\n\t\t\t\t++cnt;\n\t\t\t\tif (!bcountonly)\n\t\t\t\t{\n\t\t\t\t\tfeature = btSoftBody::eFeature::Tetra;\n\t\t\t\t\tindex = i;\n\t\t\t\t\tmint = t;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn (cnt);\n}\n\n//\nvoid btSoftBody::initializeFaceTree()\n{\n\tm_fdbvt.clear();\n\tfor (int i = 0; i < m_faces.size(); ++i)\n\t{\n\t\tFace& f = m_faces[i];\n\t\tf.m_leaf = m_fdbvt.insert(VolumeOf(f, 0), &f);\n\t}\n}\n\n//\nbtVector3 btSoftBody::evaluateCom() const\n{\n\tbtVector3 com(0, 0, 0);\n\tif (m_pose.m_bframe)\n\t{\n\t\tfor (int i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t\t{\n\t\t\tcom += m_nodes[i].m_x * m_pose.m_wgh[i];\n\t\t}\n\t}\n\treturn (com);\n}\n\nbool btSoftBody::checkContact(const btCollisionObjectWrapper* colObjWrap,\n                              const btVector3& x,\n                              btScalar margin,\n                              btSoftBody::sCti& cti) const\n{\n    btVector3 nrm;\n    const btCollisionShape* shp = colObjWrap->getCollisionShape();\n    //    const btRigidBody *tmpRigid = btRigidBody::upcast(colObjWrap->getCollisionObject());\n    //const btTransform &wtr = tmpRigid ? tmpRigid->getWorldTransform() : colObjWrap->getWorldTransform();\n    const btTransform& wtr = colObjWrap->getWorldTransform();\n    //todo: check which transform is needed here\n    \n    btScalar dst =\n    m_worldInfo->m_sparsesdf.Evaluate(\n                                      wtr.invXform(x),\n                                      shp,\n                                      nrm,\n                                      margin);\n    if (dst < 0)\n    {\n        cti.m_colObj = colObjWrap->getCollisionObject();\n        cti.m_normal = wtr.getBasis() * nrm;\n        cti.m_offset = -btDot(cti.m_normal, x - cti.m_normal * dst);\n        return (true);\n    }\n    return (false);\n}\n//\nbool btSoftBody::checkDeformableContact(const btCollisionObjectWrapper* colObjWrap,\n\t\t\t\t\t\t\t  const btVector3& x,\n\t\t\t\t\t\t\t  btScalar margin,\n\t\t\t\t\t\t\t  btSoftBody::sCti& cti, bool predict) const\n{\n\tbtVector3 nrm;\n\tconst btCollisionShape* shp = colObjWrap->getCollisionShape();\n    const btCollisionObject* tmpCollisionObj = colObjWrap->getCollisionObject();\n    // use the position x_{n+1}^* = x_n + dt * v_{n+1}^* where v_{n+1}^* = v_n + dtg for collision detect\n    // but resolve contact at x_n\n    btTransform wtr = (predict) ?\n    (colObjWrap->m_preTransform != NULL ? tmpCollisionObj->getInterpolationWorldTransform()*(*colObjWrap->m_preTransform) : tmpCollisionObj->getInterpolationWorldTransform())\n                 : colObjWrap->getWorldTransform();\n\tbtScalar dst =\n\t\tm_worldInfo->m_sparsesdf.Evaluate(\n\t\t\twtr.invXform(x),\n\t\t\tshp,\n\t\t\tnrm,\n\t\t\tmargin);\n\tif (!predict)\n\t{\n\t\tcti.m_colObj = colObjWrap->getCollisionObject();\n\t\tcti.m_normal = wtr.getBasis() * nrm;\n        cti.m_offset = dst;\n\t}\n    if (dst < 0)\n        return true;\n\treturn (false);\n}\n\n//\nvoid btSoftBody::updateNormals()\n{\n\tconst btVector3 zv(0, 0, 0);\n\tint i, ni;\n\n\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t{\n\t\tm_nodes[i].m_n = zv;\n\t}\n\tfor (i = 0, ni = m_faces.size(); i < ni; ++i)\n\t{\n\t\tbtSoftBody::Face& f = m_faces[i];\n\t\tconst btVector3 n = btCross(f.m_n[1]->m_x - f.m_n[0]->m_x,\n\t\t\t\t\t\t\t\t\tf.m_n[2]->m_x - f.m_n[0]->m_x);\n\t\tf.m_normal = n.normalized();\n\t\tf.m_n[0]->m_n += n;\n\t\tf.m_n[1]->m_n += n;\n\t\tf.m_n[2]->m_n += n;\n\t}\n\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t{\n\t\tbtScalar len = m_nodes[i].m_n.length();\n\t\tif (len > SIMD_EPSILON)\n\t\t\tm_nodes[i].m_n /= len;\n\t}\n}\n\n//\nvoid btSoftBody::updateBounds()\n{\n\t/*if( m_acceleratedSoftBody )\n\t{\n\t\t// If we have an accelerated softbody we need to obtain the bounds correctly\n\t\t// For now (slightly hackily) just have a very large AABB\n\t\t// TODO: Write get bounds kernel\n\t\t// If that is updating in place, atomic collisions might be low (when the cloth isn't perfectly aligned to an axis) and we could\n\t\t// probably do a test and exchange reasonably efficiently.\n\n\t\tm_bounds[0] = btVector3(-1000, -1000, -1000);\n\t\tm_bounds[1] = btVector3(1000, 1000, 1000);\n\n\t} else {*/\n\tif (m_ndbvt.m_root)\n\t{\n\t\tconst btVector3& mins = m_ndbvt.m_root->volume.Mins();\n\t\tconst btVector3& maxs = m_ndbvt.m_root->volume.Maxs();\n\t\tconst btScalar csm = getCollisionShape()->getMargin();\n\t\tconst btVector3 mrg = btVector3(csm,\n\t\t\t\t\t\t\t\t\t\tcsm,\n\t\t\t\t\t\t\t\t\t\tcsm) *\n\t\t\t\t\t\t\t  1;  // ??? to investigate...\n\t\tm_bounds[0] = mins - mrg;\n\t\tm_bounds[1] = maxs + mrg;\n\t\tif (0 != getBroadphaseHandle())\n\t\t{\n\t\t\tm_worldInfo->m_broadphase->setAabb(getBroadphaseHandle(),\n\t\t\t\t\t\t\t\t\t\t\t   m_bounds[0],\n\t\t\t\t\t\t\t\t\t\t\t   m_bounds[1],\n\t\t\t\t\t\t\t\t\t\t\t   m_worldInfo->m_dispatcher);\n\t\t}\n\t}\n\telse\n\t{\n\t\tm_bounds[0] =\n\t\t\tm_bounds[1] = btVector3(0, 0, 0);\n\t}\n\t//}\n}\n\n//\nvoid btSoftBody::updatePose()\n{\n\tif (m_pose.m_bframe)\n\t{\n\t\tbtSoftBody::Pose& pose = m_pose;\n\t\tconst btVector3 com = evaluateCom();\n\t\t/* Com\t\t\t*/\n\t\tpose.m_com = com;\n\t\t/* Rotation\t\t*/\n\t\tbtMatrix3x3 Apq;\n\t\tconst btScalar eps = SIMD_EPSILON;\n\t\tApq[0] = Apq[1] = Apq[2] = btVector3(0, 0, 0);\n\t\tApq[0].setX(eps);\n\t\tApq[1].setY(eps * 2);\n\t\tApq[2].setZ(eps * 3);\n\t\tfor (int i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t\t{\n\t\t\tconst btVector3 a = pose.m_wgh[i] * (m_nodes[i].m_x - com);\n\t\t\tconst btVector3& b = pose.m_pos[i];\n\t\t\tApq[0] += a.x() * b;\n\t\t\tApq[1] += a.y() * b;\n\t\t\tApq[2] += a.z() * b;\n\t\t}\n\t\tbtMatrix3x3 r, s;\n\t\tPolarDecompose(Apq, r, s);\n\t\tpose.m_rot = r;\n\t\tpose.m_scl = pose.m_aqq * r.transpose() * Apq;\n\t\tif (m_cfg.maxvolume > 1)\n\t\t{\n\t\t\tconst btScalar idet = Clamp<btScalar>(1 / pose.m_scl.determinant(),\n\t\t\t\t\t\t\t\t\t\t\t\t  1, m_cfg.maxvolume);\n\t\t\tpose.m_scl = Mul(pose.m_scl, idet);\n\t\t}\n\t}\n}\n\n//\nvoid btSoftBody::updateArea(bool averageArea)\n{\n\tint i, ni;\n\n\t/* Face area\t\t*/\n\tfor (i = 0, ni = m_faces.size(); i < ni; ++i)\n\t{\n\t\tFace& f = m_faces[i];\n\t\tf.m_ra = AreaOf(f.m_n[0]->m_x, f.m_n[1]->m_x, f.m_n[2]->m_x);\n\t}\n\n\t/* Node area\t\t*/\n\n\tif (averageArea)\n\t{\n\t\tbtAlignedObjectArray<int> counts;\n\t\tcounts.resize(m_nodes.size(), 0);\n\t\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t\t{\n\t\t\tm_nodes[i].m_area = 0;\n\t\t}\n\t\tfor (i = 0, ni = m_faces.size(); i < ni; ++i)\n\t\t{\n\t\t\tbtSoftBody::Face& f = m_faces[i];\n\t\t\tfor (int j = 0; j < 3; ++j)\n\t\t\t{\n\t\t\t\tconst int index = (int)(f.m_n[j] - &m_nodes[0]);\n\t\t\t\tcounts[index]++;\n\t\t\t\tf.m_n[j]->m_area += btFabs(f.m_ra);\n\t\t\t}\n\t\t}\n\t\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t\t{\n\t\t\tif (counts[i] > 0)\n\t\t\t\tm_nodes[i].m_area /= (btScalar)counts[i];\n\t\t\telse\n\t\t\t\tm_nodes[i].m_area = 0;\n\t\t}\n\t}\n\telse\n\t{\n\t\t// initialize node area as zero\n\t\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t\t{\n\t\t\tm_nodes[i].m_area = 0;\n\t\t}\n\n\t\tfor (i = 0, ni = m_faces.size(); i < ni; ++i)\n\t\t{\n\t\t\tbtSoftBody::Face& f = m_faces[i];\n\n\t\t\tfor (int j = 0; j < 3; ++j)\n\t\t\t{\n\t\t\t\tf.m_n[j]->m_area += f.m_ra;\n\t\t\t}\n\t\t}\n\n\t\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t\t{\n\t\t\tm_nodes[i].m_area *= 0.3333333f;\n\t\t}\n\t}\n}\n\nvoid btSoftBody::updateLinkConstants()\n{\n\tint i, ni;\n\n\t/* Links\t\t*/\n\tfor (i = 0, ni = m_links.size(); i < ni; ++i)\n\t{\n\t\tLink& l = m_links[i];\n\t\tMaterial& m = *l.m_material;\n\t\tl.m_c0 = (l.m_n[0]->m_im + l.m_n[1]->m_im) / m.m_kLST;\n\t}\n}\n\nvoid btSoftBody::updateConstants()\n{\n\tresetLinkRestLengths();\n\tupdateLinkConstants();\n\tupdateArea();\n}\n\n//\nvoid btSoftBody::initializeClusters()\n{\n\tint i;\n\n\tfor (i = 0; i < m_clusters.size(); ++i)\n\t{\n\t\tCluster& c = *m_clusters[i];\n\t\tc.m_imass = 0;\n\t\tc.m_masses.resize(c.m_nodes.size());\n\t\tfor (int j = 0; j < c.m_nodes.size(); ++j)\n\t\t{\n\t\t\tif (c.m_nodes[j]->m_im == 0)\n\t\t\t{\n\t\t\t\tc.m_containsAnchor = true;\n\t\t\t\tc.m_masses[j] = BT_LARGE_FLOAT;\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tc.m_masses[j] = btScalar(1.) / c.m_nodes[j]->m_im;\n\t\t\t}\n\t\t\tc.m_imass += c.m_masses[j];\n\t\t}\n\t\tc.m_imass = btScalar(1.) / c.m_imass;\n\t\tc.m_com = btSoftBody::clusterCom(&c);\n\t\tc.m_lv = btVector3(0, 0, 0);\n\t\tc.m_av = btVector3(0, 0, 0);\n\t\tc.m_leaf = 0;\n\t\t/* Inertia\t*/\n\t\tbtMatrix3x3& ii = c.m_locii;\n\t\tii[0] = ii[1] = ii[2] = btVector3(0, 0, 0);\n\t\t{\n\t\t\tint i, ni;\n\n\t\t\tfor (i = 0, ni = c.m_nodes.size(); i < ni; ++i)\n\t\t\t{\n\t\t\t\tconst btVector3 k = c.m_nodes[i]->m_x - c.m_com;\n\t\t\t\tconst btVector3 q = k * k;\n\t\t\t\tconst btScalar m = c.m_masses[i];\n\t\t\t\tii[0][0] += m * (q[1] + q[2]);\n\t\t\t\tii[1][1] += m * (q[0] + q[2]);\n\t\t\t\tii[2][2] += m * (q[0] + q[1]);\n\t\t\t\tii[0][1] -= m * k[0] * k[1];\n\t\t\t\tii[0][2] -= m * k[0] * k[2];\n\t\t\t\tii[1][2] -= m * k[1] * k[2];\n\t\t\t}\n\t\t}\n\t\tii[1][0] = ii[0][1];\n\t\tii[2][0] = ii[0][2];\n\t\tii[2][1] = ii[1][2];\n\n\t\tii = ii.inverse();\n\n\t\t/* Frame\t*/\n\t\tc.m_framexform.setIdentity();\n\t\tc.m_framexform.setOrigin(c.m_com);\n\t\tc.m_framerefs.resize(c.m_nodes.size());\n\t\t{\n\t\t\tint i;\n\t\t\tfor (i = 0; i < c.m_framerefs.size(); ++i)\n\t\t\t{\n\t\t\t\tc.m_framerefs[i] = c.m_nodes[i]->m_x - c.m_com;\n\t\t\t}\n\t\t}\n\t}\n}\n\n//\nvoid btSoftBody::updateClusters()\n{\n\tBT_PROFILE(\"UpdateClusters\");\n\tint i;\n\n\tfor (i = 0; i < m_clusters.size(); ++i)\n\t{\n\t\tbtSoftBody::Cluster& c = *m_clusters[i];\n\t\tconst int n = c.m_nodes.size();\n\t\t//const btScalar\t\t\tinvn=1/(btScalar)n;\n\t\tif (n)\n\t\t{\n\t\t\t/* Frame\t\t\t\t*/\n\t\t\tconst btScalar eps = btScalar(0.0001);\n\t\t\tbtMatrix3x3 m, r, s;\n\t\t\tm[0] = m[1] = m[2] = btVector3(0, 0, 0);\n\t\t\tm[0][0] = eps * 1;\n\t\t\tm[1][1] = eps * 2;\n\t\t\tm[2][2] = eps * 3;\n\t\t\tc.m_com = clusterCom(&c);\n\t\t\tfor (int i = 0; i < c.m_nodes.size(); ++i)\n\t\t\t{\n\t\t\t\tconst btVector3 a = c.m_nodes[i]->m_x - c.m_com;\n\t\t\t\tconst btVector3& b = c.m_framerefs[i];\n\t\t\t\tm[0] += a[0] * b;\n\t\t\t\tm[1] += a[1] * b;\n\t\t\t\tm[2] += a[2] * b;\n\t\t\t}\n\t\t\tPolarDecompose(m, r, s);\n\t\t\tc.m_framexform.setOrigin(c.m_com);\n\t\t\tc.m_framexform.setBasis(r);\n\t\t\t/* Inertia\t\t\t*/\n#if 1 /* Constant\t*/\n\t\t\tc.m_invwi = c.m_framexform.getBasis() * c.m_locii * c.m_framexform.getBasis().transpose();\n#else\n#if 0 /* Sphere\t*/ \n\t\t\tconst btScalar\trk=(2*c.m_extents.length2())/(5*c.m_imass);\n\t\t\tconst btVector3\tinertia(rk,rk,rk);\n\t\t\tconst btVector3\tiin(btFabs(inertia[0])>SIMD_EPSILON?1/inertia[0]:0,\n\t\t\t\tbtFabs(inertia[1])>SIMD_EPSILON?1/inertia[1]:0,\n\t\t\t\tbtFabs(inertia[2])>SIMD_EPSILON?1/inertia[2]:0);\n\n\t\t\tc.m_invwi=c.m_xform.getBasis().scaled(iin)*c.m_xform.getBasis().transpose();\n#else /* Actual\t*/\n\t\t\tc.m_invwi[0] = c.m_invwi[1] = c.m_invwi[2] = btVector3(0, 0, 0);\n\t\t\tfor (int i = 0; i < n; ++i)\n\t\t\t{\n\t\t\t\tconst btVector3 k = c.m_nodes[i]->m_x - c.m_com;\n\t\t\t\tconst btVector3 q = k * k;\n\t\t\t\tconst btScalar m = 1 / c.m_nodes[i]->m_im;\n\t\t\t\tc.m_invwi[0][0] += m * (q[1] + q[2]);\n\t\t\t\tc.m_invwi[1][1] += m * (q[0] + q[2]);\n\t\t\t\tc.m_invwi[2][2] += m * (q[0] + q[1]);\n\t\t\t\tc.m_invwi[0][1] -= m * k[0] * k[1];\n\t\t\t\tc.m_invwi[0][2] -= m * k[0] * k[2];\n\t\t\t\tc.m_invwi[1][2] -= m * k[1] * k[2];\n\t\t\t}\n\t\t\tc.m_invwi[1][0] = c.m_invwi[0][1];\n\t\t\tc.m_invwi[2][0] = c.m_invwi[0][2];\n\t\t\tc.m_invwi[2][1] = c.m_invwi[1][2];\n\t\t\tc.m_invwi = c.m_invwi.inverse();\n#endif\n#endif\n\t\t\t/* Velocities\t\t\t*/\n\t\t\tc.m_lv = btVector3(0, 0, 0);\n\t\t\tc.m_av = btVector3(0, 0, 0);\n\t\t\t{\n\t\t\t\tint i;\n\n\t\t\t\tfor (i = 0; i < n; ++i)\n\t\t\t\t{\n\t\t\t\t\tconst btVector3 v = c.m_nodes[i]->m_v * c.m_masses[i];\n\t\t\t\t\tc.m_lv += v;\n\t\t\t\t\tc.m_av += btCross(c.m_nodes[i]->m_x - c.m_com, v);\n\t\t\t\t}\n\t\t\t}\n\t\t\tc.m_lv = c.m_imass * c.m_lv * (1 - c.m_ldamping);\n\t\t\tc.m_av = c.m_invwi * c.m_av * (1 - c.m_adamping);\n\t\t\tc.m_vimpulses[0] =\n\t\t\t\tc.m_vimpulses[1] = btVector3(0, 0, 0);\n\t\t\tc.m_dimpulses[0] =\n\t\t\t\tc.m_dimpulses[1] = btVector3(0, 0, 0);\n\t\t\tc.m_nvimpulses = 0;\n\t\t\tc.m_ndimpulses = 0;\n\t\t\t/* Matching\t\t\t\t*/\n\t\t\tif (c.m_matching > 0)\n\t\t\t{\n\t\t\t\tfor (int j = 0; j < c.m_nodes.size(); ++j)\n\t\t\t\t{\n\t\t\t\t\tNode& n = *c.m_nodes[j];\n\t\t\t\t\tconst btVector3 x = c.m_framexform * c.m_framerefs[j];\n\t\t\t\t\tn.m_x = Lerp(n.m_x, x, c.m_matching);\n\t\t\t\t}\n\t\t\t}\n\t\t\t/* Dbvt\t\t\t\t\t*/\n\t\t\tif (c.m_collide)\n\t\t\t{\n\t\t\t\tbtVector3 mi = c.m_nodes[0]->m_x;\n\t\t\t\tbtVector3 mx = mi;\n\t\t\t\tfor (int j = 1; j < n; ++j)\n\t\t\t\t{\n\t\t\t\t\tmi.setMin(c.m_nodes[j]->m_x);\n\t\t\t\t\tmx.setMax(c.m_nodes[j]->m_x);\n\t\t\t\t}\n\t\t\t\tATTRIBUTE_ALIGNED16(btDbvtVolume)\n\t\t\t\tbounds = btDbvtVolume::FromMM(mi, mx);\n\t\t\t\tif (c.m_leaf)\n\t\t\t\t\tm_cdbvt.update(c.m_leaf, bounds, c.m_lv * m_sst.sdt * 3, m_sst.radmrg);\n\t\t\t\telse\n\t\t\t\t\tc.m_leaf = m_cdbvt.insert(bounds, &c);\n\t\t\t}\n\t\t}\n\t}\n}\n\n//\nvoid btSoftBody::cleanupClusters()\n{\n\tfor (int i = 0; i < m_joints.size(); ++i)\n\t{\n\t\tm_joints[i]->Terminate(m_sst.sdt);\n\t\tif (m_joints[i]->m_delete)\n\t\t{\n\t\t\tbtAlignedFree(m_joints[i]);\n\t\t\tm_joints.remove(m_joints[i--]);\n\t\t}\n\t}\n}\n\n//\nvoid btSoftBody::prepareClusters(int iterations)\n{\n\tfor (int i = 0; i < m_joints.size(); ++i)\n\t{\n\t\tm_joints[i]->Prepare(m_sst.sdt, iterations);\n\t}\n}\n\n//\nvoid btSoftBody::solveClusters(btScalar sor)\n{\n\tfor (int i = 0, ni = m_joints.size(); i < ni; ++i)\n\t{\n\t\tm_joints[i]->Solve(m_sst.sdt, sor);\n\t}\n}\n\n//\nvoid btSoftBody::applyClusters(bool drift)\n{\n\tBT_PROFILE(\"ApplyClusters\");\n\t//\tconst btScalar\t\t\t\t\tf0=m_sst.sdt;\n\t//const btScalar\t\t\t\t\tf1=f0/2;\n\tbtAlignedObjectArray<btVector3> deltas;\n\tbtAlignedObjectArray<btScalar> weights;\n\tdeltas.resize(m_nodes.size(), btVector3(0, 0, 0));\n\tweights.resize(m_nodes.size(), 0);\n\tint i;\n\n\tif (drift)\n\t{\n\t\tfor (i = 0; i < m_clusters.size(); ++i)\n\t\t{\n\t\t\tCluster& c = *m_clusters[i];\n\t\t\tif (c.m_ndimpulses)\n\t\t\t{\n\t\t\t\tc.m_dimpulses[0] /= (btScalar)c.m_ndimpulses;\n\t\t\t\tc.m_dimpulses[1] /= (btScalar)c.m_ndimpulses;\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (i = 0; i < m_clusters.size(); ++i)\n\t{\n\t\tCluster& c = *m_clusters[i];\n\t\tif (0 < (drift ? c.m_ndimpulses : c.m_nvimpulses))\n\t\t{\n\t\t\tconst btVector3 v = (drift ? c.m_dimpulses[0] : c.m_vimpulses[0]) * m_sst.sdt;\n\t\t\tconst btVector3 w = (drift ? c.m_dimpulses[1] : c.m_vimpulses[1]) * m_sst.sdt;\n\t\t\tfor (int j = 0; j < c.m_nodes.size(); ++j)\n\t\t\t{\n\t\t\t\tconst int idx = int(c.m_nodes[j] - &m_nodes[0]);\n\t\t\t\tconst btVector3& x = c.m_nodes[j]->m_x;\n\t\t\t\tconst btScalar q = c.m_masses[j];\n\t\t\t\tdeltas[idx] += (v + btCross(w, x - c.m_com)) * q;\n\t\t\t\tweights[idx] += q;\n\t\t\t}\n\t\t}\n\t}\n\tfor (i = 0; i < deltas.size(); ++i)\n\t{\n\t\tif (weights[i] > 0)\n\t\t{\n\t\t\tm_nodes[i].m_x += deltas[i] / weights[i];\n\t\t}\n\t}\n}\n\n//\nvoid btSoftBody::dampClusters()\n{\n\tint i;\n\n\tfor (i = 0; i < m_clusters.size(); ++i)\n\t{\n\t\tCluster& c = *m_clusters[i];\n\t\tif (c.m_ndamping > 0)\n\t\t{\n\t\t\tfor (int j = 0; j < c.m_nodes.size(); ++j)\n\t\t\t{\n\t\t\t\tNode& n = *c.m_nodes[j];\n\t\t\t\tif (n.m_im > 0)\n\t\t\t\t{\n\t\t\t\t\tconst btVector3 vx = c.m_lv + btCross(c.m_av, c.m_nodes[j]->m_q - c.m_com);\n\t\t\t\t\tif (vx.length2() <= n.m_v.length2())\n\t\t\t\t\t{\n\t\t\t\t\t\tn.m_v += c.m_ndamping * (vx - n.m_v);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid btSoftBody::setSpringStiffness(btScalar k)\n{\n    for (int i = 0; i < m_links.size(); ++i)\n    {\n        m_links[i].Feature::m_material->m_kLST = k;\n    }\n}\n\nvoid btSoftBody::initializeDmInverse()\n{\n    btScalar unit_simplex_measure = 1./6.;\n    \n    for (int i = 0; i < m_tetras.size(); ++i)\n    {\n        Tetra &t = m_tetras[i];\n        btVector3 c1 = t.m_n[1]->m_q - t.m_n[0]->m_q;\n        btVector3 c2 = t.m_n[2]->m_q - t.m_n[0]->m_q;\n        btVector3 c3 = t.m_n[3]->m_q - t.m_n[0]->m_q;\n        btMatrix3x3 Dm(c1.getX(), c2.getX(), c3.getX(),\n                       c1.getY(), c2.getY(), c3.getY(),\n                       c1.getZ(), c2.getZ(), c3.getZ());\n        t.m_element_measure = Dm.determinant() * unit_simplex_measure;\n        t.m_Dm_inverse = Dm.inverse();\n    }\n}\n\nvoid btSoftBody::updateDeformation()\n{\n    for (int i = 0; i < m_tetras.size(); ++i)\n    {\n        // updateDeformation is called before predictMotion where m_q is sync'd.\n        // So m_x is the current position of the node.\n        btSoftBody::Tetra& t = m_tetras[i];\n        btVector3 c1 = t.m_n[1]->m_x - t.m_n[0]->m_x;\n        btVector3 c2 = t.m_n[2]->m_x - t.m_n[0]->m_x;\n        btVector3 c3 = t.m_n[3]->m_x - t.m_n[0]->m_x;\n        btMatrix3x3 Ds(c1.getX(), c2.getX(), c3.getX(),\n                       c1.getY(), c2.getY(), c3.getY(),\n                       c1.getZ(), c2.getZ(), c3.getZ());\n        t.m_F = Ds * t.m_Dm_inverse;\n    }\n}\n//\nvoid btSoftBody::Joint::Prepare(btScalar dt, int)\n{\n\tm_bodies[0].activate();\n\tm_bodies[1].activate();\n}\n\n//\nvoid btSoftBody::LJoint::Prepare(btScalar dt, int iterations)\n{\n\tstatic const btScalar maxdrift = 4;\n\tJoint::Prepare(dt, iterations);\n\tm_rpos[0] = m_bodies[0].xform() * m_refs[0];\n\tm_rpos[1] = m_bodies[1].xform() * m_refs[1];\n\tm_drift = Clamp(m_rpos[0] - m_rpos[1], maxdrift) * m_erp / dt;\n\tm_rpos[0] -= m_bodies[0].xform().getOrigin();\n\tm_rpos[1] -= m_bodies[1].xform().getOrigin();\n\tm_massmatrix = ImpulseMatrix(m_bodies[0].invMass(), m_bodies[0].invWorldInertia(), m_rpos[0],\n\t\t\t\t\t\t\t\t m_bodies[1].invMass(), m_bodies[1].invWorldInertia(), m_rpos[1]);\n\tif (m_split > 0)\n\t{\n\t\tm_sdrift = m_massmatrix * (m_drift * m_split);\n\t\tm_drift *= 1 - m_split;\n\t}\n\tm_drift /= (btScalar)iterations;\n}\n\n//\nvoid btSoftBody::LJoint::Solve(btScalar dt, btScalar sor)\n{\n\tconst btVector3 va = m_bodies[0].velocity(m_rpos[0]);\n\tconst btVector3 vb = m_bodies[1].velocity(m_rpos[1]);\n\tconst btVector3 vr = va - vb;\n\tbtSoftBody::Impulse impulse;\n\timpulse.m_asVelocity = 1;\n\timpulse.m_velocity = m_massmatrix * (m_drift + vr * m_cfm) * sor;\n\tm_bodies[0].applyImpulse(-impulse, m_rpos[0]);\n\tm_bodies[1].applyImpulse(impulse, m_rpos[1]);\n}\n\n//\nvoid btSoftBody::LJoint::Terminate(btScalar dt)\n{\n\tif (m_split > 0)\n\t{\n\t\tm_bodies[0].applyDImpulse(-m_sdrift, m_rpos[0]);\n\t\tm_bodies[1].applyDImpulse(m_sdrift, m_rpos[1]);\n\t}\n}\n\n//\nvoid btSoftBody::AJoint::Prepare(btScalar dt, int iterations)\n{\n\tstatic const btScalar maxdrift = SIMD_PI / 16;\n\tm_icontrol->Prepare(this);\n\tJoint::Prepare(dt, iterations);\n\tm_axis[0] = m_bodies[0].xform().getBasis() * m_refs[0];\n\tm_axis[1] = m_bodies[1].xform().getBasis() * m_refs[1];\n\tm_drift = NormalizeAny(btCross(m_axis[1], m_axis[0]));\n\tm_drift *= btMin(maxdrift, btAcos(Clamp<btScalar>(btDot(m_axis[0], m_axis[1]), -1, +1)));\n\tm_drift *= m_erp / dt;\n\tm_massmatrix = AngularImpulseMatrix(m_bodies[0].invWorldInertia(), m_bodies[1].invWorldInertia());\n\tif (m_split > 0)\n\t{\n\t\tm_sdrift = m_massmatrix * (m_drift * m_split);\n\t\tm_drift *= 1 - m_split;\n\t}\n\tm_drift /= (btScalar)iterations;\n}\n\n//\nvoid btSoftBody::AJoint::Solve(btScalar dt, btScalar sor)\n{\n\tconst btVector3 va = m_bodies[0].angularVelocity();\n\tconst btVector3 vb = m_bodies[1].angularVelocity();\n\tconst btVector3 vr = va - vb;\n\tconst btScalar sp = btDot(vr, m_axis[0]);\n\tconst btVector3 vc = vr - m_axis[0] * m_icontrol->Speed(this, sp);\n\tbtSoftBody::Impulse impulse;\n\timpulse.m_asVelocity = 1;\n\timpulse.m_velocity = m_massmatrix * (m_drift + vc * m_cfm) * sor;\n\tm_bodies[0].applyAImpulse(-impulse);\n\tm_bodies[1].applyAImpulse(impulse);\n}\n\n//\nvoid btSoftBody::AJoint::Terminate(btScalar dt)\n{\n\tif (m_split > 0)\n\t{\n\t\tm_bodies[0].applyDAImpulse(-m_sdrift);\n\t\tm_bodies[1].applyDAImpulse(m_sdrift);\n\t}\n}\n\n//\nvoid btSoftBody::CJoint::Prepare(btScalar dt, int iterations)\n{\n\tJoint::Prepare(dt, iterations);\n\tconst bool dodrift = (m_life == 0);\n\tm_delete = (++m_life) > m_maxlife;\n\tif (dodrift)\n\t{\n\t\tm_drift = m_drift * m_erp / dt;\n\t\tif (m_split > 0)\n\t\t{\n\t\t\tm_sdrift = m_massmatrix * (m_drift * m_split);\n\t\t\tm_drift *= 1 - m_split;\n\t\t}\n\t\tm_drift /= (btScalar)iterations;\n\t}\n\telse\n\t{\n\t\tm_drift = m_sdrift = btVector3(0, 0, 0);\n\t}\n}\n\n//\nvoid btSoftBody::CJoint::Solve(btScalar dt, btScalar sor)\n{\n\tconst btVector3 va = m_bodies[0].velocity(m_rpos[0]);\n\tconst btVector3 vb = m_bodies[1].velocity(m_rpos[1]);\n\tconst btVector3 vrel = va - vb;\n\tconst btScalar rvac = btDot(vrel, m_normal);\n\tbtSoftBody::Impulse impulse;\n\timpulse.m_asVelocity = 1;\n\timpulse.m_velocity = m_drift;\n\tif (rvac < 0)\n\t{\n\t\tconst btVector3 iv = m_normal * rvac;\n\t\tconst btVector3 fv = vrel - iv;\n\t\timpulse.m_velocity += iv + fv * m_friction;\n\t}\n\timpulse.m_velocity = m_massmatrix * impulse.m_velocity * sor;\n\n\tif (m_bodies[0].m_soft == m_bodies[1].m_soft)\n\t{\n\t\tif ((impulse.m_velocity.getX() == impulse.m_velocity.getX()) && (impulse.m_velocity.getY() == impulse.m_velocity.getY()) &&\n\t\t\t(impulse.m_velocity.getZ() == impulse.m_velocity.getZ()))\n\t\t{\n\t\t\tif (impulse.m_asVelocity)\n\t\t\t{\n\t\t\t\tif (impulse.m_velocity.length() < m_bodies[0].m_soft->m_maxSelfCollisionImpulse)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tm_bodies[0].applyImpulse(-impulse * m_bodies[0].m_soft->m_selfCollisionImpulseFactor, m_rpos[0]);\n\t\t\t\t\tm_bodies[1].applyImpulse(impulse * m_bodies[0].m_soft->m_selfCollisionImpulseFactor, m_rpos[1]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\telse\n\t{\n\t\tm_bodies[0].applyImpulse(-impulse, m_rpos[0]);\n\t\tm_bodies[1].applyImpulse(impulse, m_rpos[1]);\n\t}\n}\n\n//\nvoid btSoftBody::CJoint::Terminate(btScalar dt)\n{\n\tif (m_split > 0)\n\t{\n\t\tm_bodies[0].applyDImpulse(-m_sdrift, m_rpos[0]);\n\t\tm_bodies[1].applyDImpulse(m_sdrift, m_rpos[1]);\n\t}\n}\n\n//\nvoid btSoftBody::applyForces()\n{\n\tBT_PROFILE(\"SoftBody applyForces\");\n\t//\tconst btScalar\t\t\t\t\tdt =\t\t\tm_sst.sdt;\n\tconst btScalar kLF = m_cfg.kLF;\n\tconst btScalar kDG = m_cfg.kDG;\n\tconst btScalar kPR = m_cfg.kPR;\n\tconst btScalar kVC = m_cfg.kVC;\n\tconst bool as_lift = kLF > 0;\n\tconst bool as_drag = kDG > 0;\n\tconst bool as_pressure = kPR != 0;\n\tconst bool as_volume = kVC > 0;\n\tconst bool as_aero = as_lift ||\n\t\t\t\t\t\t as_drag;\n\t//const bool\t\t\t\t\t\tas_vaero =\t\tas_aero\t&&\n\t//\t\t\t\t\t\t\t\t\t\t\t\t(m_cfg.aeromodel < btSoftBody::eAeroModel::F_TwoSided);\n\t//const bool\t\t\t\t\t\tas_faero =\t\tas_aero\t&&\n\t//\t\t\t\t\t\t\t\t\t\t\t\t(m_cfg.aeromodel >= btSoftBody::eAeroModel::F_TwoSided);\n\tconst bool use_medium = as_aero;\n\tconst bool use_volume = as_pressure ||\n\t\t\t\t\t\t\tas_volume;\n\tbtScalar volume = 0;\n\tbtScalar ivolumetp = 0;\n\tbtScalar dvolumetv = 0;\n\tbtSoftBody::sMedium medium;\n\tif (use_volume)\n\t{\n\t\tvolume = getVolume();\n\t\tivolumetp = 1 / btFabs(volume) * kPR;\n\t\tdvolumetv = (m_pose.m_volume - volume) * kVC;\n\t}\n\t/* Per vertex forces\t\t\t*/\n\tint i, ni;\n\n\tfor (i = 0, ni = m_nodes.size(); i < ni; ++i)\n\t{\n\t\tbtSoftBody::Node& n = m_nodes[i];\n\t\tif (n.m_im > 0)\n\t\t{\n\t\t\tif (use_medium)\n\t\t\t{\n\t\t\t\t/* Aerodynamics\t\t\t*/\n\t\t\t\taddAeroForceToNode(m_windVelocity, i);\n\t\t\t}\n\t\t\t/* Pressure\t\t\t\t*/\n\t\t\tif (as_pressure)\n\t\t\t{\n\t\t\t\tn.m_f += n.m_n * (n.m_area * ivolumetp);\n\t\t\t}\n\t\t\t/* Volume\t\t\t\t*/\n\t\t\tif (as_volume)\n\t\t\t{\n\t\t\t\tn.m_f += n.m_n * (n.m_area * dvolumetv);\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Per face forces\t\t\t\t*/\n\tfor (i = 0, ni = m_faces.size(); i < ni; ++i)\n\t{\n\t\t//\tbtSoftBody::Face&\tf=m_faces[i];\n\n\t\t/* Aerodynamics\t\t\t*/\n\t\taddAeroForceToFace(m_windVelocity, i);\n\t}\n}\n\n//\nvoid btSoftBody::PSolve_Anchors(btSoftBody* psb, btScalar kst, btScalar ti)\n{\n\tBT_PROFILE(\"PSolve_Anchors\");\n\tconst btScalar kAHR = psb->m_cfg.kAHR * kst;\n\tconst btScalar dt = psb->m_sst.sdt;\n\tfor (int i = 0, ni = psb->m_anchors.size(); i < ni; ++i)\n\t{\n\t\tconst Anchor& a = psb->m_anchors[i];\n\t\tconst btTransform& t = a.m_body->getWorldTransform();\n\t\tNode& n = *a.m_node;\n\t\tconst btVector3 wa = t * a.m_local;\n\t\tconst btVector3 va = a.m_body->getVelocityInLocalPoint(a.m_c1) * dt;\n\t\tconst btVector3 vb = n.m_x - n.m_q;\n\t\tconst btVector3 vr = (va - vb) + (wa - n.m_x) * kAHR;\n\t\tconst btVector3 impulse = a.m_c0 * vr * a.m_influence;\n\t\tn.m_x += impulse * a.m_c2;\n\t\ta.m_body->applyImpulse(-impulse, a.m_c1);\n\t}\n}\n\n//\nvoid btSoftBody::PSolve_RContacts(btSoftBody* psb, btScalar kst, btScalar ti)\n{\n\tBT_PROFILE(\"PSolve_RContacts\");\n\tconst btScalar dt = psb->m_sst.sdt;\n\tconst btScalar mrg = psb->getCollisionShape()->getMargin();\n\tbtMultiBodyJacobianData jacobianData;\n\tfor (int i = 0, ni = psb->m_rcontacts.size(); i < ni; ++i)\n\t{\n\t\tconst RContact& c = psb->m_rcontacts[i];\n\t\tconst sCti& cti = c.m_cti;\n\t\tif (cti.m_colObj->hasContactResponse())\n\t\t{\n\t\t\tbtVector3 va(0, 0, 0);\n\t\t\tbtRigidBody* rigidCol = 0;\n\t\t\tbtMultiBodyLinkCollider* multibodyLinkCol = 0;\n\t\t\tbtScalar* deltaV;\n\n\t\t\tif (cti.m_colObj->getInternalType() == btCollisionObject::CO_RIGID_BODY)\n\t\t\t{\n\t\t\t\trigidCol = (btRigidBody*)btRigidBody::upcast(cti.m_colObj);\n\t\t\t\tva = rigidCol ? rigidCol->getVelocityInLocalPoint(c.m_c1) * dt : btVector3(0, 0, 0);\n\t\t\t}\n\t\t\telse if (cti.m_colObj->getInternalType() == btCollisionObject::CO_FEATHERSTONE_LINK)\n\t\t\t{\n\t\t\t\tmultibodyLinkCol = (btMultiBodyLinkCollider*)btMultiBodyLinkCollider::upcast(cti.m_colObj);\n\t\t\t\tif (multibodyLinkCol)\n\t\t\t\t{\n\t\t\t\t\tconst int ndof = multibodyLinkCol->m_multiBody->getNumDofs() + 6;\n\t\t\t\t\tjacobianData.m_jacobians.resize(ndof);\n\t\t\t\t\tjacobianData.m_deltaVelocitiesUnitImpulse.resize(ndof);\n\t\t\t\t\tbtScalar* jac = &jacobianData.m_jacobians[0];\n\n\t\t\t\t\tmultibodyLinkCol->m_multiBody->fillContactJacobianMultiDof(multibodyLinkCol->m_link, c.m_node->m_x, cti.m_normal, jac, jacobianData.scratch_r, jacobianData.scratch_v, jacobianData.scratch_m);\n\t\t\t\t\tdeltaV = &jacobianData.m_deltaVelocitiesUnitImpulse[0];\n\t\t\t\t\tmultibodyLinkCol->m_multiBody->calcAccelerationDeltasMultiDof(&jacobianData.m_jacobians[0], deltaV, jacobianData.scratch_r, jacobianData.scratch_v);\n\n\t\t\t\t\tbtScalar vel = 0.0;\n\t\t\t\t\tfor (int j = 0; j < ndof; ++j)\n\t\t\t\t\t{\n\t\t\t\t\t\tvel += multibodyLinkCol->m_multiBody->getVelocityVector()[j] * jac[j];\n\t\t\t\t\t}\n\t\t\t\t\tva = cti.m_normal * vel * dt;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tconst btVector3 vb = c.m_node->m_x - c.m_node->m_q;\n\t\t\tconst btVector3 vr = vb - va;\n\t\t\tconst btScalar dn = btDot(vr, cti.m_normal);\n\t\t\tif (dn <= SIMD_EPSILON)\n\t\t\t{\n\t\t\t\tconst btScalar dp = btMin((btDot(c.m_node->m_x, cti.m_normal) + cti.m_offset), mrg);\n\t\t\t\tconst btVector3 fv = vr - (cti.m_normal * dn);\n\t\t\t\t// c0 is the impulse matrix, c3 is 1 - the friction coefficient or 0, c4 is the contact hardness coefficient\n\t\t\t\tconst btVector3 impulse = c.m_c0 * ((vr - (fv * c.m_c3) + (cti.m_normal * (dp * c.m_c4))) * kst);\n\t\t\t\tc.m_node->m_x -= impulse * c.m_c2;\n\n\t\t\t\tif (cti.m_colObj->getInternalType() == btCollisionObject::CO_RIGID_BODY)\n\t\t\t\t{\n\t\t\t\t\tif (rigidCol)\n\t\t\t\t\t\trigidCol->applyImpulse(impulse, c.m_c1);\n\t\t\t\t}\n\t\t\t\telse if (cti.m_colObj->getInternalType() == btCollisionObject::CO_FEATHERSTONE_LINK)\n\t\t\t\t{\n\t\t\t\t\tif (multibodyLinkCol)\n\t\t\t\t\t{\n\t\t\t\t\t\tdouble multiplier = 0.5;\n\t\t\t\t\t\tmultibodyLinkCol->m_multiBody->applyDeltaVeeMultiDof(deltaV, -impulse.length() * multiplier);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n//\nvoid btSoftBody::PSolve_SContacts(btSoftBody* psb, btScalar, btScalar ti)\n{\n\tBT_PROFILE(\"PSolve_SContacts\");\n\n\tfor (int i = 0, ni = psb->m_scontacts.size(); i < ni; ++i)\n\t{\n\t\tconst SContact& c = psb->m_scontacts[i];\n\t\tconst btVector3& nr = c.m_normal;\n\t\tNode& n = *c.m_node;\n\t\tFace& f = *c.m_face;\n\t\tconst btVector3 p = BaryEval(f.m_n[0]->m_x,\n\t\t\t\t\t\t\t\t\t f.m_n[1]->m_x,\n\t\t\t\t\t\t\t\t\t f.m_n[2]->m_x,\n\t\t\t\t\t\t\t\t\t c.m_weights);\n\t\tconst btVector3 q = BaryEval(f.m_n[0]->m_q,\n\t\t\t\t\t\t\t\t\t f.m_n[1]->m_q,\n\t\t\t\t\t\t\t\t\t f.m_n[2]->m_q,\n\t\t\t\t\t\t\t\t\t c.m_weights);\n\t\tconst btVector3 vr = (n.m_x - n.m_q) - (p - q);\n\t\tbtVector3 corr(0, 0, 0);\n\t\tbtScalar dot = btDot(vr, nr);\n\t\tif (dot < 0)\n\t\t{\n\t\t\tconst btScalar j = c.m_margin - (btDot(nr, n.m_x) - btDot(nr, p));\n\t\t\tcorr += c.m_normal * j;\n\t\t}\n\t\tcorr -= ProjectOnPlane(vr, nr) * c.m_friction;\n\t\tn.m_x += corr * c.m_cfm[0];\n\t\tf.m_n[0]->m_x -= corr * (c.m_cfm[1] * c.m_weights.x());\n\t\tf.m_n[1]->m_x -= corr * (c.m_cfm[1] * c.m_weights.y());\n\t\tf.m_n[2]->m_x -= corr * (c.m_cfm[1] * c.m_weights.z());\n\t}\n}\n\n//\nvoid btSoftBody::PSolve_Links(btSoftBody* psb, btScalar kst, btScalar ti)\n{\n\tBT_PROFILE(\"PSolve_Links\");\n\tfor (int i = 0, ni = psb->m_links.size(); i < ni; ++i)\n\t{\n\t\tLink& l = psb->m_links[i];\n\t\tif (l.m_c0 > 0)\n\t\t{\n\t\t\tNode& a = *l.m_n[0];\n\t\t\tNode& b = *l.m_n[1];\n\t\t\tconst btVector3 del = b.m_x - a.m_x;\n\t\t\tconst btScalar len = del.length2();\n\t\t\tif (l.m_c1 + len > SIMD_EPSILON)\n\t\t\t{\n\t\t\t\tconst btScalar k = ((l.m_c1 - len) / (l.m_c0 * (l.m_c1 + len))) * kst;\n\t\t\t\ta.m_x -= del * (k * a.m_im);\n\t\t\t\tb.m_x += del * (k * b.m_im);\n\t\t\t}\n\t\t}\n\t}\n}\n\n//\nvoid btSoftBody::VSolve_Links(btSoftBody* psb, btScalar kst)\n{\n\tBT_PROFILE(\"VSolve_Links\");\n\tfor (int i = 0, ni = psb->m_links.size(); i < ni; ++i)\n\t{\n\t\tLink& l = psb->m_links[i];\n\t\tNode** n = l.m_n;\n\t\tconst btScalar j = -btDot(l.m_c3, n[0]->m_v - n[1]->m_v) * l.m_c2 * kst;\n\t\tn[0]->m_v += l.m_c3 * (j * n[0]->m_im);\n\t\tn[1]->m_v -= l.m_c3 * (j * n[1]->m_im);\n\t}\n}\n\n//\nbtSoftBody::psolver_t btSoftBody::getSolver(ePSolver::_ solver)\n{\n\tswitch (solver)\n\t{\n\t\tcase ePSolver::Anchors:\n\t\t\treturn (&btSoftBody::PSolve_Anchors);\n\t\tcase ePSolver::Linear:\n\t\t\treturn (&btSoftBody::PSolve_Links);\n\t\tcase ePSolver::RContacts:\n\t\t\treturn (&btSoftBody::PSolve_RContacts);\n\t\tcase ePSolver::SContacts:\n\t\t\treturn (&btSoftBody::PSolve_SContacts);\n\t\tdefault:\n\t\t{\n\t\t}\n\t}\n\treturn (0);\n}\n\n//\nbtSoftBody::vsolver_t btSoftBody::getSolver(eVSolver::_ solver)\n{\n\tswitch (solver)\n\t{\n\t\tcase eVSolver::Linear:\n\t\t\treturn (&btSoftBody::VSolve_Links);\n\t\tdefault:\n\t\t{\n\t\t}\n\t}\n\treturn (0);\n}\n\n//\nvoid btSoftBody::defaultCollisionHandler(const btCollisionObjectWrapper* pcoWrap)\n{\n\tswitch (m_cfg.collisions & fCollision::RVSmask)\n\t{\n\t\tcase fCollision::SDF_RS:\n\t\t{\n\t\t\tbtSoftColliders::CollideSDF_RS docollide;\n\t\t\tbtRigidBody* prb1 = (btRigidBody*)btRigidBody::upcast(pcoWrap->getCollisionObject());\n\t\t\tbtTransform wtr = pcoWrap->getWorldTransform();\n\n\t\t\tconst btTransform ctr = pcoWrap->getWorldTransform();\n\t\t\tconst btScalar timemargin = (wtr.getOrigin() - ctr.getOrigin()).length();\n\t\t\tconst btScalar basemargin = getCollisionShape()->getMargin();\n\t\t\tbtVector3 mins;\n\t\t\tbtVector3 maxs;\n\t\t\tATTRIBUTE_ALIGNED16(btDbvtVolume)\n\t\t\tvolume;\n\t\t\tpcoWrap->getCollisionShape()->getAabb(pcoWrap->getWorldTransform(),\n\t\t\t\t\t\t\t\t\t\t\t\t  mins,\n\t\t\t\t\t\t\t\t\t\t\t\t  maxs);\n\t\t\tvolume = btDbvtVolume::FromMM(mins, maxs);\n\t\t\tvolume.Expand(btVector3(basemargin, basemargin, basemargin));\n\t\t\tdocollide.psb = this;\n\t\t\tdocollide.m_colObj1Wrap = pcoWrap;\n\t\t\tdocollide.m_rigidBody = prb1;\n\n\t\t\tdocollide.dynmargin = basemargin + timemargin;\n\t\t\tdocollide.stamargin = basemargin;\n\t\t\tm_ndbvt.collideTV(m_ndbvt.m_root, volume, docollide);\n\t\t}\n\t\tbreak;\n\t\tcase fCollision::CL_RS:\n\t\t{\n\t\t\tbtSoftColliders::CollideCL_RS collider;\n\t\t\tcollider.ProcessColObj(this, pcoWrap);\n\t\t}\n\t\tbreak;\n        case fCollision::SDF_RD:\n        {\n            btSoftColliders::CollideSDF_RD docollide;\n            btRigidBody* prb1 = (btRigidBody*)btRigidBody::upcast(pcoWrap->getCollisionObject());\n            btTransform wtr = pcoWrap->getWorldTransform();\n            \n            const btTransform ctr = pcoWrap->getWorldTransform();\n            const btScalar timemargin = (wtr.getOrigin() - ctr.getOrigin()).length();\n            const btScalar basemargin = getCollisionShape()->getMargin();\n            btVector3 mins;\n            btVector3 maxs;\n            ATTRIBUTE_ALIGNED16(btDbvtVolume)\n            volume;\n            pcoWrap->getCollisionShape()->getAabb(pcoWrap->getWorldTransform(),\n                                                  mins,\n                                                  maxs);\n            volume = btDbvtVolume::FromMM(mins, maxs);\n            volume.Expand(btVector3(basemargin, basemargin, basemargin));\n            docollide.psb = this;\n            docollide.m_colObj1Wrap = pcoWrap;\n            docollide.m_rigidBody = prb1;\n            \n            docollide.dynmargin = basemargin + timemargin;\n            docollide.stamargin = basemargin;\n            m_ndbvt.collideTV(m_ndbvt.m_root, volume, docollide);\n        }\n        break;\n\t}\n}\n\n//\nvoid btSoftBody::defaultCollisionHandler(btSoftBody* psb)\n{\n\tconst int cf = m_cfg.collisions & psb->m_cfg.collisions;\n\tswitch (cf & fCollision::SVSmask)\n\t{\n\t\tcase fCollision::CL_SS:\n\t\t{\n\t\t\t//support self-collision if CL_SELF flag set\n\t\t\tif (this != psb || psb->m_cfg.collisions & fCollision::CL_SELF)\n\t\t\t{\n\t\t\t\tbtSoftColliders::CollideCL_SS docollide;\n\t\t\t\tdocollide.ProcessSoftSoft(this, psb);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t\tcase fCollision::VF_SS:\n\t\t{\n\t\t\t//only self-collision for Cluster, not Vertex-Face yet\n\t\t\tif (this != psb)\n\t\t\t{\n\t\t\t\tbtSoftColliders::CollideVF_SS docollide;\n\t\t\t\t/* common\t\t\t\t\t*/\n\t\t\t\tdocollide.mrg = getCollisionShape()->getMargin() +\n\t\t\t\t\t\t\t\tpsb->getCollisionShape()->getMargin();\n\t\t\t\t/* psb0 nodes vs psb1 faces\t*/\n\t\t\t\tdocollide.psb[0] = this;\n\t\t\t\tdocollide.psb[1] = psb;\n\t\t\t\tdocollide.psb[0]->m_ndbvt.collideTT(docollide.psb[0]->m_ndbvt.m_root,\n\t\t\t\t\t\t\t\t\t\t\t\t\tdocollide.psb[1]->m_fdbvt.m_root,\n\t\t\t\t\t\t\t\t\t\t\t\t\tdocollide);\n\t\t\t\t/* psb1 nodes vs psb0 faces\t*/\n\t\t\t\tdocollide.psb[0] = psb;\n\t\t\t\tdocollide.psb[1] = this;\n\t\t\t\tdocollide.psb[0]->m_ndbvt.collideTT(docollide.psb[0]->m_ndbvt.m_root,\n\t\t\t\t\t\t\t\t\t\t\t\t\tdocollide.psb[1]->m_fdbvt.m_root,\n\t\t\t\t\t\t\t\t\t\t\t\t\tdocollide);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t\tdefault:\n\t\t{\n\t\t}\n\t}\n}\n\nvoid btSoftBody::setWindVelocity(const btVector3& velocity)\n{\n\tm_windVelocity = velocity;\n}\n\nconst btVector3& btSoftBody::getWindVelocity()\n{\n\treturn m_windVelocity;\n}\n\nint btSoftBody::calculateSerializeBufferSize() const\n{\n\tint sz = sizeof(btSoftBodyData);\n\treturn sz;\n}\n\n///fills the dataBuffer and returns the struct name (and 0 on failure)\nconst char* btSoftBody::serialize(void* dataBuffer, class btSerializer* serializer) const\n{\n\tbtSoftBodyData* sbd = (btSoftBodyData*)dataBuffer;\n\n\tbtCollisionObject::serialize(&sbd->m_collisionObjectData, serializer);\n\n\tbtHashMap<btHashPtr, int> m_nodeIndexMap;\n\n\tsbd->m_numMaterials = m_materials.size();\n\tsbd->m_materials = sbd->m_numMaterials ? (SoftBodyMaterialData**)serializer->getUniquePointer((void*)&m_materials) : 0;\n\n\tif (sbd->m_materials)\n\t{\n\t\tint sz = sizeof(SoftBodyMaterialData*);\n\t\tint numElem = sbd->m_numMaterials;\n\t\tbtChunk* chunk = serializer->allocate(sz, numElem);\n\t\t//SoftBodyMaterialData** memPtr = chunk->m_oldPtr;\n\t\tSoftBodyMaterialData** memPtr = (SoftBodyMaterialData**)chunk->m_oldPtr;\n\t\tfor (int i = 0; i < numElem; i++, memPtr++)\n\t\t{\n\t\t\tbtSoftBody::Material* mat = m_materials[i];\n\t\t\t*memPtr = mat ? (SoftBodyMaterialData*)serializer->getUniquePointer((void*)mat) : 0;\n\t\t\tif (!serializer->findPointer(mat))\n\t\t\t{\n\t\t\t\t//serialize it here\n\t\t\t\tbtChunk* chunk = serializer->allocate(sizeof(SoftBodyMaterialData), 1);\n\t\t\t\tSoftBodyMaterialData* memPtr = (SoftBodyMaterialData*)chunk->m_oldPtr;\n\t\t\t\tmemPtr->m_flags = mat->m_flags;\n\t\t\t\tmemPtr->m_angularStiffness = mat->m_kAST;\n\t\t\t\tmemPtr->m_linearStiffness = mat->m_kLST;\n\t\t\t\tmemPtr->m_volumeStiffness = mat->m_kVST;\n\t\t\t\tserializer->finalizeChunk(chunk, \"SoftBodyMaterialData\", BT_SBMATERIAL_CODE, mat);\n\t\t\t}\n\t\t}\n\t\tserializer->finalizeChunk(chunk, \"SoftBodyMaterialData\", BT_ARRAY_CODE, (void*)&m_materials);\n\t}\n\n\tsbd->m_numNodes = m_nodes.size();\n\tsbd->m_nodes = sbd->m_numNodes ? (SoftBodyNodeData*)serializer->getUniquePointer((void*)&m_nodes) : 0;\n\tif (sbd->m_nodes)\n\t{\n\t\tint sz = sizeof(SoftBodyNodeData);\n\t\tint numElem = sbd->m_numNodes;\n\t\tbtChunk* chunk = serializer->allocate(sz, numElem);\n\t\tSoftBodyNodeData* memPtr = (SoftBodyNodeData*)chunk->m_oldPtr;\n\t\tfor (int i = 0; i < numElem; i++, memPtr++)\n\t\t{\n\t\t\tm_nodes[i].m_f.serializeFloat(memPtr->m_accumulatedForce);\n\t\t\tmemPtr->m_area = m_nodes[i].m_area;\n\t\t\tmemPtr->m_attach = m_nodes[i].m_battach;\n\t\t\tmemPtr->m_inverseMass = m_nodes[i].m_im;\n\t\t\tmemPtr->m_material = m_nodes[i].m_material ? (SoftBodyMaterialData*)serializer->getUniquePointer((void*)m_nodes[i].m_material) : 0;\n\t\t\tm_nodes[i].m_n.serializeFloat(memPtr->m_normal);\n\t\t\tm_nodes[i].m_x.serializeFloat(memPtr->m_position);\n\t\t\tm_nodes[i].m_q.serializeFloat(memPtr->m_previousPosition);\n\t\t\tm_nodes[i].m_v.serializeFloat(memPtr->m_velocity);\n\t\t\tm_nodeIndexMap.insert(&m_nodes[i], i);\n\t\t}\n\t\tserializer->finalizeChunk(chunk, \"SoftBodyNodeData\", BT_SBNODE_CODE, (void*)&m_nodes);\n\t}\n\n\tsbd->m_numLinks = m_links.size();\n\tsbd->m_links = sbd->m_numLinks ? (SoftBodyLinkData*)serializer->getUniquePointer((void*)&m_links[0]) : 0;\n\tif (sbd->m_links)\n\t{\n\t\tint sz = sizeof(SoftBodyLinkData);\n\t\tint numElem = sbd->m_numLinks;\n\t\tbtChunk* chunk = serializer->allocate(sz, numElem);\n\t\tSoftBodyLinkData* memPtr = (SoftBodyLinkData*)chunk->m_oldPtr;\n\t\tfor (int i = 0; i < numElem; i++, memPtr++)\n\t\t{\n\t\t\tmemPtr->m_bbending = m_links[i].m_bbending;\n\t\t\tmemPtr->m_material = m_links[i].m_material ? (SoftBodyMaterialData*)serializer->getUniquePointer((void*)m_links[i].m_material) : 0;\n\t\t\tmemPtr->m_nodeIndices[0] = m_links[i].m_n[0] ? m_links[i].m_n[0] - &m_nodes[0] : -1;\n\t\t\tmemPtr->m_nodeIndices[1] = m_links[i].m_n[1] ? m_links[i].m_n[1] - &m_nodes[0] : -1;\n\t\t\tbtAssert(memPtr->m_nodeIndices[0] < m_nodes.size());\n\t\t\tbtAssert(memPtr->m_nodeIndices[1] < m_nodes.size());\n\t\t\tmemPtr->m_restLength = m_links[i].m_rl;\n\t\t}\n\t\tserializer->finalizeChunk(chunk, \"SoftBodyLinkData\", BT_ARRAY_CODE, (void*)&m_links[0]);\n\t}\n\n\tsbd->m_numFaces = m_faces.size();\n\tsbd->m_faces = sbd->m_numFaces ? (SoftBodyFaceData*)serializer->getUniquePointer((void*)&m_faces[0]) : 0;\n\tif (sbd->m_faces)\n\t{\n\t\tint sz = sizeof(SoftBodyFaceData);\n\t\tint numElem = sbd->m_numFaces;\n\t\tbtChunk* chunk = serializer->allocate(sz, numElem);\n\t\tSoftBodyFaceData* memPtr = (SoftBodyFaceData*)chunk->m_oldPtr;\n\t\tfor (int i = 0; i < numElem; i++, memPtr++)\n\t\t{\n\t\t\tmemPtr->m_material = m_faces[i].m_material ? (SoftBodyMaterialData*)serializer->getUniquePointer((void*)m_faces[i].m_material) : 0;\n\t\t\tm_faces[i].m_normal.serializeFloat(memPtr->m_normal);\n\t\t\tfor (int j = 0; j < 3; j++)\n\t\t\t{\n\t\t\t\tmemPtr->m_nodeIndices[j] = m_faces[i].m_n[j] ? m_faces[i].m_n[j] - &m_nodes[0] : -1;\n\t\t\t}\n\t\t\tmemPtr->m_restArea = m_faces[i].m_ra;\n\t\t}\n\t\tserializer->finalizeChunk(chunk, \"SoftBodyFaceData\", BT_ARRAY_CODE, (void*)&m_faces[0]);\n\t}\n\n\tsbd->m_numTetrahedra = m_tetras.size();\n\tsbd->m_tetrahedra = sbd->m_numTetrahedra ? (SoftBodyTetraData*)serializer->getUniquePointer((void*)&m_tetras[0]) : 0;\n\tif (sbd->m_tetrahedra)\n\t{\n\t\tint sz = sizeof(SoftBodyTetraData);\n\t\tint numElem = sbd->m_numTetrahedra;\n\t\tbtChunk* chunk = serializer->allocate(sz, numElem);\n\t\tSoftBodyTetraData* memPtr = (SoftBodyTetraData*)chunk->m_oldPtr;\n\t\tfor (int i = 0; i < numElem; i++, memPtr++)\n\t\t{\n\t\t\tfor (int j = 0; j < 4; j++)\n\t\t\t{\n\t\t\t\tm_tetras[i].m_c0[j].serializeFloat(memPtr->m_c0[j]);\n\t\t\t\tmemPtr->m_nodeIndices[j] = m_tetras[j].m_n[j] ? m_tetras[j].m_n[j] - &m_nodes[0] : -1;\n\t\t\t}\n\t\t\tmemPtr->m_c1 = m_tetras[i].m_c1;\n\t\t\tmemPtr->m_c2 = m_tetras[i].m_c2;\n\t\t\tmemPtr->m_material = m_tetras[i].m_material ? (SoftBodyMaterialData*)serializer->getUniquePointer((void*)m_tetras[i].m_material) : 0;\n\t\t\tmemPtr->m_restVolume = m_tetras[i].m_rv;\n\t\t}\n\t\tserializer->finalizeChunk(chunk, \"SoftBodyTetraData\", BT_ARRAY_CODE, (void*)&m_tetras[0]);\n\t}\n\n\tsbd->m_numAnchors = m_anchors.size();\n\tsbd->m_anchors = sbd->m_numAnchors ? (SoftRigidAnchorData*)serializer->getUniquePointer((void*)&m_anchors[0]) : 0;\n\tif (sbd->m_anchors)\n\t{\n\t\tint sz = sizeof(SoftRigidAnchorData);\n\t\tint numElem = sbd->m_numAnchors;\n\t\tbtChunk* chunk = serializer->allocate(sz, numElem);\n\t\tSoftRigidAnchorData* memPtr = (SoftRigidAnchorData*)chunk->m_oldPtr;\n\t\tfor (int i = 0; i < numElem; i++, memPtr++)\n\t\t{\n\t\t\tm_anchors[i].m_c0.serializeFloat(memPtr->m_c0);\n\t\t\tm_anchors[i].m_c1.serializeFloat(memPtr->m_c1);\n\t\t\tmemPtr->m_c2 = m_anchors[i].m_c2;\n\t\t\tm_anchors[i].m_local.serializeFloat(memPtr->m_localFrame);\n\t\t\tmemPtr->m_nodeIndex = m_anchors[i].m_node ? m_anchors[i].m_node - &m_nodes[0] : -1;\n\n\t\t\tmemPtr->m_rigidBody = m_anchors[i].m_body ? (btRigidBodyData*)serializer->getUniquePointer((void*)m_anchors[i].m_body) : 0;\n\t\t\tbtAssert(memPtr->m_nodeIndex < m_nodes.size());\n\t\t}\n\t\tserializer->finalizeChunk(chunk, \"SoftRigidAnchorData\", BT_ARRAY_CODE, (void*)&m_anchors[0]);\n\t}\n\n\tsbd->m_config.m_dynamicFriction = m_cfg.kDF;\n\tsbd->m_config.m_baumgarte = m_cfg.kVCF;\n\tsbd->m_config.m_pressure = m_cfg.kPR;\n\tsbd->m_config.m_aeroModel = this->m_cfg.aeromodel;\n\tsbd->m_config.m_lift = m_cfg.kLF;\n\tsbd->m_config.m_drag = m_cfg.kDG;\n\tsbd->m_config.m_positionIterations = m_cfg.piterations;\n\tsbd->m_config.m_driftIterations = m_cfg.diterations;\n\tsbd->m_config.m_clusterIterations = m_cfg.citerations;\n\tsbd->m_config.m_velocityIterations = m_cfg.viterations;\n\tsbd->m_config.m_maxVolume = m_cfg.maxvolume;\n\tsbd->m_config.m_damping = m_cfg.kDP;\n\tsbd->m_config.m_poseMatch = m_cfg.kMT;\n\tsbd->m_config.m_collisionFlags = m_cfg.collisions;\n\tsbd->m_config.m_volume = m_cfg.kVC;\n\tsbd->m_config.m_rigidContactHardness = m_cfg.kCHR;\n\tsbd->m_config.m_kineticContactHardness = m_cfg.kKHR;\n\tsbd->m_config.m_softContactHardness = m_cfg.kSHR;\n\tsbd->m_config.m_anchorHardness = m_cfg.kAHR;\n\tsbd->m_config.m_timeScale = m_cfg.timescale;\n\tsbd->m_config.m_maxVolume = m_cfg.maxvolume;\n\tsbd->m_config.m_softRigidClusterHardness = m_cfg.kSRHR_CL;\n\tsbd->m_config.m_softKineticClusterHardness = m_cfg.kSKHR_CL;\n\tsbd->m_config.m_softSoftClusterHardness = m_cfg.kSSHR_CL;\n\tsbd->m_config.m_softRigidClusterImpulseSplit = m_cfg.kSR_SPLT_CL;\n\tsbd->m_config.m_softKineticClusterImpulseSplit = m_cfg.kSK_SPLT_CL;\n\tsbd->m_config.m_softSoftClusterImpulseSplit = m_cfg.kSS_SPLT_CL;\n\n\t//pose for shape matching\n\t{\n\t\tsbd->m_pose = (SoftBodyPoseData*)serializer->getUniquePointer((void*)&m_pose);\n\n\t\tint sz = sizeof(SoftBodyPoseData);\n\t\tbtChunk* chunk = serializer->allocate(sz, 1);\n\t\tSoftBodyPoseData* memPtr = (SoftBodyPoseData*)chunk->m_oldPtr;\n\n\t\tm_pose.m_aqq.serializeFloat(memPtr->m_aqq);\n\t\tmemPtr->m_bframe = m_pose.m_bframe;\n\t\tmemPtr->m_bvolume = m_pose.m_bvolume;\n\t\tm_pose.m_com.serializeFloat(memPtr->m_com);\n\n\t\tmemPtr->m_numPositions = m_pose.m_pos.size();\n\t\tmemPtr->m_positions = memPtr->m_numPositions ? (btVector3FloatData*)serializer->getUniquePointer((void*)&m_pose.m_pos[0]) : 0;\n\t\tif (memPtr->m_numPositions)\n\t\t{\n\t\t\tint numElem = memPtr->m_numPositions;\n\t\t\tint sz = sizeof(btVector3Data);\n\t\t\tbtChunk* chunk = serializer->allocate(sz, numElem);\n\t\t\tbtVector3FloatData* memPtr = (btVector3FloatData*)chunk->m_oldPtr;\n\t\t\tfor (int i = 0; i < numElem; i++, memPtr++)\n\t\t\t{\n\t\t\t\tm_pose.m_pos[i].serializeFloat(*memPtr);\n\t\t\t}\n\t\t\tserializer->finalizeChunk(chunk, \"btVector3FloatData\", BT_ARRAY_CODE, (void*)&m_pose.m_pos[0]);\n\t\t}\n\t\tmemPtr->m_restVolume = m_pose.m_volume;\n\t\tm_pose.m_rot.serializeFloat(memPtr->m_rot);\n\t\tm_pose.m_scl.serializeFloat(memPtr->m_scale);\n\n\t\tmemPtr->m_numWeigts = m_pose.m_wgh.size();\n\t\tmemPtr->m_weights = memPtr->m_numWeigts ? (float*)serializer->getUniquePointer((void*)&m_pose.m_wgh[0]) : 0;\n\t\tif (memPtr->m_numWeigts)\n\t\t{\n\t\t\tint numElem = memPtr->m_numWeigts;\n\t\t\tint sz = sizeof(float);\n\t\t\tbtChunk* chunk = serializer->allocate(sz, numElem);\n\t\t\tfloat* memPtr = (float*)chunk->m_oldPtr;\n\t\t\tfor (int i = 0; i < numElem; i++, memPtr++)\n\t\t\t{\n\t\t\t\t*memPtr = m_pose.m_wgh[i];\n\t\t\t}\n\t\t\tserializer->finalizeChunk(chunk, \"float\", BT_ARRAY_CODE, (void*)&m_pose.m_wgh[0]);\n\t\t}\n\n\t\tserializer->finalizeChunk(chunk, \"SoftBodyPoseData\", BT_ARRAY_CODE, (void*)&m_pose);\n\t}\n\n\t//clusters for convex-cluster collision detection\n\n\tsbd->m_numClusters = m_clusters.size();\n\tsbd->m_clusters = sbd->m_numClusters ? (SoftBodyClusterData*)serializer->getUniquePointer((void*)m_clusters[0]) : 0;\n\tif (sbd->m_numClusters)\n\t{\n\t\tint numElem = sbd->m_numClusters;\n\t\tint sz = sizeof(SoftBodyClusterData);\n\t\tbtChunk* chunk = serializer->allocate(sz, numElem);\n\t\tSoftBodyClusterData* memPtr = (SoftBodyClusterData*)chunk->m_oldPtr;\n\t\tfor (int i = 0; i < numElem; i++, memPtr++)\n\t\t{\n\t\t\tmemPtr->m_adamping = m_clusters[i]->m_adamping;\n\t\t\tm_clusters[i]->m_av.serializeFloat(memPtr->m_av);\n\t\t\tmemPtr->m_clusterIndex = m_clusters[i]->m_clusterIndex;\n\t\t\tmemPtr->m_collide = m_clusters[i]->m_collide;\n\t\t\tm_clusters[i]->m_com.serializeFloat(memPtr->m_com);\n\t\t\tmemPtr->m_containsAnchor = m_clusters[i]->m_containsAnchor;\n\t\t\tm_clusters[i]->m_dimpulses[0].serializeFloat(memPtr->m_dimpulses[0]);\n\t\t\tm_clusters[i]->m_dimpulses[1].serializeFloat(memPtr->m_dimpulses[1]);\n\t\t\tm_clusters[i]->m_framexform.serializeFloat(memPtr->m_framexform);\n\t\t\tmemPtr->m_idmass = m_clusters[i]->m_idmass;\n\t\t\tmemPtr->m_imass = m_clusters[i]->m_imass;\n\t\t\tm_clusters[i]->m_invwi.serializeFloat(memPtr->m_invwi);\n\t\t\tmemPtr->m_ldamping = m_clusters[i]->m_ldamping;\n\t\t\tm_clusters[i]->m_locii.serializeFloat(memPtr->m_locii);\n\t\t\tm_clusters[i]->m_lv.serializeFloat(memPtr->m_lv);\n\t\t\tmemPtr->m_matching = m_clusters[i]->m_matching;\n\t\t\tmemPtr->m_maxSelfCollisionImpulse = m_clusters[i]->m_maxSelfCollisionImpulse;\n\t\t\tmemPtr->m_ndamping = m_clusters[i]->m_ndamping;\n\t\t\tmemPtr->m_ldamping = m_clusters[i]->m_ldamping;\n\t\t\tmemPtr->m_adamping = m_clusters[i]->m_adamping;\n\t\t\tmemPtr->m_selfCollisionImpulseFactor = m_clusters[i]->m_selfCollisionImpulseFactor;\n\n\t\t\tmemPtr->m_numFrameRefs = m_clusters[i]->m_framerefs.size();\n\t\t\tmemPtr->m_numMasses = m_clusters[i]->m_masses.size();\n\t\t\tmemPtr->m_numNodes = m_clusters[i]->m_nodes.size();\n\n\t\t\tmemPtr->m_nvimpulses = m_clusters[i]->m_nvimpulses;\n\t\t\tm_clusters[i]->m_vimpulses[0].serializeFloat(memPtr->m_vimpulses[0]);\n\t\t\tm_clusters[i]->m_vimpulses[1].serializeFloat(memPtr->m_vimpulses[1]);\n\t\t\tmemPtr->m_ndimpulses = m_clusters[i]->m_ndimpulses;\n\n\t\t\tmemPtr->m_framerefs = memPtr->m_numFrameRefs ? (btVector3FloatData*)serializer->getUniquePointer((void*)&m_clusters[i]->m_framerefs[0]) : 0;\n\t\t\tif (memPtr->m_framerefs)\n\t\t\t{\n\t\t\t\tint numElem = memPtr->m_numFrameRefs;\n\t\t\t\tint sz = sizeof(btVector3FloatData);\n\t\t\t\tbtChunk* chunk = serializer->allocate(sz, numElem);\n\t\t\t\tbtVector3FloatData* memPtr = (btVector3FloatData*)chunk->m_oldPtr;\n\t\t\t\tfor (int j = 0; j < numElem; j++, memPtr++)\n\t\t\t\t{\n\t\t\t\t\tm_clusters[i]->m_framerefs[j].serializeFloat(*memPtr);\n\t\t\t\t}\n\t\t\t\tserializer->finalizeChunk(chunk, \"btVector3FloatData\", BT_ARRAY_CODE, (void*)&m_clusters[i]->m_framerefs[0]);\n\t\t\t}\n\n\t\t\tmemPtr->m_masses = memPtr->m_numMasses ? (float*)serializer->getUniquePointer((void*)&m_clusters[i]->m_masses[0]) : 0;\n\t\t\tif (memPtr->m_masses)\n\t\t\t{\n\t\t\t\tint numElem = memPtr->m_numMasses;\n\t\t\t\tint sz = sizeof(float);\n\t\t\t\tbtChunk* chunk = serializer->allocate(sz, numElem);\n\t\t\t\tfloat* memPtr = (float*)chunk->m_oldPtr;\n\t\t\t\tfor (int j = 0; j < numElem; j++, memPtr++)\n\t\t\t\t{\n\t\t\t\t\t*memPtr = m_clusters[i]->m_masses[j];\n\t\t\t\t}\n\t\t\t\tserializer->finalizeChunk(chunk, \"float\", BT_ARRAY_CODE, (void*)&m_clusters[i]->m_masses[0]);\n\t\t\t}\n\n\t\t\tmemPtr->m_nodeIndices = memPtr->m_numNodes ? (int*)serializer->getUniquePointer((void*)&m_clusters[i]->m_nodes) : 0;\n\t\t\tif (memPtr->m_nodeIndices)\n\t\t\t{\n\t\t\t\tint numElem = memPtr->m_numMasses;\n\t\t\t\tint sz = sizeof(int);\n\t\t\t\tbtChunk* chunk = serializer->allocate(sz, numElem);\n\t\t\t\tint* memPtr = (int*)chunk->m_oldPtr;\n\t\t\t\tfor (int j = 0; j < numElem; j++, memPtr++)\n\t\t\t\t{\n\t\t\t\t\tint* indexPtr = m_nodeIndexMap.find(m_clusters[i]->m_nodes[j]);\n\t\t\t\t\tbtAssert(indexPtr);\n\t\t\t\t\t*memPtr = *indexPtr;\n\t\t\t\t}\n\t\t\t\tserializer->finalizeChunk(chunk, \"int\", BT_ARRAY_CODE, (void*)&m_clusters[i]->m_nodes);\n\t\t\t}\n\t\t}\n\t\tserializer->finalizeChunk(chunk, \"SoftBodyClusterData\", BT_ARRAY_CODE, (void*)m_clusters[0]);\n\t}\n\n\tsbd->m_numJoints = m_joints.size();\n\tsbd->m_joints = m_joints.size() ? (btSoftBodyJointData*)serializer->getUniquePointer((void*)&m_joints[0]) : 0;\n\n\tif (sbd->m_joints)\n\t{\n\t\tint sz = sizeof(btSoftBodyJointData);\n\t\tint numElem = m_joints.size();\n\t\tbtChunk* chunk = serializer->allocate(sz, numElem);\n\t\tbtSoftBodyJointData* memPtr = (btSoftBodyJointData*)chunk->m_oldPtr;\n\n\t\tfor (int i = 0; i < numElem; i++, memPtr++)\n\t\t{\n\t\t\tmemPtr->m_jointType = (int)m_joints[i]->Type();\n\t\t\tm_joints[i]->m_refs[0].serializeFloat(memPtr->m_refs[0]);\n\t\t\tm_joints[i]->m_refs[1].serializeFloat(memPtr->m_refs[1]);\n\t\t\tmemPtr->m_cfm = m_joints[i]->m_cfm;\n\t\t\tmemPtr->m_erp = float(m_joints[i]->m_erp);\n\t\t\tmemPtr->m_split = float(m_joints[i]->m_split);\n\t\t\tmemPtr->m_delete = m_joints[i]->m_delete;\n\n\t\t\tfor (int j = 0; j < 4; j++)\n\t\t\t{\n\t\t\t\tmemPtr->m_relPosition[0].m_floats[j] = 0.f;\n\t\t\t\tmemPtr->m_relPosition[1].m_floats[j] = 0.f;\n\t\t\t}\n\t\t\tmemPtr->m_bodyA = 0;\n\t\t\tmemPtr->m_bodyB = 0;\n\t\t\tif (m_joints[i]->m_bodies[0].m_soft)\n\t\t\t{\n\t\t\t\tmemPtr->m_bodyAtype = BT_JOINT_SOFT_BODY_CLUSTER;\n\t\t\t\tmemPtr->m_bodyA = serializer->getUniquePointer((void*)m_joints[i]->m_bodies[0].m_soft);\n\t\t\t}\n\t\t\tif (m_joints[i]->m_bodies[0].m_collisionObject)\n\t\t\t{\n\t\t\t\tmemPtr->m_bodyAtype = BT_JOINT_COLLISION_OBJECT;\n\t\t\t\tmemPtr->m_bodyA = serializer->getUniquePointer((void*)m_joints[i]->m_bodies[0].m_collisionObject);\n\t\t\t}\n\t\t\tif (m_joints[i]->m_bodies[0].m_rigid)\n\t\t\t{\n\t\t\t\tmemPtr->m_bodyAtype = BT_JOINT_RIGID_BODY;\n\t\t\t\tmemPtr->m_bodyA = serializer->getUniquePointer((void*)m_joints[i]->m_bodies[0].m_rigid);\n\t\t\t}\n\n\t\t\tif (m_joints[i]->m_bodies[1].m_soft)\n\t\t\t{\n\t\t\t\tmemPtr->m_bodyBtype = BT_JOINT_SOFT_BODY_CLUSTER;\n\t\t\t\tmemPtr->m_bodyB = serializer->getUniquePointer((void*)m_joints[i]->m_bodies[1].m_soft);\n\t\t\t}\n\t\t\tif (m_joints[i]->m_bodies[1].m_collisionObject)\n\t\t\t{\n\t\t\t\tmemPtr->m_bodyBtype = BT_JOINT_COLLISION_OBJECT;\n\t\t\t\tmemPtr->m_bodyB = serializer->getUniquePointer((void*)m_joints[i]->m_bodies[1].m_collisionObject);\n\t\t\t}\n\t\t\tif (m_joints[i]->m_bodies[1].m_rigid)\n\t\t\t{\n\t\t\t\tmemPtr->m_bodyBtype = BT_JOINT_RIGID_BODY;\n\t\t\t\tmemPtr->m_bodyB = serializer->getUniquePointer((void*)m_joints[i]->m_bodies[1].m_rigid);\n\t\t\t}\n\t\t}\n\t\tserializer->finalizeChunk(chunk, \"btSoftBodyJointData\", BT_ARRAY_CODE, (void*)&m_joints[0]);\n\t}\n\n\treturn btSoftBodyDataName;\n}\n", "hunk": "@@ -20,7 +20,6 @@ subject to the following restrictions:\n #include \"LinearMath/btSerializer.h\"\n #include \"BulletDynamics/Featherstone/btMultiBodyLinkCollider.h\"\n #include \"BulletDynamics/Featherstone/btMultiBodyConstraint.h\"\n-#include <iostream>\n //\n btSoftBody::btSoftBody(btSoftBodyWorldInfo* worldInfo, int node_count, const btVector3* x, const btScalar* m)\n \t: m_softBodySolver(0), m_worldInfo(worldInfo)\n", "comment": "remove, not needed?", "ids": ["10024", "076c8b11df6392421a6d7b5b2e8090c4400aac4f", "4df31305a8677828725318805b5555c56e612ab5"], "repo": "bulletphysics/bullet3", "ghid": 2373, "old": " #include \"LinearMath/btSerializer.h\"\n #include \"BulletDynamics/Featherstone/btMultiBodyLinkCollider.h\"\n #include \"BulletDynamics/Featherstone/btMultiBodyConstraint.h\"\n-#include <iostream>\n //\n btSoftBody::btSoftBody(btSoftBodyWorldInfo* worldInfo, int node_count, const btVector3* x, const btScalar* m)\n \t: m_softBodySolver(0), m_worldInfo(worldInfo)", "new": " #include \"LinearMath/btSerializer.h\"\n #include \"BulletDynamics/Featherstone/btMultiBodyLinkCollider.h\"\n #include \"BulletDynamics/Featherstone/btMultiBodyConstraint.h\"\n //\n btSoftBody::btSoftBody(btSoftBodyWorldInfo* worldInfo, int node_count, const btVector3* x, const btScalar* m)\n \t: m_softBodySolver(0), m_worldInfo(worldInfo)", "lang": "cpp", "norm_lang": "cpp"}
