{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.997333333333334,
  "eval_steps": 500,
  "global_step": 4685,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.39315149188041687,
      "learning_rate": 4.989327641408751e-05,
      "loss": 1.8649,
      "step": 10
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.7232233881950378,
      "learning_rate": 4.978655282817503e-05,
      "loss": 1.6598,
      "step": 20
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.48653826117515564,
      "learning_rate": 4.967982924226254e-05,
      "loss": 1.4566,
      "step": 30
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.6166185140609741,
      "learning_rate": 4.957310565635006e-05,
      "loss": 1.1628,
      "step": 40
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.29660433530807495,
      "learning_rate": 4.946638207043757e-05,
      "loss": 0.9958,
      "step": 50
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.21966253221035004,
      "learning_rate": 4.9359658484525085e-05,
      "loss": 0.9104,
      "step": 60
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.25128474831581116,
      "learning_rate": 4.9252934898612595e-05,
      "loss": 0.7793,
      "step": 70
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.7901301383972168,
      "learning_rate": 4.9146211312700106e-05,
      "loss": 0.7767,
      "step": 80
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.1272401213645935,
      "learning_rate": 4.903948772678762e-05,
      "loss": 0.7666,
      "step": 90
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.18555931746959686,
      "learning_rate": 4.893276414087514e-05,
      "loss": 0.7948,
      "step": 100
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.14884790778160095,
      "learning_rate": 4.882604055496265e-05,
      "loss": 0.7713,
      "step": 110
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.1321442723274231,
      "learning_rate": 4.871931696905016e-05,
      "loss": 0.7515,
      "step": 120
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.1595715880393982,
      "learning_rate": 4.861259338313768e-05,
      "loss": 0.777,
      "step": 130
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.13599473237991333,
      "learning_rate": 4.850586979722519e-05,
      "loss": 0.702,
      "step": 140
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.1295763999223709,
      "learning_rate": 4.83991462113127e-05,
      "loss": 0.7121,
      "step": 150
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.1304360181093216,
      "learning_rate": 4.8292422625400216e-05,
      "loss": 0.7333,
      "step": 160
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.16289834678173065,
      "learning_rate": 4.818569903948773e-05,
      "loss": 0.7596,
      "step": 170
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.14978666603565216,
      "learning_rate": 4.8078975453575244e-05,
      "loss": 0.7098,
      "step": 180
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.13779111206531525,
      "learning_rate": 4.7972251867662754e-05,
      "loss": 0.7584,
      "step": 190
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.17351262271404266,
      "learning_rate": 4.786552828175027e-05,
      "loss": 0.7333,
      "step": 200
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.15001042187213898,
      "learning_rate": 4.775880469583778e-05,
      "loss": 0.7431,
      "step": 210
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.15785029530525208,
      "learning_rate": 4.765208110992529e-05,
      "loss": 0.7124,
      "step": 220
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.20871877670288086,
      "learning_rate": 4.754535752401281e-05,
      "loss": 0.7443,
      "step": 230
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.17311517894268036,
      "learning_rate": 4.7438633938100326e-05,
      "loss": 0.7451,
      "step": 240
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.16002130508422852,
      "learning_rate": 4.7331910352187837e-05,
      "loss": 0.758,
      "step": 250
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.16440792381763458,
      "learning_rate": 4.722518676627535e-05,
      "loss": 0.6841,
      "step": 260
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.1622059941291809,
      "learning_rate": 4.7118463180362864e-05,
      "loss": 0.7289,
      "step": 270
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.12765084207057953,
      "learning_rate": 4.7011739594450374e-05,
      "loss": 0.7203,
      "step": 280
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.16265790164470673,
      "learning_rate": 4.690501600853789e-05,
      "loss": 0.7362,
      "step": 290
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.15405231714248657,
      "learning_rate": 4.67982924226254e-05,
      "loss": 0.671,
      "step": 300
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.14225955307483673,
      "learning_rate": 4.669156883671292e-05,
      "loss": 0.7196,
      "step": 310
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.16231924295425415,
      "learning_rate": 4.658484525080043e-05,
      "loss": 0.7538,
      "step": 320
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.15867199003696442,
      "learning_rate": 4.647812166488794e-05,
      "loss": 0.7562,
      "step": 330
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.18693692982196808,
      "learning_rate": 4.637139807897545e-05,
      "loss": 0.7062,
      "step": 340
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.1409597098827362,
      "learning_rate": 4.626467449306297e-05,
      "loss": 0.6925,
      "step": 350
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.18996088206768036,
      "learning_rate": 4.6157950907150485e-05,
      "loss": 0.728,
      "step": 360
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.19320620596408844,
      "learning_rate": 4.6051227321237995e-05,
      "loss": 0.7236,
      "step": 370
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.17149055004119873,
      "learning_rate": 4.594450373532551e-05,
      "loss": 0.7167,
      "step": 380
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.1423613280057907,
      "learning_rate": 4.583778014941302e-05,
      "loss": 0.699,
      "step": 390
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.17849543690681458,
      "learning_rate": 4.573105656350053e-05,
      "loss": 0.7305,
      "step": 400
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.153657466173172,
      "learning_rate": 4.562433297758805e-05,
      "loss": 0.7336,
      "step": 410
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.16449181735515594,
      "learning_rate": 4.551760939167557e-05,
      "loss": 0.7182,
      "step": 420
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.1856709122657776,
      "learning_rate": 4.541088580576308e-05,
      "loss": 0.7302,
      "step": 430
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.17976801097393036,
      "learning_rate": 4.530416221985059e-05,
      "loss": 0.7051,
      "step": 440
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.19145567715168,
      "learning_rate": 4.5197438633938105e-05,
      "loss": 0.7065,
      "step": 450
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.1428457498550415,
      "learning_rate": 4.5090715048025616e-05,
      "loss": 0.7195,
      "step": 460
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.1683633029460907,
      "learning_rate": 4.4983991462113126e-05,
      "loss": 0.7674,
      "step": 470
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.13846641778945923,
      "learning_rate": 4.487726787620064e-05,
      "loss": 0.709,
      "step": 480
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.24768641591072083,
      "learning_rate": 4.477054429028816e-05,
      "loss": 0.6866,
      "step": 490
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.18456393480300903,
      "learning_rate": 4.466382070437567e-05,
      "loss": 0.7099,
      "step": 500
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.14992839097976685,
      "learning_rate": 4.455709711846318e-05,
      "loss": 0.731,
      "step": 510
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.16389383375644684,
      "learning_rate": 4.445037353255069e-05,
      "loss": 0.6937,
      "step": 520
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.18198269605636597,
      "learning_rate": 4.434364994663821e-05,
      "loss": 0.69,
      "step": 530
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.22387534379959106,
      "learning_rate": 4.423692636072572e-05,
      "loss": 0.7132,
      "step": 540
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.1463167816400528,
      "learning_rate": 4.4130202774813236e-05,
      "loss": 0.711,
      "step": 550
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.1675569862127304,
      "learning_rate": 4.402347918890075e-05,
      "loss": 0.6919,
      "step": 560
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.16866162419319153,
      "learning_rate": 4.3916755602988264e-05,
      "loss": 0.7346,
      "step": 570
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.23900574445724487,
      "learning_rate": 4.3810032017075774e-05,
      "loss": 0.6936,
      "step": 580
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.14893801510334015,
      "learning_rate": 4.3703308431163285e-05,
      "loss": 0.7232,
      "step": 590
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.16957466304302216,
      "learning_rate": 4.35965848452508e-05,
      "loss": 0.6749,
      "step": 600
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.1837603598833084,
      "learning_rate": 4.348986125933832e-05,
      "loss": 0.7321,
      "step": 610
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.2097473293542862,
      "learning_rate": 4.338313767342583e-05,
      "loss": 0.7013,
      "step": 620
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.11885891854763031,
      "learning_rate": 4.3276414087513346e-05,
      "loss": 0.6534,
      "step": 630
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.21636512875556946,
      "learning_rate": 4.316969050160086e-05,
      "loss": 0.6934,
      "step": 640
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.2532427906990051,
      "learning_rate": 4.306296691568837e-05,
      "loss": 0.7092,
      "step": 650
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.18687649071216583,
      "learning_rate": 4.295624332977588e-05,
      "loss": 0.663,
      "step": 660
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.1455783098936081,
      "learning_rate": 4.2849519743863395e-05,
      "loss": 0.656,
      "step": 670
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.21690978109836578,
      "learning_rate": 4.274279615795091e-05,
      "loss": 0.6908,
      "step": 680
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.19117318093776703,
      "learning_rate": 4.263607257203842e-05,
      "loss": 0.6602,
      "step": 690
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.24022670090198517,
      "learning_rate": 4.252934898612593e-05,
      "loss": 0.6907,
      "step": 700
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.1909240335226059,
      "learning_rate": 4.242262540021345e-05,
      "loss": 0.6958,
      "step": 710
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.21230433881282806,
      "learning_rate": 4.231590181430096e-05,
      "loss": 0.7722,
      "step": 720
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.1414554864168167,
      "learning_rate": 4.220917822838848e-05,
      "loss": 0.6947,
      "step": 730
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.18378247320652008,
      "learning_rate": 4.2102454642475994e-05,
      "loss": 0.7558,
      "step": 740
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.1874721795320511,
      "learning_rate": 4.1995731056563505e-05,
      "loss": 0.6772,
      "step": 750
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.20392732322216034,
      "learning_rate": 4.1889007470651015e-05,
      "loss": 0.7076,
      "step": 760
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.12976324558258057,
      "learning_rate": 4.1782283884738526e-05,
      "loss": 0.7194,
      "step": 770
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.22555527091026306,
      "learning_rate": 4.167556029882604e-05,
      "loss": 0.6905,
      "step": 780
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.2074316442012787,
      "learning_rate": 4.156883671291355e-05,
      "loss": 0.6782,
      "step": 790
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.14952561259269714,
      "learning_rate": 4.146211312700107e-05,
      "loss": 0.6985,
      "step": 800
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.18298178911209106,
      "learning_rate": 4.135538954108859e-05,
      "loss": 0.722,
      "step": 810
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.23262664675712585,
      "learning_rate": 4.12486659551761e-05,
      "loss": 0.741,
      "step": 820
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.20429743826389313,
      "learning_rate": 4.114194236926361e-05,
      "loss": 0.7076,
      "step": 830
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.17883698642253876,
      "learning_rate": 4.103521878335112e-05,
      "loss": 0.7214,
      "step": 840
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.171857088804245,
      "learning_rate": 4.0928495197438636e-05,
      "loss": 0.6746,
      "step": 850
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.18807074427604675,
      "learning_rate": 4.0821771611526146e-05,
      "loss": 0.725,
      "step": 860
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.16499720513820648,
      "learning_rate": 4.071504802561366e-05,
      "loss": 0.6792,
      "step": 870
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.23079945147037506,
      "learning_rate": 4.060832443970118e-05,
      "loss": 0.7075,
      "step": 880
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.21911008656024933,
      "learning_rate": 4.050160085378869e-05,
      "loss": 0.6986,
      "step": 890
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.19417890906333923,
      "learning_rate": 4.03948772678762e-05,
      "loss": 0.6722,
      "step": 900
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.17859052121639252,
      "learning_rate": 4.028815368196371e-05,
      "loss": 0.6936,
      "step": 910
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.2804301679134369,
      "learning_rate": 4.018143009605123e-05,
      "loss": 0.6698,
      "step": 920
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.21827533841133118,
      "learning_rate": 4.0074706510138746e-05,
      "loss": 0.7281,
      "step": 930
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.2162380814552307,
      "learning_rate": 3.9967982924226256e-05,
      "loss": 0.6795,
      "step": 940
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.17012855410575867,
      "learning_rate": 3.986125933831377e-05,
      "loss": 0.6992,
      "step": 950
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.17172664403915405,
      "learning_rate": 3.9754535752401284e-05,
      "loss": 0.683,
      "step": 960
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.2001529335975647,
      "learning_rate": 3.9647812166488794e-05,
      "loss": 0.6875,
      "step": 970
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.2359590381383896,
      "learning_rate": 3.9541088580576305e-05,
      "loss": 0.6781,
      "step": 980
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.17030252516269684,
      "learning_rate": 3.943436499466382e-05,
      "loss": 0.6837,
      "step": 990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.1911737024784088,
      "learning_rate": 3.932764140875134e-05,
      "loss": 0.7121,
      "step": 1000
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.20846010744571686,
      "learning_rate": 3.922091782283885e-05,
      "loss": 0.7464,
      "step": 1010
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.18140888214111328,
      "learning_rate": 3.911419423692636e-05,
      "loss": 0.7122,
      "step": 1020
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.1952214539051056,
      "learning_rate": 3.900747065101388e-05,
      "loss": 0.6589,
      "step": 1030
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.23907457292079926,
      "learning_rate": 3.890074706510139e-05,
      "loss": 0.6804,
      "step": 1040
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.19612807035446167,
      "learning_rate": 3.8794023479188905e-05,
      "loss": 0.6485,
      "step": 1050
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.2104843407869339,
      "learning_rate": 3.868729989327642e-05,
      "loss": 0.6572,
      "step": 1060
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.16674494743347168,
      "learning_rate": 3.858057630736393e-05,
      "loss": 0.6993,
      "step": 1070
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.18902920186519623,
      "learning_rate": 3.847385272145144e-05,
      "loss": 0.7461,
      "step": 1080
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.2078789472579956,
      "learning_rate": 3.836712913553895e-05,
      "loss": 0.7181,
      "step": 1090
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.20381996035575867,
      "learning_rate": 3.826040554962647e-05,
      "loss": 0.6797,
      "step": 1100
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.20989003777503967,
      "learning_rate": 3.815368196371398e-05,
      "loss": 0.7185,
      "step": 1110
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.23547278344631195,
      "learning_rate": 3.80469583778015e-05,
      "loss": 0.6982,
      "step": 1120
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.18762676417827606,
      "learning_rate": 3.794023479188901e-05,
      "loss": 0.6994,
      "step": 1130
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.213088721036911,
      "learning_rate": 3.7833511205976525e-05,
      "loss": 0.6967,
      "step": 1140
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.2242196500301361,
      "learning_rate": 3.7726787620064035e-05,
      "loss": 0.6674,
      "step": 1150
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.2097507119178772,
      "learning_rate": 3.7620064034151546e-05,
      "loss": 0.6832,
      "step": 1160
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.23843298852443695,
      "learning_rate": 3.751334044823906e-05,
      "loss": 0.6862,
      "step": 1170
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.15552538633346558,
      "learning_rate": 3.740661686232657e-05,
      "loss": 0.6762,
      "step": 1180
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.1627255380153656,
      "learning_rate": 3.729989327641409e-05,
      "loss": 0.7114,
      "step": 1190
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.23144246637821198,
      "learning_rate": 3.71931696905016e-05,
      "loss": 0.6801,
      "step": 1200
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.2540591061115265,
      "learning_rate": 3.708644610458912e-05,
      "loss": 0.7146,
      "step": 1210
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.20279955863952637,
      "learning_rate": 3.697972251867663e-05,
      "loss": 0.7016,
      "step": 1220
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.2434258908033371,
      "learning_rate": 3.687299893276414e-05,
      "loss": 0.7265,
      "step": 1230
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.15988418459892273,
      "learning_rate": 3.6766275346851656e-05,
      "loss": 0.7164,
      "step": 1240
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.19720208644866943,
      "learning_rate": 3.665955176093917e-05,
      "loss": 0.6988,
      "step": 1250
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.23216931521892548,
      "learning_rate": 3.6552828175026684e-05,
      "loss": 0.6792,
      "step": 1260
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.20333850383758545,
      "learning_rate": 3.6446104589114194e-05,
      "loss": 0.6638,
      "step": 1270
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.19859528541564941,
      "learning_rate": 3.633938100320171e-05,
      "loss": 0.6579,
      "step": 1280
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.2602043151855469,
      "learning_rate": 3.623265741728922e-05,
      "loss": 0.7246,
      "step": 1290
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.18574796617031097,
      "learning_rate": 3.612593383137673e-05,
      "loss": 0.6634,
      "step": 1300
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.17374229431152344,
      "learning_rate": 3.601921024546425e-05,
      "loss": 0.665,
      "step": 1310
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.2042325884103775,
      "learning_rate": 3.5912486659551766e-05,
      "loss": 0.6667,
      "step": 1320
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.2535931169986725,
      "learning_rate": 3.5805763073639277e-05,
      "loss": 0.7075,
      "step": 1330
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.2305154651403427,
      "learning_rate": 3.569903948772679e-05,
      "loss": 0.7208,
      "step": 1340
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.22205843031406403,
      "learning_rate": 3.5592315901814304e-05,
      "loss": 0.6722,
      "step": 1350
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.24596905708312988,
      "learning_rate": 3.5485592315901815e-05,
      "loss": 0.6881,
      "step": 1360
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.18312782049179077,
      "learning_rate": 3.537886872998933e-05,
      "loss": 0.727,
      "step": 1370
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.3052126467227936,
      "learning_rate": 3.527214514407684e-05,
      "loss": 0.6833,
      "step": 1380
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.22231629490852356,
      "learning_rate": 3.516542155816436e-05,
      "loss": 0.6475,
      "step": 1390
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.19054578244686127,
      "learning_rate": 3.505869797225187e-05,
      "loss": 0.7163,
      "step": 1400
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.24421140551567078,
      "learning_rate": 3.495197438633938e-05,
      "loss": 0.7329,
      "step": 1410
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.21246083080768585,
      "learning_rate": 3.48452508004269e-05,
      "loss": 0.7052,
      "step": 1420
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.18109168112277985,
      "learning_rate": 3.473852721451441e-05,
      "loss": 0.6472,
      "step": 1430
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.17983883619308472,
      "learning_rate": 3.4631803628601925e-05,
      "loss": 0.7118,
      "step": 1440
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.22573883831501007,
      "learning_rate": 3.4525080042689435e-05,
      "loss": 0.7186,
      "step": 1450
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.29673320055007935,
      "learning_rate": 3.441835645677695e-05,
      "loss": 0.7046,
      "step": 1460
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.18770591914653778,
      "learning_rate": 3.431163287086446e-05,
      "loss": 0.7135,
      "step": 1470
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.17907392978668213,
      "learning_rate": 3.420490928495197e-05,
      "loss": 0.716,
      "step": 1480
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.19616210460662842,
      "learning_rate": 3.409818569903949e-05,
      "loss": 0.6994,
      "step": 1490
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.20491744577884674,
      "learning_rate": 3.3991462113127e-05,
      "loss": 0.6894,
      "step": 1500
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.19506055116653442,
      "learning_rate": 3.388473852721452e-05,
      "loss": 0.7141,
      "step": 1510
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.19708651304244995,
      "learning_rate": 3.377801494130203e-05,
      "loss": 0.6707,
      "step": 1520
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.22450290620326996,
      "learning_rate": 3.3671291355389545e-05,
      "loss": 0.654,
      "step": 1530
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.17565590143203735,
      "learning_rate": 3.3564567769477056e-05,
      "loss": 0.6929,
      "step": 1540
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.24137593805789948,
      "learning_rate": 3.3457844183564566e-05,
      "loss": 0.7084,
      "step": 1550
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.24566271901130676,
      "learning_rate": 3.335112059765208e-05,
      "loss": 0.7214,
      "step": 1560
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.19699178636074066,
      "learning_rate": 3.32443970117396e-05,
      "loss": 0.6731,
      "step": 1570
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.30191025137901306,
      "learning_rate": 3.313767342582711e-05,
      "loss": 0.7023,
      "step": 1580
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.24787668883800507,
      "learning_rate": 3.303094983991462e-05,
      "loss": 0.6939,
      "step": 1590
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.22325973212718964,
      "learning_rate": 3.292422625400214e-05,
      "loss": 0.7029,
      "step": 1600
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.23324540257453918,
      "learning_rate": 3.281750266808965e-05,
      "loss": 0.692,
      "step": 1610
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.3875426948070526,
      "learning_rate": 3.271077908217716e-05,
      "loss": 0.686,
      "step": 1620
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.21206851303577423,
      "learning_rate": 3.2604055496264676e-05,
      "loss": 0.7263,
      "step": 1630
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.15261982381343842,
      "learning_rate": 3.249733191035219e-05,
      "loss": 0.6343,
      "step": 1640
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.3940684497356415,
      "learning_rate": 3.2390608324439704e-05,
      "loss": 0.6591,
      "step": 1650
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.2745518982410431,
      "learning_rate": 3.2283884738527214e-05,
      "loss": 0.7074,
      "step": 1660
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.25032898783683777,
      "learning_rate": 3.217716115261473e-05,
      "loss": 0.7113,
      "step": 1670
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.31599798798561096,
      "learning_rate": 3.207043756670224e-05,
      "loss": 0.7172,
      "step": 1680
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.24669413268566132,
      "learning_rate": 3.196371398078976e-05,
      "loss": 0.6739,
      "step": 1690
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.2398202121257782,
      "learning_rate": 3.185699039487727e-05,
      "loss": 0.7225,
      "step": 1700
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.33361637592315674,
      "learning_rate": 3.1750266808964786e-05,
      "loss": 0.7064,
      "step": 1710
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.21621499955654144,
      "learning_rate": 3.16435432230523e-05,
      "loss": 0.7175,
      "step": 1720
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.1970442831516266,
      "learning_rate": 3.153681963713981e-05,
      "loss": 0.6818,
      "step": 1730
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.20512673258781433,
      "learning_rate": 3.143009605122732e-05,
      "loss": 0.7127,
      "step": 1740
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.21329395473003387,
      "learning_rate": 3.1323372465314835e-05,
      "loss": 0.7513,
      "step": 1750
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.22758838534355164,
      "learning_rate": 3.121664887940235e-05,
      "loss": 0.6811,
      "step": 1760
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.21328574419021606,
      "learning_rate": 3.110992529348986e-05,
      "loss": 0.7064,
      "step": 1770
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.258022278547287,
      "learning_rate": 3.100320170757738e-05,
      "loss": 0.6954,
      "step": 1780
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.2866305112838745,
      "learning_rate": 3.089647812166489e-05,
      "loss": 0.7165,
      "step": 1790
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.23526230454444885,
      "learning_rate": 3.07897545357524e-05,
      "loss": 0.7492,
      "step": 1800
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.2144688367843628,
      "learning_rate": 3.068303094983991e-05,
      "loss": 0.6928,
      "step": 1810
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.2404981553554535,
      "learning_rate": 3.057630736392743e-05,
      "loss": 0.6754,
      "step": 1820
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.2073257714509964,
      "learning_rate": 3.046958377801494e-05,
      "loss": 0.6658,
      "step": 1830
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.2183234989643097,
      "learning_rate": 3.0362860192102455e-05,
      "loss": 0.6718,
      "step": 1840
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.24871061742305756,
      "learning_rate": 3.0256136606189972e-05,
      "loss": 0.7326,
      "step": 1850
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.3233272135257721,
      "learning_rate": 3.0149413020277483e-05,
      "loss": 0.6868,
      "step": 1860
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 0.21745933592319489,
      "learning_rate": 3.0042689434364997e-05,
      "loss": 0.6692,
      "step": 1870
    },
    {
      "epoch": 2.005333333333333,
      "grad_norm": 0.2379501760005951,
      "learning_rate": 2.9935965848452507e-05,
      "loss": 0.6934,
      "step": 1880
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.23766635358333588,
      "learning_rate": 2.9829242262540024e-05,
      "loss": 0.7029,
      "step": 1890
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.23234614729881287,
      "learning_rate": 2.9722518676627538e-05,
      "loss": 0.6974,
      "step": 1900
    },
    {
      "epoch": 2.037333333333333,
      "grad_norm": 0.26801279187202454,
      "learning_rate": 2.961579509071505e-05,
      "loss": 0.7133,
      "step": 1910
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.2236751765012741,
      "learning_rate": 2.950907150480256e-05,
      "loss": 0.7037,
      "step": 1920
    },
    {
      "epoch": 2.058666666666667,
      "grad_norm": 0.26362311840057373,
      "learning_rate": 2.9402347918890076e-05,
      "loss": 0.6809,
      "step": 1930
    },
    {
      "epoch": 2.0693333333333332,
      "grad_norm": 0.20284001529216766,
      "learning_rate": 2.929562433297759e-05,
      "loss": 0.6831,
      "step": 1940
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.3805771768093109,
      "learning_rate": 2.91889007470651e-05,
      "loss": 0.6723,
      "step": 1950
    },
    {
      "epoch": 2.0906666666666665,
      "grad_norm": 0.2803097069263458,
      "learning_rate": 2.9082177161152617e-05,
      "loss": 0.6923,
      "step": 1960
    },
    {
      "epoch": 2.1013333333333333,
      "grad_norm": 0.23863375186920166,
      "learning_rate": 2.897545357524013e-05,
      "loss": 0.7277,
      "step": 1970
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.2176768183708191,
      "learning_rate": 2.886872998932764e-05,
      "loss": 0.6752,
      "step": 1980
    },
    {
      "epoch": 2.1226666666666665,
      "grad_norm": 0.24624812602996826,
      "learning_rate": 2.8762006403415155e-05,
      "loss": 0.6358,
      "step": 1990
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.19733937084674835,
      "learning_rate": 2.8655282817502672e-05,
      "loss": 0.7006,
      "step": 2000
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.22223447263240814,
      "learning_rate": 2.8548559231590183e-05,
      "loss": 0.6815,
      "step": 2010
    },
    {
      "epoch": 2.1546666666666665,
      "grad_norm": 0.2349349707365036,
      "learning_rate": 2.8441835645677693e-05,
      "loss": 0.6963,
      "step": 2020
    },
    {
      "epoch": 2.1653333333333333,
      "grad_norm": 0.35477495193481445,
      "learning_rate": 2.8335112059765214e-05,
      "loss": 0.7087,
      "step": 2030
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.2817108631134033,
      "learning_rate": 2.8228388473852724e-05,
      "loss": 0.6929,
      "step": 2040
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.321239173412323,
      "learning_rate": 2.8121664887940234e-05,
      "loss": 0.6704,
      "step": 2050
    },
    {
      "epoch": 2.1973333333333334,
      "grad_norm": 0.25610828399658203,
      "learning_rate": 2.8014941302027748e-05,
      "loss": 0.6953,
      "step": 2060
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.21442820131778717,
      "learning_rate": 2.7908217716115265e-05,
      "loss": 0.6751,
      "step": 2070
    },
    {
      "epoch": 2.2186666666666666,
      "grad_norm": 0.26534998416900635,
      "learning_rate": 2.7801494130202776e-05,
      "loss": 0.6556,
      "step": 2080
    },
    {
      "epoch": 2.2293333333333334,
      "grad_norm": 0.2144535332918167,
      "learning_rate": 2.769477054429029e-05,
      "loss": 0.6657,
      "step": 2090
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.25956422090530396,
      "learning_rate": 2.75880469583778e-05,
      "loss": 0.6998,
      "step": 2100
    },
    {
      "epoch": 2.2506666666666666,
      "grad_norm": 0.24529626965522766,
      "learning_rate": 2.7481323372465317e-05,
      "loss": 0.6543,
      "step": 2110
    },
    {
      "epoch": 2.2613333333333334,
      "grad_norm": 0.2542750835418701,
      "learning_rate": 2.737459978655283e-05,
      "loss": 0.7391,
      "step": 2120
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.295322060585022,
      "learning_rate": 2.726787620064034e-05,
      "loss": 0.6461,
      "step": 2130
    },
    {
      "epoch": 2.2826666666666666,
      "grad_norm": 0.2938515841960907,
      "learning_rate": 2.716115261472786e-05,
      "loss": 0.7157,
      "step": 2140
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.21199433505535126,
      "learning_rate": 2.705442902881537e-05,
      "loss": 0.6924,
      "step": 2150
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.3504100441932678,
      "learning_rate": 2.6947705442902882e-05,
      "loss": 0.6797,
      "step": 2160
    },
    {
      "epoch": 2.3146666666666667,
      "grad_norm": 0.29771971702575684,
      "learning_rate": 2.6840981856990393e-05,
      "loss": 0.7627,
      "step": 2170
    },
    {
      "epoch": 2.3253333333333335,
      "grad_norm": 0.2720763087272644,
      "learning_rate": 2.673425827107791e-05,
      "loss": 0.7103,
      "step": 2180
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.24417491257190704,
      "learning_rate": 2.6627534685165424e-05,
      "loss": 0.6845,
      "step": 2190
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.3275480270385742,
      "learning_rate": 2.6520811099252934e-05,
      "loss": 0.71,
      "step": 2200
    },
    {
      "epoch": 2.3573333333333335,
      "grad_norm": 0.31455519795417786,
      "learning_rate": 2.641408751334045e-05,
      "loss": 0.6669,
      "step": 2210
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.2258215695619583,
      "learning_rate": 2.6307363927427965e-05,
      "loss": 0.658,
      "step": 2220
    },
    {
      "epoch": 2.3786666666666667,
      "grad_norm": 0.28350913524627686,
      "learning_rate": 2.6200640341515475e-05,
      "loss": 0.68,
      "step": 2230
    },
    {
      "epoch": 2.389333333333333,
      "grad_norm": 0.3048681914806366,
      "learning_rate": 2.6093916755602986e-05,
      "loss": 0.7446,
      "step": 2240
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.2774466872215271,
      "learning_rate": 2.5987193169690503e-05,
      "loss": 0.713,
      "step": 2250
    },
    {
      "epoch": 2.4106666666666667,
      "grad_norm": 0.2091231793165207,
      "learning_rate": 2.5880469583778017e-05,
      "loss": 0.6891,
      "step": 2260
    },
    {
      "epoch": 2.421333333333333,
      "grad_norm": 0.2834564447402954,
      "learning_rate": 2.5773745997865527e-05,
      "loss": 0.6931,
      "step": 2270
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.2789973020553589,
      "learning_rate": 2.5667022411953044e-05,
      "loss": 0.6577,
      "step": 2280
    },
    {
      "epoch": 2.4426666666666668,
      "grad_norm": 0.3369174897670746,
      "learning_rate": 2.5560298826040558e-05,
      "loss": 0.6589,
      "step": 2290
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.24351458251476288,
      "learning_rate": 2.545357524012807e-05,
      "loss": 0.6991,
      "step": 2300
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.26489633321762085,
      "learning_rate": 2.5346851654215582e-05,
      "loss": 0.7103,
      "step": 2310
    },
    {
      "epoch": 2.474666666666667,
      "grad_norm": 0.255263090133667,
      "learning_rate": 2.52401280683031e-05,
      "loss": 0.6788,
      "step": 2320
    },
    {
      "epoch": 2.485333333333333,
      "grad_norm": 0.2983437478542328,
      "learning_rate": 2.513340448239061e-05,
      "loss": 0.678,
      "step": 2330
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.2957524061203003,
      "learning_rate": 2.502668089647812e-05,
      "loss": 0.6795,
      "step": 2340
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.3012699782848358,
      "learning_rate": 2.4919957310565637e-05,
      "loss": 0.687,
      "step": 2350
    },
    {
      "epoch": 2.517333333333333,
      "grad_norm": 0.37142306566238403,
      "learning_rate": 2.4813233724653148e-05,
      "loss": 0.7181,
      "step": 2360
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.24925312399864197,
      "learning_rate": 2.470651013874066e-05,
      "loss": 0.7067,
      "step": 2370
    },
    {
      "epoch": 2.538666666666667,
      "grad_norm": 0.25066423416137695,
      "learning_rate": 2.459978655282818e-05,
      "loss": 0.6626,
      "step": 2380
    },
    {
      "epoch": 2.5493333333333332,
      "grad_norm": 0.3031352460384369,
      "learning_rate": 2.449306296691569e-05,
      "loss": 0.6796,
      "step": 2390
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.25202447175979614,
      "learning_rate": 2.4386339381003203e-05,
      "loss": 0.6863,
      "step": 2400
    },
    {
      "epoch": 2.570666666666667,
      "grad_norm": 0.26353010535240173,
      "learning_rate": 2.4279615795090717e-05,
      "loss": 0.7054,
      "step": 2410
    },
    {
      "epoch": 2.5813333333333333,
      "grad_norm": 0.3581090271472931,
      "learning_rate": 2.417289220917823e-05,
      "loss": 0.6397,
      "step": 2420
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.284954696893692,
      "learning_rate": 2.406616862326574e-05,
      "loss": 0.7043,
      "step": 2430
    },
    {
      "epoch": 2.602666666666667,
      "grad_norm": 0.29389044642448425,
      "learning_rate": 2.3959445037353258e-05,
      "loss": 0.7172,
      "step": 2440
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.29732492566108704,
      "learning_rate": 2.385272145144077e-05,
      "loss": 0.6893,
      "step": 2450
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.2889479994773865,
      "learning_rate": 2.3745997865528282e-05,
      "loss": 0.7252,
      "step": 2460
    },
    {
      "epoch": 2.634666666666667,
      "grad_norm": 0.26220348477363586,
      "learning_rate": 2.3639274279615796e-05,
      "loss": 0.6973,
      "step": 2470
    },
    {
      "epoch": 2.6453333333333333,
      "grad_norm": 0.30554381012916565,
      "learning_rate": 2.353255069370331e-05,
      "loss": 0.7409,
      "step": 2480
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.2959260642528534,
      "learning_rate": 2.3425827107790823e-05,
      "loss": 0.6954,
      "step": 2490
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.28324297070503235,
      "learning_rate": 2.3319103521878334e-05,
      "loss": 0.6919,
      "step": 2500
    },
    {
      "epoch": 2.6773333333333333,
      "grad_norm": 0.3085155189037323,
      "learning_rate": 2.321237993596585e-05,
      "loss": 0.6802,
      "step": 2510
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.3591720461845398,
      "learning_rate": 2.310565635005336e-05,
      "loss": 0.7077,
      "step": 2520
    },
    {
      "epoch": 2.6986666666666665,
      "grad_norm": 0.2872592806816101,
      "learning_rate": 2.2998932764140875e-05,
      "loss": 0.6639,
      "step": 2530
    },
    {
      "epoch": 2.7093333333333334,
      "grad_norm": 0.3298971951007843,
      "learning_rate": 2.2892209178228392e-05,
      "loss": 0.6944,
      "step": 2540
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.27329039573669434,
      "learning_rate": 2.2785485592315903e-05,
      "loss": 0.6862,
      "step": 2550
    },
    {
      "epoch": 2.7306666666666666,
      "grad_norm": 0.31970590353012085,
      "learning_rate": 2.2678762006403416e-05,
      "loss": 0.7078,
      "step": 2560
    },
    {
      "epoch": 2.7413333333333334,
      "grad_norm": 0.31508493423461914,
      "learning_rate": 2.257203842049093e-05,
      "loss": 0.6845,
      "step": 2570
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.33385103940963745,
      "learning_rate": 2.2465314834578444e-05,
      "loss": 0.728,
      "step": 2580
    },
    {
      "epoch": 2.7626666666666666,
      "grad_norm": 0.3404044508934021,
      "learning_rate": 2.2358591248665954e-05,
      "loss": 0.7286,
      "step": 2590
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.32337188720703125,
      "learning_rate": 2.225186766275347e-05,
      "loss": 0.7567,
      "step": 2600
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.2905850410461426,
      "learning_rate": 2.2145144076840982e-05,
      "loss": 0.6766,
      "step": 2610
    },
    {
      "epoch": 2.7946666666666666,
      "grad_norm": 0.30250245332717896,
      "learning_rate": 2.2038420490928496e-05,
      "loss": 0.6861,
      "step": 2620
    },
    {
      "epoch": 2.8053333333333335,
      "grad_norm": 0.33005282282829285,
      "learning_rate": 2.193169690501601e-05,
      "loss": 0.6837,
      "step": 2630
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.2733724117279053,
      "learning_rate": 2.1824973319103523e-05,
      "loss": 0.716,
      "step": 2640
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.24600441753864288,
      "learning_rate": 2.1718249733191037e-05,
      "loss": 0.7362,
      "step": 2650
    },
    {
      "epoch": 2.8373333333333335,
      "grad_norm": 0.2974744439125061,
      "learning_rate": 2.1611526147278547e-05,
      "loss": 0.6785,
      "step": 2660
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.3164414167404175,
      "learning_rate": 2.1504802561366065e-05,
      "loss": 0.731,
      "step": 2670
    },
    {
      "epoch": 2.8586666666666667,
      "grad_norm": 0.34855008125305176,
      "learning_rate": 2.1398078975453575e-05,
      "loss": 0.6818,
      "step": 2680
    },
    {
      "epoch": 2.8693333333333335,
      "grad_norm": 0.3037462532520294,
      "learning_rate": 2.129135538954109e-05,
      "loss": 0.6913,
      "step": 2690
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.288785457611084,
      "learning_rate": 2.1184631803628602e-05,
      "loss": 0.7058,
      "step": 2700
    },
    {
      "epoch": 2.8906666666666667,
      "grad_norm": 0.2929092049598694,
      "learning_rate": 2.1077908217716116e-05,
      "loss": 0.7206,
      "step": 2710
    },
    {
      "epoch": 2.9013333333333335,
      "grad_norm": 0.2858732044696808,
      "learning_rate": 2.097118463180363e-05,
      "loss": 0.6712,
      "step": 2720
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.2533702552318573,
      "learning_rate": 2.0864461045891144e-05,
      "loss": 0.7009,
      "step": 2730
    },
    {
      "epoch": 2.9226666666666667,
      "grad_norm": 0.3428458869457245,
      "learning_rate": 2.0757737459978658e-05,
      "loss": 0.6795,
      "step": 2740
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.2980753779411316,
      "learning_rate": 2.0651013874066168e-05,
      "loss": 0.6698,
      "step": 2750
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.29078081250190735,
      "learning_rate": 2.0544290288153685e-05,
      "loss": 0.7123,
      "step": 2760
    },
    {
      "epoch": 2.9546666666666668,
      "grad_norm": 0.32628482580184937,
      "learning_rate": 2.0437566702241196e-05,
      "loss": 0.6775,
      "step": 2770
    },
    {
      "epoch": 2.9653333333333336,
      "grad_norm": 0.30978673696517944,
      "learning_rate": 2.033084311632871e-05,
      "loss": 0.6743,
      "step": 2780
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.34201568365097046,
      "learning_rate": 2.0224119530416223e-05,
      "loss": 0.7093,
      "step": 2790
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.3012973666191101,
      "learning_rate": 2.0117395944503737e-05,
      "loss": 0.7207,
      "step": 2800
    },
    {
      "epoch": 2.997333333333333,
      "grad_norm": 0.3408491909503937,
      "learning_rate": 2.001067235859125e-05,
      "loss": 0.6832,
      "step": 2810
    },
    {
      "epoch": 3.008,
      "grad_norm": 0.22999565303325653,
      "learning_rate": 1.9903948772678764e-05,
      "loss": 0.6523,
      "step": 2820
    },
    {
      "epoch": 3.018666666666667,
      "grad_norm": 0.28717947006225586,
      "learning_rate": 1.9797225186766278e-05,
      "loss": 0.6935,
      "step": 2830
    },
    {
      "epoch": 3.029333333333333,
      "grad_norm": 0.2803426682949066,
      "learning_rate": 1.969050160085379e-05,
      "loss": 0.673,
      "step": 2840
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.31004446744918823,
      "learning_rate": 1.9583778014941302e-05,
      "loss": 0.7,
      "step": 2850
    },
    {
      "epoch": 3.050666666666667,
      "grad_norm": 0.3051164746284485,
      "learning_rate": 1.9477054429028816e-05,
      "loss": 0.7073,
      "step": 2860
    },
    {
      "epoch": 3.0613333333333332,
      "grad_norm": 0.3371521532535553,
      "learning_rate": 1.937033084311633e-05,
      "loss": 0.7262,
      "step": 2870
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.3129386901855469,
      "learning_rate": 1.926360725720384e-05,
      "loss": 0.7017,
      "step": 2880
    },
    {
      "epoch": 3.0826666666666664,
      "grad_norm": 0.3656955659389496,
      "learning_rate": 1.9156883671291357e-05,
      "loss": 0.6961,
      "step": 2890
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.41413429379463196,
      "learning_rate": 1.905016008537887e-05,
      "loss": 0.6734,
      "step": 2900
    },
    {
      "epoch": 3.104,
      "grad_norm": 0.26354116201400757,
      "learning_rate": 1.894343649946638e-05,
      "loss": 0.6831,
      "step": 2910
    },
    {
      "epoch": 3.1146666666666665,
      "grad_norm": 0.29318657517433167,
      "learning_rate": 1.88367129135539e-05,
      "loss": 0.6618,
      "step": 2920
    },
    {
      "epoch": 3.1253333333333333,
      "grad_norm": 0.35019299387931824,
      "learning_rate": 1.872998932764141e-05,
      "loss": 0.7271,
      "step": 2930
    },
    {
      "epoch": 3.136,
      "grad_norm": 0.32810482382774353,
      "learning_rate": 1.8623265741728923e-05,
      "loss": 0.6286,
      "step": 2940
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.27279528975486755,
      "learning_rate": 1.8516542155816437e-05,
      "loss": 0.6795,
      "step": 2950
    },
    {
      "epoch": 3.1573333333333333,
      "grad_norm": 0.3471270203590393,
      "learning_rate": 1.840981856990395e-05,
      "loss": 0.7182,
      "step": 2960
    },
    {
      "epoch": 3.168,
      "grad_norm": 0.2921459376811981,
      "learning_rate": 1.830309498399146e-05,
      "loss": 0.7066,
      "step": 2970
    },
    {
      "epoch": 3.1786666666666665,
      "grad_norm": 0.3075445294380188,
      "learning_rate": 1.8196371398078978e-05,
      "loss": 0.6523,
      "step": 2980
    },
    {
      "epoch": 3.1893333333333334,
      "grad_norm": 0.33047857880592346,
      "learning_rate": 1.8089647812166492e-05,
      "loss": 0.7379,
      "step": 2990
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.3291586935520172,
      "learning_rate": 1.7982924226254002e-05,
      "loss": 0.6986,
      "step": 3000
    },
    {
      "epoch": 3.2106666666666666,
      "grad_norm": 0.28082016110420227,
      "learning_rate": 1.7876200640341516e-05,
      "loss": 0.7106,
      "step": 3010
    },
    {
      "epoch": 3.2213333333333334,
      "grad_norm": 0.4224933385848999,
      "learning_rate": 1.776947705442903e-05,
      "loss": 0.7196,
      "step": 3020
    },
    {
      "epoch": 3.232,
      "grad_norm": 0.3950693607330322,
      "learning_rate": 1.7662753468516543e-05,
      "loss": 0.7151,
      "step": 3030
    },
    {
      "epoch": 3.2426666666666666,
      "grad_norm": 0.2704998254776001,
      "learning_rate": 1.7556029882604054e-05,
      "loss": 0.6591,
      "step": 3040
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.3132375478744507,
      "learning_rate": 1.744930629669157e-05,
      "loss": 0.6849,
      "step": 3050
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 0.2558743357658386,
      "learning_rate": 1.734258271077908e-05,
      "loss": 0.6465,
      "step": 3060
    },
    {
      "epoch": 3.2746666666666666,
      "grad_norm": 0.41477257013320923,
      "learning_rate": 1.7235859124866595e-05,
      "loss": 0.7062,
      "step": 3070
    },
    {
      "epoch": 3.2853333333333334,
      "grad_norm": 0.2824729382991791,
      "learning_rate": 1.7129135538954112e-05,
      "loss": 0.7102,
      "step": 3080
    },
    {
      "epoch": 3.296,
      "grad_norm": 0.3549652099609375,
      "learning_rate": 1.7022411953041623e-05,
      "loss": 0.6994,
      "step": 3090
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.2813431918621063,
      "learning_rate": 1.6915688367129136e-05,
      "loss": 0.6788,
      "step": 3100
    },
    {
      "epoch": 3.3173333333333335,
      "grad_norm": 0.32047295570373535,
      "learning_rate": 1.680896478121665e-05,
      "loss": 0.7456,
      "step": 3110
    },
    {
      "epoch": 3.328,
      "grad_norm": 0.32119184732437134,
      "learning_rate": 1.6702241195304164e-05,
      "loss": 0.6837,
      "step": 3120
    },
    {
      "epoch": 3.3386666666666667,
      "grad_norm": 0.3075251579284668,
      "learning_rate": 1.6595517609391674e-05,
      "loss": 0.6764,
      "step": 3130
    },
    {
      "epoch": 3.3493333333333335,
      "grad_norm": 0.3571680188179016,
      "learning_rate": 1.648879402347919e-05,
      "loss": 0.6501,
      "step": 3140
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.30146628618240356,
      "learning_rate": 1.6382070437566702e-05,
      "loss": 0.6759,
      "step": 3150
    },
    {
      "epoch": 3.3706666666666667,
      "grad_norm": 0.3187093734741211,
      "learning_rate": 1.6275346851654216e-05,
      "loss": 0.6762,
      "step": 3160
    },
    {
      "epoch": 3.3813333333333335,
      "grad_norm": 0.37995079159736633,
      "learning_rate": 1.616862326574173e-05,
      "loss": 0.7217,
      "step": 3170
    },
    {
      "epoch": 3.392,
      "grad_norm": 0.2780855894088745,
      "learning_rate": 1.6061899679829243e-05,
      "loss": 0.6862,
      "step": 3180
    },
    {
      "epoch": 3.4026666666666667,
      "grad_norm": 0.259642094373703,
      "learning_rate": 1.5955176093916757e-05,
      "loss": 0.6852,
      "step": 3190
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.3757437765598297,
      "learning_rate": 1.5848452508004267e-05,
      "loss": 0.6994,
      "step": 3200
    },
    {
      "epoch": 3.424,
      "grad_norm": 0.3267432153224945,
      "learning_rate": 1.5741728922091785e-05,
      "loss": 0.6545,
      "step": 3210
    },
    {
      "epoch": 3.4346666666666668,
      "grad_norm": 0.39178723096847534,
      "learning_rate": 1.5635005336179295e-05,
      "loss": 0.7085,
      "step": 3220
    },
    {
      "epoch": 3.445333333333333,
      "grad_norm": 0.44401606917381287,
      "learning_rate": 1.552828175026681e-05,
      "loss": 0.6905,
      "step": 3230
    },
    {
      "epoch": 3.456,
      "grad_norm": 0.3353785276412964,
      "learning_rate": 1.5421558164354323e-05,
      "loss": 0.658,
      "step": 3240
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.29615268111228943,
      "learning_rate": 1.5314834578441836e-05,
      "loss": 0.7014,
      "step": 3250
    },
    {
      "epoch": 3.477333333333333,
      "grad_norm": 0.3338247537612915,
      "learning_rate": 1.520811099252935e-05,
      "loss": 0.6897,
      "step": 3260
    },
    {
      "epoch": 3.488,
      "grad_norm": 0.45884162187576294,
      "learning_rate": 1.5101387406616862e-05,
      "loss": 0.6966,
      "step": 3270
    },
    {
      "epoch": 3.498666666666667,
      "grad_norm": 0.38272011280059814,
      "learning_rate": 1.4994663820704378e-05,
      "loss": 0.7063,
      "step": 3280
    },
    {
      "epoch": 3.509333333333333,
      "grad_norm": 0.32744312286376953,
      "learning_rate": 1.488794023479189e-05,
      "loss": 0.6595,
      "step": 3290
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.34340500831604004,
      "learning_rate": 1.4781216648879403e-05,
      "loss": 0.7226,
      "step": 3300
    },
    {
      "epoch": 3.530666666666667,
      "grad_norm": 0.30055710673332214,
      "learning_rate": 1.4674493062966916e-05,
      "loss": 0.6814,
      "step": 3310
    },
    {
      "epoch": 3.541333333333333,
      "grad_norm": 0.3625183403491974,
      "learning_rate": 1.456776947705443e-05,
      "loss": 0.7133,
      "step": 3320
    },
    {
      "epoch": 3.552,
      "grad_norm": 0.4832961857318878,
      "learning_rate": 1.4461045891141945e-05,
      "loss": 0.7087,
      "step": 3330
    },
    {
      "epoch": 3.562666666666667,
      "grad_norm": 0.32196861505508423,
      "learning_rate": 1.4354322305229457e-05,
      "loss": 0.6769,
      "step": 3340
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.3517327308654785,
      "learning_rate": 1.424759871931697e-05,
      "loss": 0.6845,
      "step": 3350
    },
    {
      "epoch": 3.584,
      "grad_norm": 0.3622489273548126,
      "learning_rate": 1.4140875133404483e-05,
      "loss": 0.6877,
      "step": 3360
    },
    {
      "epoch": 3.594666666666667,
      "grad_norm": 0.2891097068786621,
      "learning_rate": 1.4034151547491996e-05,
      "loss": 0.6983,
      "step": 3370
    },
    {
      "epoch": 3.6053333333333333,
      "grad_norm": 0.3156876862049103,
      "learning_rate": 1.3927427961579509e-05,
      "loss": 0.6852,
      "step": 3380
    },
    {
      "epoch": 3.616,
      "grad_norm": 0.3539501428604126,
      "learning_rate": 1.3820704375667024e-05,
      "loss": 0.7279,
      "step": 3390
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.3300153315067291,
      "learning_rate": 1.3713980789754536e-05,
      "loss": 0.7208,
      "step": 3400
    },
    {
      "epoch": 3.6373333333333333,
      "grad_norm": 0.36536741256713867,
      "learning_rate": 1.360725720384205e-05,
      "loss": 0.7249,
      "step": 3410
    },
    {
      "epoch": 3.648,
      "grad_norm": 0.36967119574546814,
      "learning_rate": 1.3500533617929564e-05,
      "loss": 0.7128,
      "step": 3420
    },
    {
      "epoch": 3.6586666666666665,
      "grad_norm": 0.3356037437915802,
      "learning_rate": 1.3393810032017076e-05,
      "loss": 0.6858,
      "step": 3430
    },
    {
      "epoch": 3.6693333333333333,
      "grad_norm": 0.3822891414165497,
      "learning_rate": 1.3287086446104591e-05,
      "loss": 0.6465,
      "step": 3440
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.3321053385734558,
      "learning_rate": 1.3180362860192103e-05,
      "loss": 0.7203,
      "step": 3450
    },
    {
      "epoch": 3.6906666666666665,
      "grad_norm": 0.3815690875053406,
      "learning_rate": 1.3073639274279617e-05,
      "loss": 0.6804,
      "step": 3460
    },
    {
      "epoch": 3.7013333333333334,
      "grad_norm": 0.344780832529068,
      "learning_rate": 1.2966915688367129e-05,
      "loss": 0.6483,
      "step": 3470
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 0.3235776424407959,
      "learning_rate": 1.2860192102454643e-05,
      "loss": 0.6687,
      "step": 3480
    },
    {
      "epoch": 3.7226666666666666,
      "grad_norm": 0.3372635543346405,
      "learning_rate": 1.2753468516542155e-05,
      "loss": 0.6837,
      "step": 3490
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.3860567510128021,
      "learning_rate": 1.264674493062967e-05,
      "loss": 0.6883,
      "step": 3500
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 0.29392722249031067,
      "learning_rate": 1.2540021344717184e-05,
      "loss": 0.7078,
      "step": 3510
    },
    {
      "epoch": 3.7546666666666666,
      "grad_norm": 0.42315688729286194,
      "learning_rate": 1.2433297758804696e-05,
      "loss": 0.7038,
      "step": 3520
    },
    {
      "epoch": 3.7653333333333334,
      "grad_norm": 0.49315616488456726,
      "learning_rate": 1.232657417289221e-05,
      "loss": 0.6678,
      "step": 3530
    },
    {
      "epoch": 3.776,
      "grad_norm": 0.33091628551483154,
      "learning_rate": 1.2219850586979722e-05,
      "loss": 0.7077,
      "step": 3540
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.3673006594181061,
      "learning_rate": 1.2113127001067238e-05,
      "loss": 0.6839,
      "step": 3550
    },
    {
      "epoch": 3.7973333333333334,
      "grad_norm": 0.3909212052822113,
      "learning_rate": 1.200640341515475e-05,
      "loss": 0.6634,
      "step": 3560
    },
    {
      "epoch": 3.808,
      "grad_norm": 0.33070850372314453,
      "learning_rate": 1.1899679829242263e-05,
      "loss": 0.6918,
      "step": 3570
    },
    {
      "epoch": 3.8186666666666667,
      "grad_norm": 0.4052761197090149,
      "learning_rate": 1.1792956243329777e-05,
      "loss": 0.6823,
      "step": 3580
    },
    {
      "epoch": 3.8293333333333335,
      "grad_norm": 0.47373491525650024,
      "learning_rate": 1.168623265741729e-05,
      "loss": 0.6893,
      "step": 3590
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.37801310420036316,
      "learning_rate": 1.1579509071504803e-05,
      "loss": 0.701,
      "step": 3600
    },
    {
      "epoch": 3.8506666666666667,
      "grad_norm": 0.3655286729335785,
      "learning_rate": 1.1472785485592317e-05,
      "loss": 0.6813,
      "step": 3610
    },
    {
      "epoch": 3.8613333333333335,
      "grad_norm": 0.33787083625793457,
      "learning_rate": 1.1366061899679829e-05,
      "loss": 0.7309,
      "step": 3620
    },
    {
      "epoch": 3.872,
      "grad_norm": 0.3425810933113098,
      "learning_rate": 1.1259338313767343e-05,
      "loss": 0.6378,
      "step": 3630
    },
    {
      "epoch": 3.8826666666666667,
      "grad_norm": 0.38069167733192444,
      "learning_rate": 1.1152614727854856e-05,
      "loss": 0.6977,
      "step": 3640
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.28536197543144226,
      "learning_rate": 1.104589114194237e-05,
      "loss": 0.7207,
      "step": 3650
    },
    {
      "epoch": 3.904,
      "grad_norm": 0.3565845489501953,
      "learning_rate": 1.0939167556029884e-05,
      "loss": 0.7162,
      "step": 3660
    },
    {
      "epoch": 3.9146666666666667,
      "grad_norm": 0.5270786285400391,
      "learning_rate": 1.0832443970117396e-05,
      "loss": 0.7009,
      "step": 3670
    },
    {
      "epoch": 3.9253333333333336,
      "grad_norm": 0.36512288451194763,
      "learning_rate": 1.072572038420491e-05,
      "loss": 0.7155,
      "step": 3680
    },
    {
      "epoch": 3.936,
      "grad_norm": 0.3701567053794861,
      "learning_rate": 1.0618996798292424e-05,
      "loss": 0.7107,
      "step": 3690
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.5046980977058411,
      "learning_rate": 1.0512273212379936e-05,
      "loss": 0.6994,
      "step": 3700
    },
    {
      "epoch": 3.9573333333333336,
      "grad_norm": 0.40062403678894043,
      "learning_rate": 1.040554962646745e-05,
      "loss": 0.7054,
      "step": 3710
    },
    {
      "epoch": 3.968,
      "grad_norm": 0.2876752018928528,
      "learning_rate": 1.0298826040554963e-05,
      "loss": 0.6803,
      "step": 3720
    },
    {
      "epoch": 3.978666666666667,
      "grad_norm": 0.3416246473789215,
      "learning_rate": 1.0192102454642477e-05,
      "loss": 0.6898,
      "step": 3730
    },
    {
      "epoch": 3.989333333333333,
      "grad_norm": 0.37201711535453796,
      "learning_rate": 1.008537886872999e-05,
      "loss": 0.7225,
      "step": 3740
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.3464020788669586,
      "learning_rate": 9.978655282817503e-06,
      "loss": 0.6698,
      "step": 3750
    },
    {
      "epoch": 4.010666666666666,
      "grad_norm": 0.3711947202682495,
      "learning_rate": 9.871931696905017e-06,
      "loss": 0.679,
      "step": 3760
    },
    {
      "epoch": 4.021333333333334,
      "grad_norm": 0.3805159628391266,
      "learning_rate": 9.76520811099253e-06,
      "loss": 0.681,
      "step": 3770
    },
    {
      "epoch": 4.032,
      "grad_norm": 0.344482958316803,
      "learning_rate": 9.658484525080043e-06,
      "loss": 0.6894,
      "step": 3780
    },
    {
      "epoch": 4.042666666666666,
      "grad_norm": 0.3907102048397064,
      "learning_rate": 9.551760939167556e-06,
      "loss": 0.7166,
      "step": 3790
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 0.42686301469802856,
      "learning_rate": 9.44503735325507e-06,
      "loss": 0.7282,
      "step": 3800
    },
    {
      "epoch": 4.064,
      "grad_norm": 0.33167564868927,
      "learning_rate": 9.338313767342584e-06,
      "loss": 0.6782,
      "step": 3810
    },
    {
      "epoch": 4.074666666666666,
      "grad_norm": 0.3729204535484314,
      "learning_rate": 9.231590181430098e-06,
      "loss": 0.7078,
      "step": 3820
    },
    {
      "epoch": 4.085333333333334,
      "grad_norm": 0.3375701606273651,
      "learning_rate": 9.12486659551761e-06,
      "loss": 0.6699,
      "step": 3830
    },
    {
      "epoch": 4.096,
      "grad_norm": 0.338481605052948,
      "learning_rate": 9.018143009605123e-06,
      "loss": 0.6947,
      "step": 3840
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 0.35932424664497375,
      "learning_rate": 8.911419423692637e-06,
      "loss": 0.7207,
      "step": 3850
    },
    {
      "epoch": 4.117333333333334,
      "grad_norm": 0.37315982580184937,
      "learning_rate": 8.80469583778015e-06,
      "loss": 0.707,
      "step": 3860
    },
    {
      "epoch": 4.128,
      "grad_norm": 0.5653959512710571,
      "learning_rate": 8.697972251867663e-06,
      "loss": 0.6855,
      "step": 3870
    },
    {
      "epoch": 4.1386666666666665,
      "grad_norm": 0.3701940178871155,
      "learning_rate": 8.591248665955177e-06,
      "loss": 0.7279,
      "step": 3880
    },
    {
      "epoch": 4.149333333333334,
      "grad_norm": 0.38079559803009033,
      "learning_rate": 8.484525080042689e-06,
      "loss": 0.6846,
      "step": 3890
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.4147050082683563,
      "learning_rate": 8.377801494130204e-06,
      "loss": 0.7116,
      "step": 3900
    },
    {
      "epoch": 4.1706666666666665,
      "grad_norm": 0.35357922315597534,
      "learning_rate": 8.271077908217716e-06,
      "loss": 0.7041,
      "step": 3910
    },
    {
      "epoch": 4.181333333333333,
      "grad_norm": 0.37885650992393494,
      "learning_rate": 8.16435432230523e-06,
      "loss": 0.6764,
      "step": 3920
    },
    {
      "epoch": 4.192,
      "grad_norm": 0.4502229690551758,
      "learning_rate": 8.057630736392744e-06,
      "loss": 0.6912,
      "step": 3930
    },
    {
      "epoch": 4.2026666666666666,
      "grad_norm": 0.3606316149234772,
      "learning_rate": 7.950907150480256e-06,
      "loss": 0.7082,
      "step": 3940
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 0.3501879572868347,
      "learning_rate": 7.84418356456777e-06,
      "loss": 0.6785,
      "step": 3950
    },
    {
      "epoch": 4.224,
      "grad_norm": 0.3552130460739136,
      "learning_rate": 7.737459978655284e-06,
      "loss": 0.6671,
      "step": 3960
    },
    {
      "epoch": 4.234666666666667,
      "grad_norm": 0.36810681223869324,
      "learning_rate": 7.630736392742796e-06,
      "loss": 0.6578,
      "step": 3970
    },
    {
      "epoch": 4.245333333333333,
      "grad_norm": 0.293192982673645,
      "learning_rate": 7.5240128068303095e-06,
      "loss": 0.6904,
      "step": 3980
    },
    {
      "epoch": 4.256,
      "grad_norm": 0.37683266401290894,
      "learning_rate": 7.417289220917824e-06,
      "loss": 0.6755,
      "step": 3990
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.41869089007377625,
      "learning_rate": 7.310565635005337e-06,
      "loss": 0.6675,
      "step": 4000
    },
    {
      "epoch": 4.277333333333333,
      "grad_norm": 0.371439129114151,
      "learning_rate": 7.20384204909285e-06,
      "loss": 0.6953,
      "step": 4010
    },
    {
      "epoch": 4.288,
      "grad_norm": 0.3466414511203766,
      "learning_rate": 7.097118463180364e-06,
      "loss": 0.6726,
      "step": 4020
    },
    {
      "epoch": 4.298666666666667,
      "grad_norm": 0.37571442127227783,
      "learning_rate": 6.990394877267877e-06,
      "loss": 0.6784,
      "step": 4030
    },
    {
      "epoch": 4.309333333333333,
      "grad_norm": 0.43763166666030884,
      "learning_rate": 6.88367129135539e-06,
      "loss": 0.7413,
      "step": 4040
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.3382568955421448,
      "learning_rate": 6.7769477054429025e-06,
      "loss": 0.727,
      "step": 4050
    },
    {
      "epoch": 4.330666666666667,
      "grad_norm": 0.3171229362487793,
      "learning_rate": 6.670224119530416e-06,
      "loss": 0.6352,
      "step": 4060
    },
    {
      "epoch": 4.341333333333333,
      "grad_norm": 0.39234521985054016,
      "learning_rate": 6.563500533617929e-06,
      "loss": 0.7157,
      "step": 4070
    },
    {
      "epoch": 4.352,
      "grad_norm": 0.343152791261673,
      "learning_rate": 6.456776947705444e-06,
      "loss": 0.681,
      "step": 4080
    },
    {
      "epoch": 4.362666666666667,
      "grad_norm": 0.40550875663757324,
      "learning_rate": 6.350053361792957e-06,
      "loss": 0.7373,
      "step": 4090
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 0.3819980025291443,
      "learning_rate": 6.2433297758804705e-06,
      "loss": 0.668,
      "step": 4100
    },
    {
      "epoch": 4.384,
      "grad_norm": 0.30719467997550964,
      "learning_rate": 6.1366061899679835e-06,
      "loss": 0.6144,
      "step": 4110
    },
    {
      "epoch": 4.394666666666667,
      "grad_norm": 0.29177576303482056,
      "learning_rate": 6.029882604055496e-06,
      "loss": 0.7061,
      "step": 4120
    },
    {
      "epoch": 4.405333333333333,
      "grad_norm": 0.38437163829803467,
      "learning_rate": 5.923159018143009e-06,
      "loss": 0.6809,
      "step": 4130
    },
    {
      "epoch": 4.416,
      "grad_norm": 0.3935338854789734,
      "learning_rate": 5.816435432230524e-06,
      "loss": 0.7116,
      "step": 4140
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 0.45065444707870483,
      "learning_rate": 5.709711846318037e-06,
      "loss": 0.6784,
      "step": 4150
    },
    {
      "epoch": 4.437333333333333,
      "grad_norm": 0.34530818462371826,
      "learning_rate": 5.60298826040555e-06,
      "loss": 0.6928,
      "step": 4160
    },
    {
      "epoch": 4.448,
      "grad_norm": 0.40020108222961426,
      "learning_rate": 5.496264674493063e-06,
      "loss": 0.7328,
      "step": 4170
    },
    {
      "epoch": 4.458666666666667,
      "grad_norm": 0.3449672758579254,
      "learning_rate": 5.3895410885805765e-06,
      "loss": 0.6861,
      "step": 4180
    },
    {
      "epoch": 4.469333333333333,
      "grad_norm": 0.39944565296173096,
      "learning_rate": 5.28281750266809e-06,
      "loss": 0.7097,
      "step": 4190
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.3854564428329468,
      "learning_rate": 5.176093916755603e-06,
      "loss": 0.7126,
      "step": 4200
    },
    {
      "epoch": 4.490666666666667,
      "grad_norm": 0.2857269048690796,
      "learning_rate": 5.069370330843116e-06,
      "loss": 0.659,
      "step": 4210
    },
    {
      "epoch": 4.501333333333333,
      "grad_norm": 0.4264324903488159,
      "learning_rate": 4.96264674493063e-06,
      "loss": 0.666,
      "step": 4220
    },
    {
      "epoch": 4.5120000000000005,
      "grad_norm": 0.3587886393070221,
      "learning_rate": 4.855923159018144e-06,
      "loss": 0.6765,
      "step": 4230
    },
    {
      "epoch": 4.522666666666667,
      "grad_norm": 0.3830150067806244,
      "learning_rate": 4.749199573105657e-06,
      "loss": 0.6454,
      "step": 4240
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.4167061150074005,
      "learning_rate": 4.6424759871931695e-06,
      "loss": 0.6495,
      "step": 4250
    },
    {
      "epoch": 4.5440000000000005,
      "grad_norm": 0.3501930832862854,
      "learning_rate": 4.535752401280683e-06,
      "loss": 0.6695,
      "step": 4260
    },
    {
      "epoch": 4.554666666666667,
      "grad_norm": 0.3019786775112152,
      "learning_rate": 4.429028815368197e-06,
      "loss": 0.7034,
      "step": 4270
    },
    {
      "epoch": 4.565333333333333,
      "grad_norm": 0.41841185092926025,
      "learning_rate": 4.32230522945571e-06,
      "loss": 0.6728,
      "step": 4280
    },
    {
      "epoch": 4.576,
      "grad_norm": 0.4476637542247772,
      "learning_rate": 4.215581643543223e-06,
      "loss": 0.7116,
      "step": 4290
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 0.3371681272983551,
      "learning_rate": 4.108858057630737e-06,
      "loss": 0.6906,
      "step": 4300
    },
    {
      "epoch": 4.597333333333333,
      "grad_norm": 0.3735617697238922,
      "learning_rate": 4.00213447171825e-06,
      "loss": 0.6827,
      "step": 4310
    },
    {
      "epoch": 4.608,
      "grad_norm": 0.40605276823043823,
      "learning_rate": 3.895410885805763e-06,
      "loss": 0.7051,
      "step": 4320
    },
    {
      "epoch": 4.618666666666667,
      "grad_norm": 0.4111066162586212,
      "learning_rate": 3.7886872998932767e-06,
      "loss": 0.7331,
      "step": 4330
    },
    {
      "epoch": 4.629333333333333,
      "grad_norm": 0.4825804531574249,
      "learning_rate": 3.6819637139807897e-06,
      "loss": 0.6739,
      "step": 4340
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.4716322124004364,
      "learning_rate": 3.575240128068303e-06,
      "loss": 0.6946,
      "step": 4350
    },
    {
      "epoch": 4.650666666666667,
      "grad_norm": 0.390720009803772,
      "learning_rate": 3.4685165421558168e-06,
      "loss": 0.6854,
      "step": 4360
    },
    {
      "epoch": 4.661333333333333,
      "grad_norm": 0.3593064248561859,
      "learning_rate": 3.36179295624333e-06,
      "loss": 0.6583,
      "step": 4370
    },
    {
      "epoch": 4.672,
      "grad_norm": 0.4848763942718506,
      "learning_rate": 3.255069370330843e-06,
      "loss": 0.6966,
      "step": 4380
    },
    {
      "epoch": 4.682666666666667,
      "grad_norm": 0.3935381770133972,
      "learning_rate": 3.1483457844183564e-06,
      "loss": 0.7207,
      "step": 4390
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 0.3555574119091034,
      "learning_rate": 3.0416221985058698e-06,
      "loss": 0.6825,
      "step": 4400
    },
    {
      "epoch": 4.704,
      "grad_norm": 0.3227958679199219,
      "learning_rate": 2.934898612593383e-06,
      "loss": 0.6693,
      "step": 4410
    },
    {
      "epoch": 4.714666666666667,
      "grad_norm": 0.4053473174571991,
      "learning_rate": 2.8281750266808965e-06,
      "loss": 0.6627,
      "step": 4420
    },
    {
      "epoch": 4.725333333333333,
      "grad_norm": 0.3227321207523346,
      "learning_rate": 2.72145144076841e-06,
      "loss": 0.6791,
      "step": 4430
    },
    {
      "epoch": 4.736,
      "grad_norm": 0.38654640316963196,
      "learning_rate": 2.614727854855923e-06,
      "loss": 0.6883,
      "step": 4440
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 0.3659299612045288,
      "learning_rate": 2.5080042689434365e-06,
      "loss": 0.6772,
      "step": 4450
    },
    {
      "epoch": 4.757333333333333,
      "grad_norm": 0.36103594303131104,
      "learning_rate": 2.40128068303095e-06,
      "loss": 0.7113,
      "step": 4460
    },
    {
      "epoch": 4.768,
      "grad_norm": 0.39094018936157227,
      "learning_rate": 2.294557097118463e-06,
      "loss": 0.7095,
      "step": 4470
    },
    {
      "epoch": 4.778666666666666,
      "grad_norm": 0.44500818848609924,
      "learning_rate": 2.1878335112059766e-06,
      "loss": 0.7258,
      "step": 4480
    },
    {
      "epoch": 4.789333333333333,
      "grad_norm": 0.3657783269882202,
      "learning_rate": 2.08110992529349e-06,
      "loss": 0.7132,
      "step": 4490
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.46748659014701843,
      "learning_rate": 1.9743863393810032e-06,
      "loss": 0.6686,
      "step": 4500
    },
    {
      "epoch": 4.810666666666666,
      "grad_norm": 0.5261219143867493,
      "learning_rate": 1.8676627534685166e-06,
      "loss": 0.7084,
      "step": 4510
    },
    {
      "epoch": 4.8213333333333335,
      "grad_norm": 0.4139487147331238,
      "learning_rate": 1.7609391675560297e-06,
      "loss": 0.6952,
      "step": 4520
    },
    {
      "epoch": 4.832,
      "grad_norm": 0.33852091431617737,
      "learning_rate": 1.6542155816435433e-06,
      "loss": 0.6834,
      "step": 4530
    },
    {
      "epoch": 4.842666666666666,
      "grad_norm": 0.4537116289138794,
      "learning_rate": 1.5474919957310566e-06,
      "loss": 0.7079,
      "step": 4540
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 0.4537656009197235,
      "learning_rate": 1.44076840981857e-06,
      "loss": 0.7055,
      "step": 4550
    },
    {
      "epoch": 4.864,
      "grad_norm": 0.42631494998931885,
      "learning_rate": 1.3340448239060833e-06,
      "loss": 0.7308,
      "step": 4560
    },
    {
      "epoch": 4.874666666666666,
      "grad_norm": 0.4049156606197357,
      "learning_rate": 1.2273212379935965e-06,
      "loss": 0.6792,
      "step": 4570
    },
    {
      "epoch": 4.8853333333333335,
      "grad_norm": 0.4087026119232178,
      "learning_rate": 1.1205976520811098e-06,
      "loss": 0.7052,
      "step": 4580
    },
    {
      "epoch": 4.896,
      "grad_norm": 0.3529558479785919,
      "learning_rate": 1.0138740661686232e-06,
      "loss": 0.7397,
      "step": 4590
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 0.33558523654937744,
      "learning_rate": 9.071504802561366e-07,
      "loss": 0.6801,
      "step": 4600
    },
    {
      "epoch": 4.917333333333334,
      "grad_norm": 0.3467177748680115,
      "learning_rate": 8.0042689434365e-07,
      "loss": 0.698,
      "step": 4610
    },
    {
      "epoch": 4.928,
      "grad_norm": 0.4097869396209717,
      "learning_rate": 6.937033084311633e-07,
      "loss": 0.7005,
      "step": 4620
    },
    {
      "epoch": 4.938666666666666,
      "grad_norm": 0.3413357138633728,
      "learning_rate": 5.869797225186767e-07,
      "loss": 0.7231,
      "step": 4630
    },
    {
      "epoch": 4.949333333333334,
      "grad_norm": 0.37688663601875305,
      "learning_rate": 4.8025613660619e-07,
      "loss": 0.6739,
      "step": 4640
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.3653983473777771,
      "learning_rate": 3.7353255069370333e-07,
      "loss": 0.6755,
      "step": 4650
    },
    {
      "epoch": 4.970666666666666,
      "grad_norm": 0.3579256236553192,
      "learning_rate": 2.668089647812167e-07,
      "loss": 0.695,
      "step": 4660
    },
    {
      "epoch": 4.981333333333334,
      "grad_norm": 0.4157085418701172,
      "learning_rate": 1.6008537886873e-07,
      "loss": 0.7036,
      "step": 4670
    },
    {
      "epoch": 4.992,
      "grad_norm": 0.6423922777175903,
      "learning_rate": 5.336179295624334e-08,
      "loss": 0.6985,
      "step": 4680
    },
    {
      "epoch": 4.997333333333334,
      "step": 4685,
      "total_flos": 1.6447849202147328e+18,
      "train_loss": 0.7058627900598906,
      "train_runtime": 57672.0181,
      "train_samples_per_second": 1.3,
      "train_steps_per_second": 0.081
    }
  ],
  "logging_steps": 10,
  "max_steps": 4685,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6447849202147328e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
